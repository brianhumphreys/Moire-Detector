{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Copy of Playground.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianhumphreys/Moire-Detector/blob/main/Copy_of_Playground_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDFp5O-Bqa_H"
      },
      "source": [
        "# Playground"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "muinrEgNqa_J"
      },
      "source": [
        "## Test 2D Wavelet Decomposition function\n",
        "This function(fwdHaarDWT2D) computes the 2D Wavelet Transform in the image. All the input images are passed through a Haar Wavelet Decomposition module, to get the LL, LH, HL and HHH component of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWlOXh6fq3xg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ecc26f0-5fdb-4e6f-cf8a-ae9d1c8ec240"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA7CMKEhro9d",
        "outputId": "d58140d2-a6f7-4528-a04d-00279bf28788"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/\")\n",
        "!ls\n",
        "\n",
        "os.chdir(\"../..\")\n",
        "!ls"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MyDrive\n",
            "bin\t datalab  lib\t mnt   run   tensorflow-1.15.2\ttrainDataPositive\n",
            "boot\t dev\t  lib32  opt   sbin  tmp\t\tusr\n",
            "chl.jpg  etc\t  lib64  proc  srv   tools\t\tvar\n",
            "content  home\t  media  root  sys   trainDataNegative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsTZ_TLIv3TK",
        "outputId": "3b896156-26e5-4eff-a5ef-78d3b9f9461b"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bin\t datalab  lib\t mnt   run   tensorflow-1.15.2\ttrainDataPositive\n",
            "boot\t dev\t  lib32  opt   sbin  tmp\t\tusr\n",
            "chl.jpg  etc\t  lib64  proc  srv   tools\t\tvar\n",
            "content  home\t  media  root  sys   trainDataNegative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9o2VLHoqyOo"
      },
      "source": [
        "#This function(fwdHaarDWT2D) computes the 2D Wavelet Transform in the image. All the input images are passed through a Haar Wavelet Decomposition module, to get the LL, LH, HL and HHH component of the image\n",
        "\n",
        "import numpy as np\n",
        "import pywt\n",
        "\n",
        "def splitFreqBands(img, levRows, levCols):\n",
        "    halfRow = int(levRows/2)\n",
        "    halfCol = int(levCols/2)\n",
        "    LL = img[0:halfRow, 0:halfCol]\n",
        "    LH = img[0:halfRow, halfCol:levCols]\n",
        "    HL = img[halfRow:levRows, 0:halfCol]\n",
        "    HH = img[halfRow:levRows, halfCol:levCols]\n",
        "    \n",
        "    return LL, LH, HL, HH\n",
        "    \n",
        "def haarDWT1D(data, length):\n",
        "    avg0 = 0.5;\n",
        "    avg1 = 0.5;\n",
        "    dif0 = 0.5;\n",
        "    dif1 = -0.5;\n",
        "    temp = np.empty_like(data)\n",
        "    temp = temp.astype(float)\n",
        "    h = int(length/2)\n",
        "    for i in range(h):\n",
        "        k = i*2\n",
        "        temp[i] = data[k] * avg0 + data[k + 1] * avg1;\n",
        "        temp[i + h] = data[k] * dif0 + data[k + 1] * dif1;\n",
        "    \n",
        "    data[:] = temp\n",
        "\n",
        "# computes the homography coefficients for PIL.Image.transform using point correspondences\n",
        "def fwdHaarDWT2D(img):\n",
        "    img = np.array(img)\n",
        "    levRows = img.shape[0];\n",
        "    levCols = img.shape[1];\n",
        "    img = img.astype(float)\n",
        "    for i in range(levRows):\n",
        "        row = img[i,:]\n",
        "        haarDWT1D(row, levCols)\n",
        "        img[i,:] = row\n",
        "    for j in range(levCols):\n",
        "        col = img[:,j]\n",
        "        haarDWT1D(col, levRows)\n",
        "        img[:,j] = col\n",
        "        \n",
        "    return splitFreqBands(img, levRows, levCols)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a63KHmZYqa_K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "7b421570-14fb-4055-dd65-4865d1e2c73c"
      },
      "source": [
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "img = Image.open('/content/drive/MyDrive/Moire/positiveImages/350_letterbox1024.jpg').convert('L')\n",
        "img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "img.save('chl.jpg')\n",
        "LL, LH, HL, HH = fwdHaarDWT2D(img)\n",
        "fig, axes = plt.subplots(2, 2)\n",
        "fig.tight_layout()\n",
        "axes[0, 0].imshow(LL)\n",
        "axes[0, 1].imshow(LH)\n",
        "axes[1, 0].imshow(HL)\n",
        "axes[1, 1].imshow(HH)\n",
        "axes[0, 0].set_title(\"LL\")\n",
        "axes[0, 1].set_title(\"LH\")\n",
        "axes[1, 0].set_title(\"HL\")\n",
        "axes[1, 1].set_title(\"HH\")\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-5bd465c838d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Moire/positiveImages/350_letterbox1024.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLIP_LEFT_RIGHT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chl.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Moire/positiveImages/350_letterbox1024.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNnfNmoEqa_M"
      },
      "source": [
        "## Test training data creation\n",
        "The training images need to be put in two folders. positiveImages and negativeImages. positiveImages are the images which are captured from the display devices and has the presence of stron or weak Moiré patterms in it.\n",
        "negativeImages are the ones without Moiré Patterns (i.e. the images which are not captured from the display devices) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO8AKkFsqm5W"
      },
      "source": [
        "import sys\n",
        "import argparse\n",
        "from PIL import Image\n",
        "from PIL import ImageOps\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "\n",
        "#The training images need to be put in two folders. positiveImages and negativeImages. positiveImages are the images which are captured from the display devices and has the presence of stron or weak Moiré patterms in it. negativeImages are the ones without Moiré Patterns (i.e. the images which are not captured from the display devices)\n",
        "\n",
        "\n",
        "#folders to store training data\n",
        "positiveTrainImagePath = ''\n",
        "negativeTrainImagePath = ''\n",
        "\n",
        "def mainAugment(positiveImages, negativeImages, train):\n",
        "    \n",
        "    global positiveTrainImagePath\n",
        "    global negativeTrainImagePath\n",
        "    \n",
        "    positiveImagePath = (positiveImages)\n",
        "    negativeImagePath = (negativeImages)\n",
        "    \n",
        "    if (train == 0):\n",
        "        positiveTrainImagePath = './trainDataPositive'\n",
        "        negativeTrainImagePath = './trainDataNegative'\n",
        "    else:\n",
        "        positiveTrainImagePath = './testDataPositive'\n",
        "        negativeTrainImagePath = './testDataNegative'\n",
        "        \n",
        "    createTrainingData(positiveImagePath, negativeImagePath)\n",
        "\n",
        "    \n",
        "#The wavelet decomposed images are the transformed images representing the spatial and the frequency information of the image. These images are stored as 'tiff' in the disk, to preserve that information. Each image is transformed with 180 degrees rotation and as well flipped, as part of data augmentation.\n",
        "\n",
        "def transformImageAndSave(image, f, customStr, path):\n",
        "    cA, cH, cV, cD  = fwdHaarDWT2D(image);\n",
        "\n",
        "    # fig, axes = plt.subplots(2, 2)\n",
        "    # fig.tight_layout()\n",
        "    # axes[0, 0].imshow(cA)\n",
        "    # axes[0, 1].imshow(cH)\n",
        "    # axes[1, 0].imshow(cV)\n",
        "    # axes[1, 1].imshow(cD)\n",
        "    # axes[0, 0].set_title(\"LL\")\n",
        "    # axes[0, 1].set_title(\"LH\")\n",
        "    # axes[1, 0].set_title(\"HL\")\n",
        "    # axes[1, 1].set_title(\"HH\")\n",
        "    # plt.show()\n",
        "    \n",
        "    fileName = (os.path.splitext(f)[0])\n",
        "    fLL = (f.replace(fileName, fileName+'_' + customStr + 'LL')).replace('.jpg','.tiff')\n",
        "    fLH = (f.replace(fileName, fileName+'_' + customStr + 'LH')).replace('.jpg','.tiff')\n",
        "    fHL = (f.replace(fileName, fileName+'_' + customStr + 'HL')).replace('.jpg','.tiff')\n",
        "    fHH = (f.replace(fileName, fileName+'_' + customStr + 'HH')).replace('.jpg','.tiff')\n",
        "    cA = Image.fromarray(cA)\n",
        "    cH = Image.fromarray(cH)\n",
        "    cV = Image.fromarray(cV)\n",
        "    cD = Image.fromarray(cD)\n",
        "    cA.save(join(path, fLL))\n",
        "    cH.save(join(path, fLH))\n",
        "    cV.save(join(path, fHL))\n",
        "    cD.save(join(path, fHH))\n",
        "    \n",
        "    \n",
        "def augmentAndTrasformImage(f, mainFolder, trainFolder):\n",
        "    try:\n",
        "        img = Image.open(join(mainFolder, f)) \n",
        "    except:\n",
        "        print('Error: Couldnt read the file {}. Make sure only images are present in the folder'.format(f))\n",
        "        return None\n",
        "\n",
        "    imgGray = img.convert('L')\n",
        "    wdChk, htChk = imgGray.size\n",
        "    if htChk > wdChk:\n",
        "        imgGray = imgGray.rotate(-90, expand=1)\n",
        "        print('training image rotated')\n",
        "    transformImageAndSave(imgGray, f, '', trainFolder)\n",
        "\n",
        "    imgGray = imgGray.transpose(Image.ROTATE_180)\n",
        "    transformImageAndSave(imgGray, f, '180_', trainFolder)\n",
        "\n",
        "    imgGray = imgGray.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    transformImageAndSave(imgGray, f, '180_FLIP_', trainFolder)\n",
        "    \n",
        "    return True\n",
        "    \n",
        "    \n",
        "def createTrainingData(positiveImagePath, negativeImagePath):\n",
        "    \n",
        "    # get image files by classes\n",
        "    positiveImageFiles = [f for f in listdir(positiveImagePath) if (isfile(join(positiveImagePath, f)))]\n",
        "    negativeImageFiles = [f for f in listdir(negativeImagePath) if (isfile(join(negativeImagePath, f)))]\n",
        "\n",
        "    positiveCount = len(positiveImageFiles)\n",
        "    negativeCount = len(negativeImageFiles)\n",
        "\n",
        "    print('positive samples: ' + str(positiveCount))\n",
        "    print('negative samples: ' + str(negativeCount))\n",
        "    \n",
        "    # create folders (not tracked by git)\n",
        "    if not os.path.exists(positiveTrainImagePath):\n",
        "        os.makedirs(positiveTrainImagePath)\n",
        "    if not os.path.exists(negativeTrainImagePath):\n",
        "        os.makedirs(negativeTrainImagePath)\n",
        "\n",
        "    Knegative = 0\n",
        "    Kpositive = 0\n",
        "\n",
        "    # create positive training images \n",
        "    for f in positiveImageFiles:\n",
        "        ret = augmentAndTrasformImage(f, positiveImagePath, positiveTrainImagePath)\n",
        "        if ret == None:\n",
        "            continue\n",
        "        Kpositive += 3\n",
        "\n",
        "\n",
        "    # create negative training images \n",
        "    for f in negativeImageFiles:\n",
        "        ret = augmentAndTrasformImage(f, negativeImagePath, negativeTrainImagePath)\n",
        "        if ret == None:\n",
        "            continue\n",
        "        Knegative += 3;\n",
        "    \n",
        "    print('Total positive files after augmentation: ', Kpositive)\n",
        "    print('Total negative files after augmentation: ', Knegative)\n",
        "    \n",
        "        \n",
        "\n",
        "def parse_arguments(argv):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    \n",
        "    parser.add_argument('positiveImages', type=str, help='Directory with positive (Moiré pattern) images.')\n",
        "    parser.add_argument('negativeImages', type=str, help='Directory with negative (Normal) images.')\n",
        "    parser.add_argument('train', type=int, help='0 = train, 1 = test')\n",
        "    \n",
        "    return parser.parse_args(argv)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzpWIkVNqa_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3319a8ff-5b2e-47c1-d7b9-8a1e75da2092"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "positiveImagePath = '/content/drive/MyDrive/Moire/preaugmentedPositiveImages'\n",
        "negativeImagePath = '/content/drive/MyDrive/Moire/preaugmentedNegativeImages'\n",
        "\n",
        "mainAugment(positiveImagePath, negativeImagePath, 0)\n",
        "   "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive samples: 4\n",
            "negative samples: 4\n",
            "training image rotated\n",
            "training image rotated\n",
            "training image rotated\n",
            "Total positive files after augmentation:  12\n",
            "Total negative files after augmentation:  12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNTveLG9tEf6"
      },
      "source": [
        "\n",
        "!rm -rf /content/drive/MyDrive/Moire/augmentedTrainedPositive\n",
        "!rm -rf /content/drive/MyDrive/Moire/augmentedTrainedNegative\n",
        "\n",
        "!cp -r /trainDataPositive /content/drive/MyDrive/Moire/augmentedTrainedPositive\n",
        "!cp -r /trainDataNegative /content/drive/MyDrive/Moire/augmentedTrainedNegative\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M47IP9coRcYN",
        "outputId": "510a5882-10ad-4856-9788-b9002aed2e2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls /trainDataPositive\n",
        "!ls /trainDataNegative"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "284_letterbox1024_180_FLIP_HH.tiff  355_letterbox1024_180_FLIP_HH.tiff\n",
            "284_letterbox1024_180_FLIP_HL.tiff  355_letterbox1024_180_FLIP_HL.tiff\n",
            "284_letterbox1024_180_FLIP_LH.tiff  355_letterbox1024_180_FLIP_LH.tiff\n",
            "284_letterbox1024_180_FLIP_LL.tiff  355_letterbox1024_180_FLIP_LL.tiff\n",
            "284_letterbox1024_180_HH.tiff\t    355_letterbox1024_180_HH.tiff\n",
            "284_letterbox1024_180_HL.tiff\t    355_letterbox1024_180_HL.tiff\n",
            "284_letterbox1024_180_LH.tiff\t    355_letterbox1024_180_LH.tiff\n",
            "284_letterbox1024_180_LL.tiff\t    355_letterbox1024_180_LL.tiff\n",
            "284_letterbox1024_HH.tiff\t    355_letterbox1024_HH.tiff\n",
            "284_letterbox1024_HL.tiff\t    355_letterbox1024_HL.tiff\n",
            "284_letterbox1024_LH.tiff\t    355_letterbox1024_LH.tiff\n",
            "284_letterbox1024_LL.tiff\t    355_letterbox1024_LL.tiff\n",
            "350_letterbox1024_180_FLIP_HH.tiff  chl_180_FLIP_HH.tiff\n",
            "350_letterbox1024_180_FLIP_HL.tiff  chl_180_FLIP_HL.tiff\n",
            "350_letterbox1024_180_FLIP_LH.tiff  chl_180_FLIP_LH.tiff\n",
            "350_letterbox1024_180_FLIP_LL.tiff  chl_180_FLIP_LL.tiff\n",
            "350_letterbox1024_180_HH.tiff\t    chl_180_HH.tiff\n",
            "350_letterbox1024_180_HL.tiff\t    chl_180_HL.tiff\n",
            "350_letterbox1024_180_LH.tiff\t    chl_180_LH.tiff\n",
            "350_letterbox1024_180_LL.tiff\t    chl_180_LL.tiff\n",
            "350_letterbox1024_HH.tiff\t    chl_HH.tiff\n",
            "350_letterbox1024_HL.tiff\t    chl_HL.tiff\n",
            "350_letterbox1024_LH.tiff\t    chl_LH.tiff\n",
            "350_letterbox1024_LL.tiff\t    chl_LL.tiff\n",
            "0_letterbox1024_180_FLIP_HH.tiff   22_letterbox1024_180_FLIP_HH.tiff\n",
            "0_letterbox1024_180_FLIP_HL.tiff   22_letterbox1024_180_FLIP_HL.tiff\n",
            "0_letterbox1024_180_FLIP_LH.tiff   22_letterbox1024_180_FLIP_LH.tiff\n",
            "0_letterbox1024_180_FLIP_LL.tiff   22_letterbox1024_180_FLIP_LL.tiff\n",
            "0_letterbox1024_180_HH.tiff\t   22_letterbox1024_180_HH.tiff\n",
            "0_letterbox1024_180_HL.tiff\t   22_letterbox1024_180_HL.tiff\n",
            "0_letterbox1024_180_LH.tiff\t   22_letterbox1024_180_LH.tiff\n",
            "0_letterbox1024_180_LL.tiff\t   22_letterbox1024_180_LL.tiff\n",
            "0_letterbox1024_HH.tiff\t\t   22_letterbox1024_HH.tiff\n",
            "0_letterbox1024_HL.tiff\t\t   22_letterbox1024_HL.tiff\n",
            "0_letterbox1024_LH.tiff\t\t   22_letterbox1024_LH.tiff\n",
            "0_letterbox1024_LL.tiff\t\t   22_letterbox1024_LL.tiff\n",
            "14_letterbox1024_180_FLIP_HH.tiff  37_letterbox1024_180_FLIP_HH.tiff\n",
            "14_letterbox1024_180_FLIP_HL.tiff  37_letterbox1024_180_FLIP_HL.tiff\n",
            "14_letterbox1024_180_FLIP_LH.tiff  37_letterbox1024_180_FLIP_LH.tiff\n",
            "14_letterbox1024_180_FLIP_LL.tiff  37_letterbox1024_180_FLIP_LL.tiff\n",
            "14_letterbox1024_180_HH.tiff\t   37_letterbox1024_180_HH.tiff\n",
            "14_letterbox1024_180_HL.tiff\t   37_letterbox1024_180_HL.tiff\n",
            "14_letterbox1024_180_LH.tiff\t   37_letterbox1024_180_LH.tiff\n",
            "14_letterbox1024_180_LL.tiff\t   37_letterbox1024_180_LL.tiff\n",
            "14_letterbox1024_HH.tiff\t   37_letterbox1024_HH.tiff\n",
            "14_letterbox1024_HL.tiff\t   37_letterbox1024_HL.tiff\n",
            "14_letterbox1024_LH.tiff\t   37_letterbox1024_LH.tiff\n",
            "14_letterbox1024_LL.tiff\t   37_letterbox1024_LL.tiff\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUfHfeHoqa_N"
      },
      "source": [
        "## Test CNN training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7p83VhGtbqo"
      },
      "source": [
        "import os\n",
        "\n",
        "from keras.models import Model # basic class for specifying and training a neural network\n",
        "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, Add, Multiply, Maximum\n",
        "\n",
        "def createModel(height, width, depth, num_classes):\n",
        "#     num_epochs = 20 # 50 26 200 # we iterate 200 times over the entire training set\n",
        "    kernel_size_1 = 7 # we will use 7x7 kernels \n",
        "    kernel_size_2 = 3 # we will use 3x3 kernels \n",
        "    pool_size = 2 # we will use 2x2 pooling throughout\n",
        "    conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n",
        "    conv_depth_2 = 16 # ...switching to 16 after the first pooling layer\n",
        "    drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
        "    drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
        "    hidden_size = 32 # 128 512 the FC layer will have 512 neurons\n",
        "\n",
        "\n",
        "    inpLL = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
        "    inpLH = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
        "    inpHL = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
        "    inpHH = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
        "\n",
        "    conv_1_LL = Convolution2D(conv_depth_1, (kernel_size_1, kernel_size_1), padding='same', activation='relu')(inpLL)\n",
        "    conv_1_LH = Convolution2D(conv_depth_1, (kernel_size_1, kernel_size_1), padding='same', activation='relu')(inpLH)\n",
        "    conv_1_HL = Convolution2D(conv_depth_1, (kernel_size_1, kernel_size_1), padding='same', activation='relu')(inpHL)\n",
        "    conv_1_HH = Convolution2D(conv_depth_1, (kernel_size_1, kernel_size_1), padding='same', activation='relu')(inpHH)\n",
        "    pool_1_LL = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_1_LL)\n",
        "    pool_1_LH = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_1_LH)\n",
        "    pool_1_HL = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_1_HL)\n",
        "    pool_1_HH = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_1_HH)\n",
        "\n",
        "    avg_LH_HL_HH = Maximum()([pool_1_LH, pool_1_HL, pool_1_HH])\n",
        "    inp_merged = Multiply()([pool_1_LL, avg_LH_HL_HH])\n",
        "    C4 = Convolution2D(conv_depth_2, (kernel_size_2, kernel_size_2), padding='same', activation='relu')(inp_merged)\n",
        "    S2 = MaxPooling2D(pool_size=(4, 4))(C4)\n",
        "    drop_1 = Dropout(drop_prob_1)(S2)\n",
        "    C5 = Convolution2D(conv_depth_1, (kernel_size_2, kernel_size_2), padding='same', activation='relu')(drop_1)\n",
        "    S3 = MaxPooling2D(pool_size=(pool_size, pool_size))(C5)\n",
        "    C6 = Convolution2D(conv_depth_1, (kernel_size_2, kernel_size_2), padding='same', activation='relu')(S3)\n",
        "    S4 = MaxPooling2D(pool_size=(pool_size, pool_size))(C6)\n",
        "    drop_2 = Dropout(drop_prob_1)(S4)\n",
        "    # Now flatten to 1D, apply FC -> ReLU (with dropout) -> softmax\n",
        "    flat = Flatten()(drop_2)\n",
        "    hidden = Dense(hidden_size, activation='relu')(flat)\n",
        "    drop_3 = Dropout(drop_prob_2)(hidden)\n",
        "    out = Dense(num_classes, activation='softmax')(drop_3)\n",
        "    \n",
        "    model = Model(inputs=[inpLL, inpLH, inpHL, inpHH], outputs=out) # To define a model, just specify its input and output layers\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTX2vudptIaO"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import argparse\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from skimage import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#constants\n",
        "WIDTH = 500#384\n",
        "HEIGHT = 375#512\n",
        "\n",
        "def scaleData(inp, minimum, maximum):\n",
        "    minMaxScaler = preprocessing.MinMaxScaler(copy=True, feature_range=(minimum,maximum))\n",
        "    inp = inp.reshape(-1, 1)\n",
        "    inp = minMaxScaler.fit_transform(inp)\n",
        "    \n",
        "    return inp\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# - read positive and negative training data\n",
        "# - create X and Y from training data\n",
        "\n",
        "\n",
        "def mainTrain(positiveImages, negativeImages, trainingDataPositive, trainingDataNegative, epochs):\n",
        "    positiveImagePath = (positiveImages)\n",
        "    negativeImagePath = (negativeImages)\n",
        "    numEpochs = (epochs)\n",
        "    positiveTrainImagePath = trainingDataPositive\n",
        "    negativeTrainImagePath = trainingDataNegative\n",
        "\n",
        "    \n",
        "    X_LL, X_LH, X_HL, X_HH, X_index, Y, imageCount = readWaveletData(positiveImagePath, negativeImagePath, positiveTrainImagePath, negativeTrainImagePath)\n",
        "    \n",
        "    X_LL_train,X_LH_train,X_HL_train,X_HH_train,Y_train,X_LL_test,X_LH_test,X_HL_test,X_HH_test,Y_test = trainTestSplit(X_LL, X_LH, X_HL, X_HH, X_index, Y, imageCount)\n",
        "    \n",
        "    model = trainCNNModel(X_LL_train,X_LH_train,X_HL_train,X_HH_train,Y_train,\n",
        "             X_LL_test,X_LH_test,X_HL_test,X_HH_test,Y_test, numEpochs)\n",
        "    \n",
        "    evaluate(model, X_LL_test,X_LH_test,X_HL_test,X_HH_test,Y_test)\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "def readAndScaleImage(f, customStr, trainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, sampleVal):\n",
        "    fileName = (os.path.splitext(f)[0])\n",
        "    fLL = (f.replace(fileName, fileName + customStr + '_LL')).replace('.jpg','.tiff')\n",
        "    fLH = (f.replace(fileName, fileName + customStr + '_LH')).replace('.jpg','.tiff')\n",
        "    fHL = (f.replace(fileName, fileName + customStr + '_HL')).replace('.jpg','.tiff')\n",
        "    fHH = (f.replace(fileName, fileName + customStr + '_HH')).replace('.jpg','.tiff')\n",
        "    \n",
        "    try:\n",
        "        imgLL = Image.open(join(trainImagePath, fLL))\n",
        "        imgLH = Image.open(join(trainImagePath, fLH))\n",
        "        imgHL = Image.open(join(trainImagePath, fHL))\n",
        "        imgHH = Image.open(join(trainImagePath, fHH))\n",
        "    except Exception as e:\n",
        "        print('Error: Couldnt read the file {}. Make sure only images are present in the folder'.format(fileName))\n",
        "        print('Exception:', e)\n",
        "        return None\n",
        "        \n",
        "    imgLL = np.array(imgLL)\n",
        "    imgLH = np.array(imgLH)\n",
        "    imgHL = np.array(imgHL)\n",
        "    imgHH = np.array(imgHH)\n",
        "    imgLL = scaleData(imgLL, 0, 1)\n",
        "    imgLH = scaleData(imgLH, -1, 1)\n",
        "    imgHL = scaleData(imgHL, -1, 1)\n",
        "    imgHH = scaleData(imgHH, -1, 1)\n",
        "    \n",
        "    imgVector = imgLL.reshape(1, WIDTH*HEIGHT)\n",
        "    X_LL[sampleIndex, :] = imgVector\n",
        "    imgVector = imgLH.reshape(1, WIDTH*HEIGHT)\n",
        "    X_LH[sampleIndex, :] = imgVector\n",
        "    imgVector = imgHL.reshape(1, WIDTH*HEIGHT)\n",
        "    X_HL[sampleIndex, :] = imgVector\n",
        "    imgVector = imgHH.reshape(1, WIDTH*HEIGHT)\n",
        "    X_HH[sampleIndex, :] = imgVector\n",
        "    \n",
        "    Y[sampleIndex, 0] = sampleVal;\n",
        "    X_index[sampleIndex, 0] = sampleIndex;\n",
        "    \n",
        "    return True\n",
        "    \n",
        "def readImageSet(imageFiles, trainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, bClass):\n",
        "\n",
        "    for f in imageFiles:\n",
        "        ret = readAndScaleImage(f, '', trainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, bClass)\n",
        "        if ret == True:\n",
        "            sampleIndex = sampleIndex + 1\n",
        "\n",
        "        #read 180deg rotated data\n",
        "        ret = readAndScaleImage(f, '_180', trainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex,bClass)\n",
        "        if ret == True:\n",
        "            sampleIndex = sampleIndex + 1\n",
        "\n",
        "        #read 180deg FLIP data\n",
        "        ret = readAndScaleImage(f, '_180_FLIP', trainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, bClass)\n",
        "        if ret == True:\n",
        "            sampleIndex = sampleIndex + 1\n",
        "     \n",
        "    return sampleIndex\n",
        "            \n",
        "            \n",
        "def readWaveletData(positiveImagePath, negativeImagePath, positiveTrainImagePath, negativeTrainImagePath):\n",
        "    \n",
        "    # get augmented, balanced training data image files by class\n",
        "    positiveImageFiles = [f for f in listdir(positiveImagePath) if (isfile(join(positiveImagePath, f)))]\n",
        "    negativeImageFiles = [f for f in listdir(negativeImagePath) if (isfile(join(negativeImagePath, f)))]\n",
        "\n",
        "    \n",
        "    positiveCount = len(positiveImageFiles)*4\n",
        "    negativeCount = len(negativeImageFiles)*4\n",
        "\n",
        "    print('positive samples: ' + str(positiveCount))\n",
        "    print('negative samples: ' + str(negativeCount))\n",
        "    imageCount = positiveCount + negativeCount\n",
        "    #intialization\n",
        "    X_LL = np.zeros((positiveCount + negativeCount, WIDTH*HEIGHT))\n",
        "    X_LH = np.zeros((positiveCount + negativeCount, WIDTH*HEIGHT))\n",
        "    X_HL = np.zeros((positiveCount + negativeCount, WIDTH*HEIGHT))\n",
        "    X_HH = np.zeros((positiveCount + negativeCount, WIDTH*HEIGHT))\n",
        "    X_index = np.zeros((positiveCount + negativeCount, 1))\n",
        "    Y = np.zeros((positiveCount + negativeCount, 1))\n",
        "    \n",
        "    sampleIndex = 0\n",
        "    # read all images, convert to float, divide by 255 (leads to gray range 0..1), reshape into a row vector\n",
        "    # write class 0 for positive and 1 for negative samples    \n",
        "    sampleIndex = readImageSet(positiveImageFiles, positiveTrainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, 0)\n",
        "    print('positive data loaded.')\n",
        "    \n",
        "    sampleIndex += readImageSet(negativeImageFiles, negativeTrainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, 1)\n",
        "    print('negative data loaded.')\n",
        "\n",
        "    print('Total Samples Loaded: ', sampleIndex)\n",
        "    # print(X_LL)\n",
        "    # print(X_LH)\n",
        "    # print(Y)\n",
        "    \n",
        "    return X_LL, X_LH, X_HL, X_HH, X_index, Y, imageCount\n",
        "\n",
        "\n",
        "\n",
        "#Here, we perform index based splitting and use those indices to split the our multi-input datasets. This is done because the CNN model is multi-input network\n",
        "def splitTrainTestDataForBands(inputData, X_train_ind, X_test_ind):\n",
        "    X_train = np.zeros((len(X_train_ind), WIDTH*HEIGHT))\n",
        "    for i in range(len(X_train_ind)):\n",
        "        X_train[i,:] = inputData[int(X_train_ind[i,0]),:]\n",
        "        \n",
        "    X_test = np.zeros((len(X_test_ind), WIDTH*HEIGHT))\n",
        "    for i in range(len(X_test_ind)):\n",
        "        X_test[i,:] = inputData[int(X_test_ind[i,0]),:]\n",
        "        \n",
        "    return X_train, X_test\n",
        "\n",
        "\n",
        "def countPositiveSamplesAfterSplit(trainData):\n",
        "    count = 0;\n",
        "    for i in range(len(trainData)):\n",
        "        if(trainData[i,0] == 0):\n",
        "            count = count + 1\n",
        "    return count\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def trainTestSplit(X_LL, X_LH, X_HL, X_HH, X_index, Y, imageCount):\n",
        "    testCountPercent = 0.1\n",
        "\n",
        "    # evaluate the model by splitting into train and test sets\n",
        "    X_train_ind, X_test_ind, y_train, y_test = train_test_split(X_index, Y, test_size=testCountPercent, random_state=1, stratify=Y)\n",
        "\n",
        "    X_LL_train, X_LL_test = splitTrainTestDataForBands(X_LL, X_train_ind, X_test_ind)\n",
        "    X_LH_train, X_LH_test = splitTrainTestDataForBands(X_LH, X_train_ind, X_test_ind)\n",
        "    X_HL_train, X_HL_test = splitTrainTestDataForBands(X_HL, X_train_ind, X_test_ind)\n",
        "    X_HH_train, X_HH_test = splitTrainTestDataForBands(X_HH, X_train_ind, X_test_ind)\n",
        "\n",
        "    imageHeight = HEIGHT\n",
        "    imageWidth = WIDTH\n",
        "\n",
        "\n",
        "    print(countPositiveSamplesAfterSplit(y_train))\n",
        "    print(len(X_LL_train))\n",
        "    print(len(y_train))\n",
        "    print(len(X_LL_test))\n",
        "    print(len(y_test))\n",
        "\n",
        "    num_train_samples = len(y_train)\n",
        "    print('num_train_samples', num_train_samples)\n",
        "    X_LL_train = np.array(X_LL_train)\n",
        "    X_LL_train = X_LL_train.reshape((num_train_samples, imageHeight, imageWidth, 1))\n",
        "    X_LL_test = np.array(X_LL_test)\n",
        "    X_LL_test = X_LL_test.reshape((imageCount - num_train_samples, imageHeight, imageWidth, 1))\n",
        "\n",
        "    X_LH_train = np.array(X_LH_train)\n",
        "    X_LH_train = X_LH_train.reshape((num_train_samples, imageHeight, imageWidth, 1))\n",
        "    X_LH_test = np.array(X_LH_test)\n",
        "    X_LH_test = X_LH_test.reshape((imageCount - num_train_samples, imageHeight, imageWidth, 1))\n",
        "\n",
        "    X_HL_train = np.array(X_HL_train)\n",
        "    X_HL_train = X_HL_train.reshape((num_train_samples, imageHeight, imageWidth, 1))\n",
        "    X_HL_test = np.array(X_HL_test)\n",
        "    X_HL_test = X_HL_test.reshape((imageCount - num_train_samples, imageHeight, imageWidth, 1))\n",
        "    \n",
        "    X_HH_train = np.array(X_HH_train)\n",
        "    X_HH_train = X_HH_train.reshape((num_train_samples, imageHeight, imageWidth, 1))\n",
        "    X_HH_test = np.array(X_HH_test)\n",
        "    X_HH_test = X_HH_test.reshape((imageCount - num_train_samples, imageHeight, imageWidth, 1))\n",
        "\n",
        "    y_train = np.array(y_train)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "\n",
        "    num_train, height, width, depth = X_LL_train.shape\n",
        "    num_test = X_LL_test.shape[0] \n",
        "    num_classes = len(np.unique(y_train))\n",
        "    \n",
        "    return X_LL_train,X_LH_train,X_HL_train,X_HH_train,y_train,X_LL_test,X_LH_test,X_HL_test,X_HH_test,y_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def trainCNNModel(X_LL_train,X_LH_train,X_HL_train,X_HH_train,y_train,\n",
        "             X_LL_test,X_LH_test,X_HL_test,X_HH_test,y_test, num_epochs):\n",
        "\n",
        "    batch_size = 32 # in each iteration, we consider 32 training examples at once\n",
        "    num_train, height, width, depth = X_LL_train.shape\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    Y_train = np_utils.to_categorical(y_train, num_classes) # One-hot encode the labels\n",
        "    Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels\n",
        "\n",
        "    checkPointFolder = 'checkPoint'\n",
        "    checkpoint_name = checkPointFolder + '/Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "    checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "    callbacks_list = [checkpoint]\n",
        "    \n",
        "    if not os.path.exists(checkPointFolder):\n",
        "        os.makedirs(checkPointFolder)\n",
        "        \n",
        "        \n",
        "    model = createModel(height, width, depth, num_classes)\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
        "                  optimizer='adam', # using the Adam optimiser\n",
        "                  metrics=['accuracy']) # reporting the accuracy\n",
        "\n",
        "    model.fit([X_LL_train,X_LH_train,X_HL_train,X_HH_train], Y_train,                # Train the model using the training set...\n",
        "              batch_size=batch_size, epochs=num_epochs,\n",
        "              verbose=1, validation_split=0.1, callbacks=callbacks_list) # ...holding out 10% of the data for validation\n",
        "    score, acc = model.evaluate([X_LL_test,X_LH_test,X_HL_test,X_HH_test], Y_test, verbose=1)  # Evaluate the trained model on the test set!\n",
        "\n",
        "    model.save('moirePattern3CNN_.h5')\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate(model, X_LL_test,X_LH_test,X_HL_test,X_HH_test,y_test):\n",
        "\n",
        "    model_out = model.predict([X_LL_test,X_LH_test,X_HL_test,X_HH_test])\n",
        "    passCnt = 0\n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    for i in range(len(y_test)):\n",
        "        if np.argmax(model_out[i, :]) == y_test[i]:\n",
        "            str_label='Pass'\n",
        "            passCnt = passCnt + 1\n",
        "        else:\n",
        "            str_label='Fail'\n",
        "\n",
        "        if y_test[i] ==0:\n",
        "            if np.argmax(model_out[i, :]) == y_test[i]:\n",
        "                TP = TP + 1;\n",
        "            else:\n",
        "                FN = FN + 1\n",
        "        else:\n",
        "            if np.argmax(model_out[i, :]) == y_test[i]:\n",
        "                TN = TN + 1;\n",
        "            else:\n",
        "                FP = FP + 1\n",
        "\n",
        "    start = \"\\033[1m\"\n",
        "    end = \"\\033[0;0m\"\n",
        "    print(start + 'confusion matrix (test / validation)' + end)\n",
        "    print(start + 'true positive:  '+ end + str(TP))\n",
        "    print(start + 'false positive: '+ end + str(FP))\n",
        "    print(start + 'true negative:  '+ end + str(TN))\n",
        "    print(start + 'false negative: '+ end + str(FN))\n",
        "    print('\\n')\n",
        "    print(start + 'accuracy:  ' + end + \"{:.4f} %\".format(100*(TP+TN)/(TP+FP+FN+TN)))\n",
        "    print(start + 'precision: ' + end + \"{:.4f} %\".format(100*TP/(TP + FP)))\n",
        "    print(start + 'recall:  ' + end + \"{:.4f} %\".format(100*TP/(TP + FN)))\n",
        "\n",
        "\n",
        "def parse_arguments(argv):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    \n",
        "    parser.add_argument('positiveImages', type=str, help='Directory with original positive (Moiré pattern) images.')\n",
        "    parser.add_argument('negativeImages', type=str, help='Directory with original negative (Normal) images.')\n",
        "    \n",
        "    parser.add_argument('trainingDataPositive', type=str, help='Directory with transformed positive (Moiré pattern) images.')\n",
        "    parser.add_argument('trainingDataNegative', type=str, help='Directory with transformed negative (Normal) images.')\n",
        "    \n",
        "    parser.add_argument('epochs', type=int, help='Number of epochs for training')\n",
        "    \n",
        "    return parser.parse_args(argv)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "120i4Ry-qa_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae95287b-3b80-4479-d3b8-60a8cd6fe925"
      },
      "source": [
        "positiveTrainImagePath = '/content/drive/MyDrive/Moire/augmentedTrainedPositive'\n",
        "negativeTrainImagePath = '/content/drive/MyDrive/Moire/augmentedTrainedNegative'\n",
        "epochs = 20\n",
        "    \n",
        "mainTrain(positiveImagePath, negativeImagePath, positiveTrainImagePath, negativeTrainImagePath, epochs)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive samples: 16\n",
            "negative samples: 16\n",
            "positive data loaded.\n",
            "negative data loaded.\n",
            "Total Samples Loaded:  36\n",
            "18\n",
            "28\n",
            "28\n",
            "4\n",
            "4\n",
            "num_train_samples 28\n",
            "Epoch 1/20\n",
            "1/1 [==============================] - 46s 46s/step - loss: 0.6889 - accuracy: 0.6800 - val_loss: 0.6778 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.67783, saving model to checkPoint/Weights-001--0.67783.hdf5\n",
            "Epoch 2/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 333ms/step - loss: 0.6789 - accuracy: 0.6400 - val_loss: 0.6601 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.67783 to 0.66006, saving model to checkPoint/Weights-002--0.66006.hdf5\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.6602 - accuracy: 0.6400 - val_loss: 0.6382 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.66006 to 0.63816, saving model to checkPoint/Weights-003--0.63816.hdf5\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 0.6359 - accuracy: 0.6400 - val_loss: 0.6184 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.63816 to 0.61842, saving model to checkPoint/Weights-004--0.61842.hdf5\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 0.6321 - accuracy: 0.6400 - val_loss: 0.5959 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.61842 to 0.59589, saving model to checkPoint/Weights-005--0.59589.hdf5\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 335ms/step - loss: 0.5646 - accuracy: 0.6400 - val_loss: 0.5676 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.59589 to 0.56759, saving model to checkPoint/Weights-006--0.56759.hdf5\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 0.6132 - accuracy: 0.6400 - val_loss: 0.5364 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.56759 to 0.53644, saving model to checkPoint/Weights-007--0.53644.hdf5\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 0.5656 - accuracy: 0.6800 - val_loss: 0.5008 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.53644 to 0.50082, saving model to checkPoint/Weights-008--0.50082.hdf5\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 340ms/step - loss: 0.5036 - accuracy: 0.6800 - val_loss: 0.4596 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.50082 to 0.45965, saving model to checkPoint/Weights-009--0.45965.hdf5\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 0.4676 - accuracy: 0.7600 - val_loss: 0.4168 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.45965 to 0.41684, saving model to checkPoint/Weights-010--0.41684.hdf5\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 342ms/step - loss: 0.4490 - accuracy: 0.7200 - val_loss: 0.3813 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.41684 to 0.38132, saving model to checkPoint/Weights-011--0.38132.hdf5\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 0.4857 - accuracy: 0.6800 - val_loss: 0.3387 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.38132 to 0.33866, saving model to checkPoint/Weights-012--0.33866.hdf5\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 0.3795 - accuracy: 0.8400 - val_loss: 0.2939 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.33866 to 0.29387, saving model to checkPoint/Weights-013--0.29387.hdf5\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 0.4609 - accuracy: 0.7600 - val_loss: 0.2457 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.29387 to 0.24575, saving model to checkPoint/Weights-014--0.24575.hdf5\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 0.3267 - accuracy: 0.9200 - val_loss: 0.1979 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.24575 to 0.19794, saving model to checkPoint/Weights-015--0.19794.hdf5\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 0.4311 - accuracy: 0.8400 - val_loss: 0.1626 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.19794 to 0.16258, saving model to checkPoint/Weights-016--0.16258.hdf5\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 0.2567 - accuracy: 0.8800 - val_loss: 0.1357 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.16258 to 0.13567, saving model to checkPoint/Weights-017--0.13567.hdf5\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 0.2684 - accuracy: 0.9200 - val_loss: 0.1588 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.13567\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 329ms/step - loss: 0.2344 - accuracy: 0.8800 - val_loss: 0.2029 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.13567\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 0.2686 - accuracy: 0.8000 - val_loss: 0.1826 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.13567\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.2950 - accuracy: 0.7500\n",
            "\u001b[1mconfusion matrix (test / validation)\u001b[0;0m\n",
            "\u001b[1mtrue positive:  \u001b[0;0m2\n",
            "\u001b[1mfalse positive: \u001b[0;0m1\n",
            "\u001b[1mtrue negative:  \u001b[0;0m1\n",
            "\u001b[1mfalse negative: \u001b[0;0m0\n",
            "\n",
            "\n",
            "\u001b[1maccuracy:  \u001b[0;0m75.0000 %\n",
            "\u001b[1mprecision: \u001b[0;0m66.6667 %\n",
            "\u001b[1mrecall:  \u001b[0;0m100.0000 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqWblvOXe2_4",
        "outputId": "edf2e404-d018-43b2-a156-d99f6de58b8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bin\t    dev    media\t\t run\t\t    tools\n",
            "boot\t    etc    mnt\t\t\t sbin\t\t    trainDataNegative\n",
            "checkPoint  home   moirePattern3CNN_.h5  srv\t\t    trainDataPositive\n",
            "chl.jpg     lib    opt\t\t\t sys\t\t    usr\n",
            "content     lib32  proc\t\t\t tensorflow-1.15.2  var\n",
            "datalab     lib64  root\t\t\t tmp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWnNYpcTqa_O"
      },
      "source": [
        "## Test CNN Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV0rGdyIueOf"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import argparse\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from skimage import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#constants\n",
        "width = 500#384 #change dimensions according to the input image in the training\n",
        "height = 375#512 #change dimensions according to the input image in the training\n",
        "depth = 1\n",
        "num_classes = 2\n",
        "\n",
        "positiveTestImagePath = './testDataPositive'\n",
        "negativeTestImagePath = './testDataNegative'\n",
        "    \n",
        "def mainTest(weightsFile, positiveTestImages, negativeTestImages):\n",
        "    weights_file = (weightsFile)\n",
        "    positiveImagePath = (positiveTestImages)\n",
        "    negativeImagePath = (negativeTestImages)\n",
        "\n",
        "    print(positiveImagePath)\n",
        "    print(negativeImagePath)\n",
        "    print(positiveTestImagePath)\n",
        "    print(negativeTestImagePath)\n",
        "    \n",
        "    mainAugment(positiveImagePath, negativeImagePath, 1)\n",
        "    X_LL, X_LH, X_HL, X_HH, X_index, Y, imageCount = readWaveletData(positiveImagePath, negativeImagePath, positiveTestImagePath, negativeTestImagePath)\n",
        "    \n",
        "    X_LL = np.array(X_LL)\n",
        "    X_LL = X_LL.reshape((imageCount, height, width, depth))\n",
        "    X_LH = np.array(X_LH)\n",
        "    X_LH = X_LH.reshape((imageCount, height, width, depth))\n",
        "    X_HL = np.array(X_HL)\n",
        "    X_HL = X_HL.reshape((imageCount, height, width, depth))\n",
        "    X_HH = np.array(X_HH)\n",
        "    X_HH = X_HH.reshape((imageCount, height, width, depth))\n",
        "    \n",
        "    CNN_model = createModel(height, width, depth, num_classes)\n",
        "    CNN_model.load_weights(weights_file)\n",
        "    evaluate(CNN_model,X_LL,X_LH,X_HL,X_HH, Y)\n",
        "\n",
        "\n",
        "\n",
        "def run(model, X_LL_test,X_LH_test,X_HL_test,y_test):\n",
        "    return\n",
        "\n",
        "\n",
        "def parse_arguments(argv):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    \n",
        "    parser.add_argument('weightsFile', type=str, help='saved CNN model file')\n",
        "    \n",
        "    parser.add_argument('positiveTestImages', type=str, help='Directory with positive (Moiré pattern) images.')\n",
        "    parser.add_argument('negativeTestImages', type=str, help='Directory with negative (Normal) images.')\n",
        "    \n",
        "    \n",
        "    return parser.parse_args(argv)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQxLk-Y4fJT6",
        "outputId": "6360e8de-8324-4ff4-9321-b95e4390bbc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bin\t    etc    moirePattern3CNN_.h5  sys\t\t    trainDataPositive\n",
            "boot\t    home   opt\t\t\t tensorflow-1.15.2  usr\n",
            "checkPoint  lib    proc\t\t\t testDataNegative   var\n",
            "chl.jpg     lib32  root\t\t\t testDataPositive\n",
            "content     lib64  run\t\t\t tmp\n",
            "datalab     media  sbin\t\t\t tools\n",
            "dev\t    mnt    srv\t\t\t trainDataNegative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYSPiTtfqa_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ccdfc4a-01ee-4d42-f1cf-b42169b5e03e"
      },
      "source": [
        "weightsFile = \"moirePattern3CNN_.h5\"\n",
        "    \n",
        "    \n",
        "mainTest(weightsFile, positiveImagePath, negativeImagePath)\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Moire/preaugmentedPositiveImages\n",
            "/content/drive/MyDrive/Moire/preaugmentedNegativeImages\n",
            "./testDataPositive\n",
            "./testDataNegative\n",
            "positive samples: 4\n",
            "negative samples: 4\n",
            "training image rotated\n",
            "training image rotated\n",
            "training image rotated\n",
            "Total positive files after augmentation:  12\n",
            "Total negative files after augmentation:  12\n",
            "positive samples: 16\n",
            "negative samples: 16\n",
            "positive data loaded.\n",
            "negative data loaded.\n",
            "Total Samples Loaded:  36\n",
            "\u001b[1mconfusion matrix (test / validation)\u001b[0;0m\n",
            "\u001b[1mtrue positive:  \u001b[0;0m19\n",
            "\u001b[1mfalse positive: \u001b[0;0m1\n",
            "\u001b[1mtrue negative:  \u001b[0;0m11\n",
            "\u001b[1mfalse negative: \u001b[0;0m1\n",
            "\n",
            "\n",
            "\u001b[1maccuracy:  \u001b[0;0m93.7500 %\n",
            "\u001b[1mprecision: \u001b[0;0m95.0000 %\n",
            "\u001b[1mrecall:  \u001b[0;0m95.0000 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sFMSjCjfe27",
        "outputId": "a8b26439-4ede-4130-a232-c9398d6539a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls testDataNegative\n",
        "!ls testDataPositive\n",
        "!ls trainDataNegative\n",
        "!ls trainDataPositive"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0_letterbox1024_180_FLIP_HH.tiff   22_letterbox1024_180_FLIP_HH.tiff\n",
            "0_letterbox1024_180_FLIP_HL.tiff   22_letterbox1024_180_FLIP_HL.tiff\n",
            "0_letterbox1024_180_FLIP_LH.tiff   22_letterbox1024_180_FLIP_LH.tiff\n",
            "0_letterbox1024_180_FLIP_LL.tiff   22_letterbox1024_180_FLIP_LL.tiff\n",
            "0_letterbox1024_180_HH.tiff\t   22_letterbox1024_180_HH.tiff\n",
            "0_letterbox1024_180_HL.tiff\t   22_letterbox1024_180_HL.tiff\n",
            "0_letterbox1024_180_LH.tiff\t   22_letterbox1024_180_LH.tiff\n",
            "0_letterbox1024_180_LL.tiff\t   22_letterbox1024_180_LL.tiff\n",
            "0_letterbox1024_HH.tiff\t\t   22_letterbox1024_HH.tiff\n",
            "0_letterbox1024_HL.tiff\t\t   22_letterbox1024_HL.tiff\n",
            "0_letterbox1024_LH.tiff\t\t   22_letterbox1024_LH.tiff\n",
            "0_letterbox1024_LL.tiff\t\t   22_letterbox1024_LL.tiff\n",
            "14_letterbox1024_180_FLIP_HH.tiff  37_letterbox1024_180_FLIP_HH.tiff\n",
            "14_letterbox1024_180_FLIP_HL.tiff  37_letterbox1024_180_FLIP_HL.tiff\n",
            "14_letterbox1024_180_FLIP_LH.tiff  37_letterbox1024_180_FLIP_LH.tiff\n",
            "14_letterbox1024_180_FLIP_LL.tiff  37_letterbox1024_180_FLIP_LL.tiff\n",
            "14_letterbox1024_180_HH.tiff\t   37_letterbox1024_180_HH.tiff\n",
            "14_letterbox1024_180_HL.tiff\t   37_letterbox1024_180_HL.tiff\n",
            "14_letterbox1024_180_LH.tiff\t   37_letterbox1024_180_LH.tiff\n",
            "14_letterbox1024_180_LL.tiff\t   37_letterbox1024_180_LL.tiff\n",
            "14_letterbox1024_HH.tiff\t   37_letterbox1024_HH.tiff\n",
            "14_letterbox1024_HL.tiff\t   37_letterbox1024_HL.tiff\n",
            "14_letterbox1024_LH.tiff\t   37_letterbox1024_LH.tiff\n",
            "14_letterbox1024_LL.tiff\t   37_letterbox1024_LL.tiff\n",
            "284_letterbox1024_180_FLIP_HH.tiff  355_letterbox1024_180_FLIP_HH.tiff\n",
            "284_letterbox1024_180_FLIP_HL.tiff  355_letterbox1024_180_FLIP_HL.tiff\n",
            "284_letterbox1024_180_FLIP_LH.tiff  355_letterbox1024_180_FLIP_LH.tiff\n",
            "284_letterbox1024_180_FLIP_LL.tiff  355_letterbox1024_180_FLIP_LL.tiff\n",
            "284_letterbox1024_180_HH.tiff\t    355_letterbox1024_180_HH.tiff\n",
            "284_letterbox1024_180_HL.tiff\t    355_letterbox1024_180_HL.tiff\n",
            "284_letterbox1024_180_LH.tiff\t    355_letterbox1024_180_LH.tiff\n",
            "284_letterbox1024_180_LL.tiff\t    355_letterbox1024_180_LL.tiff\n",
            "284_letterbox1024_HH.tiff\t    355_letterbox1024_HH.tiff\n",
            "284_letterbox1024_HL.tiff\t    355_letterbox1024_HL.tiff\n",
            "284_letterbox1024_LH.tiff\t    355_letterbox1024_LH.tiff\n",
            "284_letterbox1024_LL.tiff\t    355_letterbox1024_LL.tiff\n",
            "350_letterbox1024_180_FLIP_HH.tiff  chl_180_FLIP_HH.tiff\n",
            "350_letterbox1024_180_FLIP_HL.tiff  chl_180_FLIP_HL.tiff\n",
            "350_letterbox1024_180_FLIP_LH.tiff  chl_180_FLIP_LH.tiff\n",
            "350_letterbox1024_180_FLIP_LL.tiff  chl_180_FLIP_LL.tiff\n",
            "350_letterbox1024_180_HH.tiff\t    chl_180_HH.tiff\n",
            "350_letterbox1024_180_HL.tiff\t    chl_180_HL.tiff\n",
            "350_letterbox1024_180_LH.tiff\t    chl_180_LH.tiff\n",
            "350_letterbox1024_180_LL.tiff\t    chl_180_LL.tiff\n",
            "350_letterbox1024_HH.tiff\t    chl_HH.tiff\n",
            "350_letterbox1024_HL.tiff\t    chl_HL.tiff\n",
            "350_letterbox1024_LH.tiff\t    chl_LH.tiff\n",
            "350_letterbox1024_LL.tiff\t    chl_LL.tiff\n",
            "0_letterbox1024_180_FLIP_HH.tiff   22_letterbox1024_180_FLIP_HH.tiff\n",
            "0_letterbox1024_180_FLIP_HL.tiff   22_letterbox1024_180_FLIP_HL.tiff\n",
            "0_letterbox1024_180_FLIP_LH.tiff   22_letterbox1024_180_FLIP_LH.tiff\n",
            "0_letterbox1024_180_FLIP_LL.tiff   22_letterbox1024_180_FLIP_LL.tiff\n",
            "0_letterbox1024_180_HH.tiff\t   22_letterbox1024_180_HH.tiff\n",
            "0_letterbox1024_180_HL.tiff\t   22_letterbox1024_180_HL.tiff\n",
            "0_letterbox1024_180_LH.tiff\t   22_letterbox1024_180_LH.tiff\n",
            "0_letterbox1024_180_LL.tiff\t   22_letterbox1024_180_LL.tiff\n",
            "0_letterbox1024_HH.tiff\t\t   22_letterbox1024_HH.tiff\n",
            "0_letterbox1024_HL.tiff\t\t   22_letterbox1024_HL.tiff\n",
            "0_letterbox1024_LH.tiff\t\t   22_letterbox1024_LH.tiff\n",
            "0_letterbox1024_LL.tiff\t\t   22_letterbox1024_LL.tiff\n",
            "14_letterbox1024_180_FLIP_HH.tiff  37_letterbox1024_180_FLIP_HH.tiff\n",
            "14_letterbox1024_180_FLIP_HL.tiff  37_letterbox1024_180_FLIP_HL.tiff\n",
            "14_letterbox1024_180_FLIP_LH.tiff  37_letterbox1024_180_FLIP_LH.tiff\n",
            "14_letterbox1024_180_FLIP_LL.tiff  37_letterbox1024_180_FLIP_LL.tiff\n",
            "14_letterbox1024_180_HH.tiff\t   37_letterbox1024_180_HH.tiff\n",
            "14_letterbox1024_180_HL.tiff\t   37_letterbox1024_180_HL.tiff\n",
            "14_letterbox1024_180_LH.tiff\t   37_letterbox1024_180_LH.tiff\n",
            "14_letterbox1024_180_LL.tiff\t   37_letterbox1024_180_LL.tiff\n",
            "14_letterbox1024_HH.tiff\t   37_letterbox1024_HH.tiff\n",
            "14_letterbox1024_HL.tiff\t   37_letterbox1024_HL.tiff\n",
            "14_letterbox1024_LH.tiff\t   37_letterbox1024_LH.tiff\n",
            "14_letterbox1024_LL.tiff\t   37_letterbox1024_LL.tiff\n",
            "284_letterbox1024_180_FLIP_HH.tiff  355_letterbox1024_180_FLIP_HH.tiff\n",
            "284_letterbox1024_180_FLIP_HL.tiff  355_letterbox1024_180_FLIP_HL.tiff\n",
            "284_letterbox1024_180_FLIP_LH.tiff  355_letterbox1024_180_FLIP_LH.tiff\n",
            "284_letterbox1024_180_FLIP_LL.tiff  355_letterbox1024_180_FLIP_LL.tiff\n",
            "284_letterbox1024_180_HH.tiff\t    355_letterbox1024_180_HH.tiff\n",
            "284_letterbox1024_180_HL.tiff\t    355_letterbox1024_180_HL.tiff\n",
            "284_letterbox1024_180_LH.tiff\t    355_letterbox1024_180_LH.tiff\n",
            "284_letterbox1024_180_LL.tiff\t    355_letterbox1024_180_LL.tiff\n",
            "284_letterbox1024_HH.tiff\t    355_letterbox1024_HH.tiff\n",
            "284_letterbox1024_HL.tiff\t    355_letterbox1024_HL.tiff\n",
            "284_letterbox1024_LH.tiff\t    355_letterbox1024_LH.tiff\n",
            "284_letterbox1024_LL.tiff\t    355_letterbox1024_LL.tiff\n",
            "350_letterbox1024_180_FLIP_HH.tiff  chl_180_FLIP_HH.tiff\n",
            "350_letterbox1024_180_FLIP_HL.tiff  chl_180_FLIP_HL.tiff\n",
            "350_letterbox1024_180_FLIP_LH.tiff  chl_180_FLIP_LH.tiff\n",
            "350_letterbox1024_180_FLIP_LL.tiff  chl_180_FLIP_LL.tiff\n",
            "350_letterbox1024_180_HH.tiff\t    chl_180_HH.tiff\n",
            "350_letterbox1024_180_HL.tiff\t    chl_180_HL.tiff\n",
            "350_letterbox1024_180_LH.tiff\t    chl_180_LH.tiff\n",
            "350_letterbox1024_180_LL.tiff\t    chl_180_LL.tiff\n",
            "350_letterbox1024_HH.tiff\t    chl_HH.tiff\n",
            "350_letterbox1024_HL.tiff\t    chl_HL.tiff\n",
            "350_letterbox1024_LH.tiff\t    chl_LH.tiff\n",
            "350_letterbox1024_LL.tiff\t    chl_LL.tiff\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnwQH5eYqa_O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}