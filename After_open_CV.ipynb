{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Copy of Playground.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianhumphreys/Moire-Detector/blob/main/After_open_CV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDFp5O-Bqa_H"
      },
      "source": [
        "# Playground"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To3GCs0JndZT",
        "outputId": "bae4fc15-d124-4486-d3b8-0df0527d8e55"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jul 26 23:17:52 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5iT4R4PnYT6",
        "outputId": "397df04b-3910-4cb0-ac75-d346f44f8c86"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "muinrEgNqa_J"
      },
      "source": [
        "## Test 2D Wavelet Decomposition function\n",
        "This function(fwdHaarDWT2D) computes the 2D Wavelet Transform in the image. All the input images are passed through a Haar Wavelet Decomposition module, to get the LL, LH, HL and HHH component of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWlOXh6fq3xg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa73a384-45e9-4d09-aaa1-78d8960127df"
      },
      "source": [
        "from google.colab import drive                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsTZ_TLIv3TK",
        "outputId": "d758f1dc-407d-42cf-bb52-6ef5634b9466"
      },
      "source": [
        "!pip install pyheif whatimage"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyheif\n",
            "  Downloading pyheif-0.5.1-cp37-cp37m-manylinux2014_x86_64.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 3.2 MB/s \n",
            "\u001b[?25hCollecting whatimage\n",
            "  Downloading whatimage-0.0.3-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pyheif) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->pyheif) (2.20)\n",
            "Installing collected packages: whatimage, pyheif\n",
            "Successfully installed pyheif-0.5.1 whatimage-0.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO9ckef9Wrs3"
      },
      "source": [
        "## Normalized Raw data to 1000x750 pixel images\n",
        "Images are cropped to fit a 3:4 aspect ratio and then resized to match a 1000x750 size. Moves photos **unnormalized** -> **preaugmented**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xaN5wepXCy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a425af8-e8d4-4239-bb4c-ee362ba9d634"
      },
      "source": [
        "# importing the module\n",
        "\n",
        "\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "import io\n",
        "\n",
        "import whatimage\n",
        "import pyheif\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def decodeImageMetadata(bytesIo):\n",
        "\n",
        "  with open(bytesIo, 'rb') as f:\n",
        "    data = f.read()\n",
        "    fmt = whatimage.identify_image(data)\n",
        "    print(fmt)\n",
        "    if fmt in ['heic', 'avif']:\n",
        "      i = pyheif.read_heif(bytesIo)\n",
        "\n",
        "      # Extract metadata etc\n",
        "      print(i.metadata)\n",
        "      for metadata in i.metadata or []:\n",
        "        if metadata['type']=='Exif':\n",
        "          print('exif:')\n",
        "        print(metadata)\n",
        "        \n",
        "      # Convert to other file format like jpeg\n",
        "      s = io.BytesIO()\n",
        "      pi = Image.frombytes(mode=i.mode, size=i.size, data=i.data)\n",
        "\n",
        "      pi.save(s, format=\"jpeg\")\n",
        "\n",
        "def decodeImage(bytesIo):\n",
        "  with open(bytesIo, 'rb') as f:\n",
        "    data = f.read()\n",
        "    fmt = whatimage.identify_image(data)\n",
        "    if fmt in ['heic', 'avif']:\n",
        "      i = pyheif.read_heif(data)\n",
        "      pi = Image.frombytes(mode=i.mode, size=i.size, data=i.data)\n",
        "      pi.save(\"heic.jpg\", format=\"jpeg\")\n",
        "\n",
        "def read_heic(path: str):\n",
        "    img = Wimage(path)\n",
        "    img.format = 'jpg'\n",
        "    img.save(filename=\"heic.jpg\")\n",
        "    img.close()\n",
        "\n",
        "def openImage(fileName):\n",
        "  # decodeImageMetadata(fileName)\n",
        "  decodeImage(fileName)\n",
        "  return Image.open(\"heic.jpg\")\n",
        "\n",
        "def cropAndSave(image, fileName):\n",
        "    width = image.size[0]\n",
        "    height = image.size[1]\n",
        "\n",
        "    aspect = width / float(height)\n",
        "\n",
        "    if (height > width):\n",
        "        image = image.rotate(90, Image.NEAREST, expand=1)\n",
        "        width = image.size[0]\n",
        "        height = image.size[1]\n",
        "\n",
        "    ideal_width = 1000\n",
        "    ideal_height = 750\n",
        "\n",
        "    ideal_aspect = ideal_width / float(ideal_height)\n",
        "\n",
        "    if aspect > ideal_aspect:\n",
        "        # Then crop the left and right edges:\n",
        "        new_width = int(ideal_aspect * height)\n",
        "        offset = (width - new_width) / 2\n",
        "        resize = (offset, 0, width - offset, height)\n",
        "    else:\n",
        "        # ... crop the top and bottom:\n",
        "        new_height = int(width / ideal_aspect)\n",
        "        offset = (height - new_height) / 2\n",
        "        resize = (0, offset, width, height - offset)\n",
        "\n",
        "    thumb = image.crop(resize).resize((ideal_width, ideal_height), Image.ANTIALIAS)\n",
        "    thumb.save(fileName)\n",
        "\n",
        "\n",
        "\n",
        "def normalizeRawImages(superpoch, dataSetNumber):\n",
        "\n",
        "  print('##### NORMALIZING - superpoch: ' + superpoch + ' - dataset: ' + dataSetNumber)\n",
        "\n",
        "  negativeFromDir = '/content/drive/MyDrive/Moire/unnormalized/negative/' + dataSetNumber + '/'\n",
        "  positiveFromDir = '/content/drive/MyDrive/Moire/unnormalized/positive/' + dataSetNumber + '/'\n",
        "  negativeToDir = '/content/drive/MyDrive/Moire/preaugmented/negative/' + dataSetNumber + '/'\n",
        "  positiveToDir = '/content/drive/MyDrive/Moire/preaugmented/positive/' + dataSetNumber + '/'\n",
        "\n",
        "  if not os.path.exists(positiveToDir):\n",
        "      os.makedirs(positiveToDir)\n",
        "  if not os.path.exists(negativeToDir):\n",
        "      os.makedirs(negativeToDir)\n",
        "\n",
        "  if len([name for name in os.listdir(positiveToDir)]) > 0:\n",
        "    print('Directory ' + positiveToDir + ' already has normalized photos in it.  Skipping normalization for negative photos.')\n",
        "  if len([name for name in os.listdir(negativeToDir)]) > 0:\n",
        "    print('Directory ' + negativeToDir + ' already has normalized photos in it.  Skipping normalization for positive photos.')\n",
        "\n",
        "  positiveImageFiles = [f for f in listdir(positiveFromDir) if (isfile(join(positiveFromDir, f)))]\n",
        "  negativeImageFiles = [f for f in listdir(negativeFromDir) if (isfile(join(negativeFromDir, f)))]\n",
        "\n",
        "  # LLList = [l for l in positiveImageFiles if 'LLL' in l]\n",
        "  # print(LLList)\n",
        "\n",
        "  if len([name for name in os.listdir(positiveToDir)]) <= 0:\n",
        "    for f in positiveImageFiles:\n",
        "        print(join(positiveFromDir, f))\n",
        "        img = openImage(join(positiveFromDir, f))\n",
        "\n",
        "        rgb_im = img.convert(\"RGB\")\n",
        "        components = f.split('.')\n",
        "        newComponents = components[:len(components) - 1]\n",
        "        newComponents.append('png')\n",
        "        newFileName = '.'.join(newComponents)\n",
        "\n",
        "        cropAndSave(rgb_im, join(positiveToDir, newFileName))\n",
        "\n",
        "  if len([name for name in os.listdir(negativeToDir)]) <= 0:\n",
        "    for f in negativeImageFiles:\n",
        "        print(join(negativeFromDir, f))\n",
        "        img = Image.open(join(negativeFromDir, f))\n",
        "\n",
        "        rgb_im = img.convert(\"RGB\")\n",
        "        components = f.split('.')\n",
        "        newComponents = components[:len(components) - 1]\n",
        "        newComponents.append('png')\n",
        "        newFileName = '.'.join(newComponents)\n",
        "\n",
        "        cropAndSave(rgb_im, join(negativeToDir, newFileName))\n",
        "\n",
        "# normalizeRawImages(\"001\", \"005\")\n",
        "# normalizeRawImages(\"001\", \"006\")  \n",
        "# normalizeRawImages(\"001\", \"007\")\n",
        "# normalizeRawImages(\"001\", \"008\") \n",
        "# normalizeRawImages(\"001\", \"009\")\n",
        "# normalizeRawImages(\"001\", \"010\")\n",
        "# normalizeRawImages(\"001\", \"011\") \n",
        "# normalizeRawImages(\"001\", \"012\") \n",
        "# normalizeRawImages(\"001\", \"013\") \n",
        "# normalizeRawImages(\"001\", \"014\") \n",
        "# normalizeRawImages(\"001\", \"015\") \n",
        "# normalizeRawImages(\"001\", \"016\") \n",
        "# normalizeRawImages(\"001\", \"017\") \n",
        "normalizeRawImages(\"001\", \"026\") \n",
        "# normalizeRawImages(\"001\", \"019\") \n",
        "# normalizeRawImages(\"001\", \"020\") \n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##### NORMALIZING - superpoch: 001 - dataset: 026\n",
            "Directory /content/drive/MyDrive/Moire/preaugmented/positive/026/ already has normalized photos in it.  Skipping normalization for negative photos.\n",
            "Directory /content/drive/MyDrive/Moire/preaugmented/negative/026/ already has normalized photos in it.  Skipping normalization for positive photos.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNnfNmoEqa_M"
      },
      "source": [
        "## Augment normalized data\n",
        "The training images need to be put in two folders. positiveImages and negativeImages. positiveImages are the images which are captured from the display devices and has the presence of stron or weak Moiré patterms in it.\n",
        "negativeImages are the ones without Moiré Patterns (i.e. the images which are not captured from the display devices).  Moves photos **preaugmented** -> **train/test** based on a split ratio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9o2VLHoqyOo"
      },
      "source": [
        "#This function(fwdHaarDWT2D) computes the 2D Wavelet Transform in the image. All the input images are passed through a Haar Wavelet Decomposition module, to get the LL, LH, HL and HHH component of the image\n",
        "\n",
        "import numpy as np\n",
        "import pywt\n",
        "\n",
        "def splitFreqBands(img, levRows, levCols):\n",
        "    halfRow = int(levRows/2)\n",
        "    halfCol = int(levCols/2)\n",
        "    LL = img[0:halfRow, 0:halfCol]\n",
        "    LH = img[0:halfRow, halfCol:levCols]\n",
        "    HL = img[halfRow:levRows, 0:halfCol]\n",
        "    HH = img[halfRow:levRows, halfCol:levCols]\n",
        "    \n",
        "    return LL, LH, HL, HH\n",
        "    \n",
        "def haarDWT1D(data, length):\n",
        "    avg0 = 0.5;\n",
        "    avg1 = 0.5;\n",
        "    dif0 = 0.5;\n",
        "    dif1 = -0.5;\n",
        "    temp = np.empty_like(data)\n",
        "    temp = temp.astype(float)\n",
        "    h = int(length/2)\n",
        "    for i in range(h):\n",
        "        k = i*2\n",
        "        temp[i] = data[k] * avg0 + data[k + 1] * avg1;\n",
        "        temp[i + h] = data[k] * dif0 + data[k + 1] * dif1;\n",
        "    \n",
        "    data[:] = temp\n",
        "\n",
        "# computes the homography coefficients for PIL.Image.transform using point correspondences\n",
        "def fwdHaarDWT2D(img):\n",
        "    img = np.array(img)\n",
        "    levRows = img.shape[0];\n",
        "    levCols = img.shape[1];\n",
        "    img = img.astype(float)\n",
        "    for i in range(levRows):\n",
        "        row = img[i,:]\n",
        "        haarDWT1D(row, levCols)\n",
        "        img[i,:] = row\n",
        "    for j in range(levCols):\n",
        "        col = img[:,j]\n",
        "        haarDWT1D(col, levRows)\n",
        "        img[:,j] = col\n",
        "        \n",
        "    return splitFreqBands(img, levRows, levCols)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a63KHmZYqa_K"
      },
      "source": [
        "# from PIL import Image\n",
        "# from matplotlib import pyplot as plt\n",
        "# !ls /content/drive/MyDrive/Moire/\n",
        "# img = Image.open('/content/drive/MyDrive/Moire/preaugmented/positive/001/IMG_2906.jpg').convert('L')\n",
        "# img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "# img.save('chl.jpg')\n",
        "# LL, LH, HL, HH = fwdHaarDWT2D(img)\n",
        "# fig, axes = plt.subplots(2, 2)\n",
        "# fig.tight_layout()\n",
        "# axes[0, 0].imshow(LL)\n",
        "# axes[0, 1].imshow(LH)\n",
        "# axes[1, 0].imshow(HL)\n",
        "# axes[1, 1].imshow(HH)\n",
        "# axes[0, 0].set_title(\"LL\")\n",
        "# axes[0, 1].set_title(\"LH\")\n",
        "# axes[1, 0].set_title(\"HL\")\n",
        "# axes[1, 1].set_title(\"HH\")\n",
        "# plt.show()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO8AKkFsqm5W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0eda6da-62b1-4dcc-e3fc-2823b2e37158"
      },
      "source": [
        "import sys\n",
        "import argparse\n",
        "from PIL import Image\n",
        "from PIL import ImageOps\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "\n",
        "#The training images need to be put in two folders. positiveImages and negativeImages. positiveImages are the images which are captured from the display devices and has the presence of stron or weak Moiré patterms in it. negativeImages are the ones without Moiré Patterns (i.e. the images which are not captured from the display devices)\n",
        "\n",
        "SPLIT_RATIO = 0.8\n",
        "\n",
        "def augmentNormalizedData(superpoch, dataSetNumber):\n",
        "\n",
        "  print('##### AUGMENTING - superpoch: ' + superpoch + ' - dataset: ' + dataSetNumber)\n",
        "\n",
        "  negativeFromDir = '/content/drive/MyDrive/Moire/preaugmented/negative/' + dataSetNumber + '/'\n",
        "  positiveFromDir = '/content/drive/MyDrive/Moire/preaugmented/positive/' + dataSetNumber + '/'\n",
        "\n",
        "  negativeToTrainDir = '/content/drive/MyDrive/Moire/train/negative/' + dataSetNumber + '/'\n",
        "  positiveToTrainDir = '/content/drive/MyDrive/Moire/train/positive/' + dataSetNumber + '/'\n",
        "  negativeToTestDir = '/content/drive/MyDrive/Moire/test/negative/' + dataSetNumber + '/'\n",
        "  positiveToTestDir = '/content/drive/MyDrive/Moire/test/positive/' + dataSetNumber + '/'\n",
        "\n",
        "  if not os.path.exists(negativeToTrainDir):\n",
        "      os.makedirs(negativeToTrainDir)\n",
        "  if not os.path.exists(positiveToTrainDir):\n",
        "      os.makedirs(positiveToTrainDir)\n",
        "  if not os.path.exists(negativeToTestDir):\n",
        "      os.makedirs(negativeToTestDir)\n",
        "  if not os.path.exists(positiveToTestDir):\n",
        "      os.makedirs(positiveToTestDir)\n",
        "\n",
        "  if len([name for name in os.listdir(negativeToTrainDir)]) > 0:\n",
        "    print('Directory ' + negativeToTrainDir + ' already has normalized photos in it.  Skipping augmentation for negative photos.')\n",
        "  if len([name for name in os.listdir(positiveToTrainDir)]) > 0:\n",
        "    print('Directory ' + positiveToTrainDir + ' already has normalized photos in it.  Skipping augmentation for positive photos.')\n",
        "  if len([name for name in os.listdir(negativeToTestDir)]) > 0:\n",
        "    print('Directory ' + negativeToTestDir + ' already has normalized photos in it.  Skipping augmentation for negative photos.')\n",
        "  if len([name for name in os.listdir(positiveToTestDir)]) > 0:\n",
        "    print('Directory ' + positiveToTestDir + ' already has normalized photos in it.  Skipping augmentation for positive photos.')\n",
        "        \n",
        "  createTrainingData(positiveFromDir, negativeFromDir, positiveToTrainDir, negativeToTrainDir, positiveToTestDir, negativeToTestDir)\n",
        "\n",
        "    \n",
        "#The wavelet decomposed images are the transformed images representing the spatial and the frequency information of the image. These images are stored as 'tiff' in the disk, to preserve that information. Each image is transformed with 180 degrees rotation and as well flipped, as part of data augmentation.\n",
        "\n",
        "def transformImageAndSave(image, f, customStr, path):\n",
        "    cA, cH, cV, cD  = fwdHaarDWT2D(image);\n",
        "    \n",
        "    fileName = (os.path.splitext(f)[0])\n",
        "    fLL = (f.replace(fileName, fileName+'_' + customStr + 'LL')).replace('.png','.tiff')\n",
        "    fLH = (f.replace(fileName, fileName+'_' + customStr + 'LH')).replace('.png','.tiff')\n",
        "    fHL = (f.replace(fileName, fileName+'_' + customStr + 'HL')).replace('.png','.tiff')\n",
        "    fHH = (f.replace(fileName, fileName+'_' + customStr + 'HH')).replace('.png','.tiff')\n",
        "\n",
        "    cA = Image.fromarray(cA)\n",
        "    cH = Image.fromarray(cH)\n",
        "    cV = Image.fromarray(cV)\n",
        "    cD = Image.fromarray(cD)\n",
        "\n",
        "    cA.save(join(path, fLL))\n",
        "    cH.save(join(path, fLH))\n",
        "    cV.save(join(path, fHL))\n",
        "    cD.save(join(path, fHH))\n",
        "    \n",
        "    \n",
        "def augmentAndTrasformImage(f, mainFolder, trainFolder):\n",
        "    try:\n",
        "        print(join(mainFolder, f))\n",
        "        img = Image.open(join(mainFolder, f)) \n",
        "    except:\n",
        "        print('Error: Couldnt read the file {}. Make sure only images are present in the folder'.format(f))\n",
        "        return None\n",
        "\n",
        "    imgGray = img.convert('L')\n",
        "    wdChk, htChk = imgGray.size\n",
        "    if htChk > wdChk:\n",
        "        imgGray = imgGray.rotate(-90, expand=1)\n",
        "        print('training image rotated')\n",
        "    transformImageAndSave(imgGray, f, '', trainFolder)\n",
        "\n",
        "    imgGray = imgGray.transpose(Image.ROTATE_180)\n",
        "    transformImageAndSave(imgGray, f, '180_', trainFolder)\n",
        "\n",
        "    imgGray = imgGray.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    transformImageAndSave(imgGray, f, '180_FLIP_', trainFolder)\n",
        "    \n",
        "    return True\n",
        "    \n",
        "    \n",
        "def createTrainingData(positiveFromDir, negativeFromDir, positiveToTrainDir, negativeToTrainDir, positiveToTestDir, negativeToTestDir):\n",
        "\n",
        "    print('positive image path: ' + positiveFromDir)\n",
        "    print('negative image path: ' + negativeFromDir)\n",
        "    \n",
        "\n",
        "    # get image files by classes\n",
        "    positiveImageFiles = [f for f in listdir(positiveFromDir) if (isfile(join(positiveFromDir, f)))]\n",
        "    negativeImageFiles = [f for f in listdir(negativeFromDir) if (isfile(join(negativeFromDir, f)))]\n",
        "\n",
        "    positiveDataBorder = round(len(positiveImageFiles) * SPLIT_RATIO)\n",
        "    negativeDataBorder = round(len(negativeImageFiles) * SPLIT_RATIO)\n",
        "\n",
        "    positiveTrainFiles = positiveImageFiles[:positiveDataBorder]\n",
        "    positiveTestFiles = positiveImageFiles[positiveDataBorder:]\n",
        "    negativeTrainFiles = negativeImageFiles[:negativeDataBorder]\n",
        "    negativeTestFiles = negativeImageFiles[negativeDataBorder:]\n",
        "\n",
        "    print('positive train samples: ' + str(len(positiveTrainFiles)))\n",
        "    print('negative train samples: ' + str(len(negativeTrainFiles)))\n",
        "    print('positive test samples: ' + str(len(positiveTestFiles)))\n",
        "    print('negative test samples: ' + str(len(negativeTestFiles)))\n",
        "\n",
        "    Knegative = 0\n",
        "    Kpositive = 0\n",
        "\n",
        "    # create positive training images\n",
        "    if len([name for name in os.listdir(positiveToTrainDir)]) <= 0:\n",
        "      for f in positiveTrainFiles:\n",
        "          ret = augmentAndTrasformImage(f, positiveFromDir, positiveToTrainDir)\n",
        "          if ret == None:\n",
        "              continue\n",
        "          Kpositive += 3\n",
        "\n",
        "    if len([name for name in os.listdir(negativeToTrainDir)]) <= 0:\n",
        "      # create positive test images\n",
        "      for f in negativeTrainFiles:\n",
        "          ret = augmentAndTrasformImage(f, negativeFromDir, negativeToTrainDir)\n",
        "          if ret == None:\n",
        "              continue\n",
        "          Kpositive += 3\n",
        "\n",
        "    if len([name for name in os.listdir(positiveToTestDir)]) <= 0:\n",
        "      # create negative training images\n",
        "      for f in positiveTestFiles:\n",
        "          ret = augmentAndTrasformImage(f, positiveFromDir, positiveToTestDir)\n",
        "          if ret == None:\n",
        "              continue\n",
        "          Knegative += 3;\n",
        "\n",
        "    if len([name for name in os.listdir(negativeToTestDir)]) <= 0:\n",
        "      # create negative training images\n",
        "      for f in negativeTestFiles:\n",
        "          ret = augmentAndTrasformImage(f, negativeFromDir, negativeToTestDir)\n",
        "          if ret == None:\n",
        "              continue\n",
        "          Knegative += 3;\n",
        "    #\n",
        "    # print('Total positive files after augmentation: ', Kpositive)\n",
        "    # print('Total negative files after augmentation: ', Knegative)\n",
        "    \n",
        "\n",
        "\n",
        "# mainAugment('/content/drive/MyDrive/Moire/preaugmentedPositiveImages', '/content/drive/MyDrive/Moire/preaugmentedNegativeImages')\n",
        "\n",
        "# augmentNormalizedData('001', '005')\n",
        "# augmentNormalizedData('001', '006')\n",
        "# augmentNormalizedData('001', '007')\n",
        "# augmentNormalizedData('001', '008')\n",
        "# augmentNormalizedData('001', '009')\n",
        "# augmentNormalizedData('001', '010')\n",
        "# augmentNormalizedData('001', '011')\n",
        "# augmentNormalizedData('001', '012')\n",
        "# augmentNormalizedData('001', '013')\n",
        "# augmentNormalizedData('001', '014')\n",
        "# augmentNormalizedData('001', '015')\n",
        "\n",
        "# augmentNormalizedData('001', '016')\n",
        "# augmentNormalizedData('001', '017')\n",
        "augmentNormalizedData('001', '026')\n",
        "# augmentNormalizedData('001', '019')\n",
        "# augmentNormalizedData('001', '020')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##### AUGMENTING - superpoch: 001 - dataset: 026\n",
            "Directory /content/drive/MyDrive/Moire/train/negative/026/ already has normalized photos in it.  Skipping augmentation for negative photos.\n",
            "Directory /content/drive/MyDrive/Moire/train/positive/026/ already has normalized photos in it.  Skipping augmentation for positive photos.\n",
            "Directory /content/drive/MyDrive/Moire/test/negative/026/ already has normalized photos in it.  Skipping augmentation for negative photos.\n",
            "Directory /content/drive/MyDrive/Moire/test/positive/026/ already has normalized photos in it.  Skipping augmentation for positive photos.\n",
            "positive image path: /content/drive/MyDrive/Moire/preaugmented/positive/026/\n",
            "negative image path: /content/drive/MyDrive/Moire/preaugmented/negative/026/\n",
            "positive train samples: 24\n",
            "negative train samples: 24\n",
            "positive test samples: 6\n",
            "negative test samples: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INPlfs7QQXYJ"
      },
      "source": [
        "## Load Data into Memory\n",
        "Get your big boy pants on because it's going to be a lot of data.  Increase runtime memory.  This section will load data from an **augmented** directory and loads it into a tensor for training or evaluation.  Recommended to make sure that there is no accelerator used so that it can be used when training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOTnUFotxJJJ"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import argparse\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from skimage import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#constants\n",
        "WIDTH = 500#384\n",
        "HEIGHT = 375#512\n",
        "\n",
        "#Here, we perform index based splitting and use those indices to split the our multi-input datasets. This is done because the CNN model is multi-input network\n",
        "# def splitTrainTestDataForBands(inputData, X_train_ind, X_test_ind):\n",
        "#     X_train = np.zeros((len(X_train_ind), WIDTH*HEIGHT))\n",
        "#     for i in range(len(X_train_ind)):\n",
        "#         X_train[i,:] = inputData[int(X_train_ind[i,0]),:]\n",
        "        \n",
        "#     X_test = np.zeros((len(X_test_ind), WIDTH*HEIGHT))\n",
        "#     for i in range(len(X_test_ind)):\n",
        "#         X_test[i,:] = inputData[int(X_test_ind[i,0]),:]\n",
        "        \n",
        "#     return X_train, X_test\n",
        "\n",
        "\n",
        "# def countPositiveSamplesAfterSplit(trainData):\n",
        "#     count = 0;\n",
        "#     for i in range(len(trainData)):\n",
        "#         if(trainData[i,0] == 0):\n",
        "#             count = count + 1\n",
        "#     return count\n",
        "\n",
        "def scaleData(inp, minimum, maximum):\n",
        "    minMaxScaler = preprocessing.MinMaxScaler(copy=True, feature_range=(minimum,maximum))\n",
        "    inp = inp.reshape(-1, 1)\n",
        "    inp = minMaxScaler.fit_transform(inp)\n",
        "    \n",
        "    return inp\n",
        "\n",
        "def readAndScaleImage(f, customStr, trainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, sampleVal):\n",
        "    fileName = (os.path.splitext(f)[0])\n",
        "\n",
        "    fLL = (f.replace(fileName, fileName + customStr + '_LL')).replace('.png','.tiff')\n",
        "    fLH = (f.replace(fileName, fileName + customStr + '_LH')).replace('.png','.tiff')\n",
        "    fHL = (f.replace(fileName, fileName + customStr + '_HL')).replace('.png','.tiff')\n",
        "    fHH = (f.replace(fileName, fileName + customStr + '_HH')).replace('.png','.tiff')\n",
        "\n",
        "    try:\n",
        "        imgLL = Image.open(join(trainImagePath, fLL))\n",
        "        imgLH = Image.open(join(trainImagePath, fLH))\n",
        "        imgHL = Image.open(join(trainImagePath, fHL))\n",
        "        imgHH = Image.open(join(trainImagePath, fHH))\n",
        "    except Exception as e:\n",
        "        print('Error: Couldnt read the file {}. Make sure only images are present in the folder'.format(fileName))\n",
        "        print('Exception:', e)\n",
        "        return None\n",
        "        \n",
        "    imgLL = np.array(imgLL)\n",
        "    imgLH = np.array(imgLH)\n",
        "    imgHL = np.array(imgHL)\n",
        "    imgHH = np.array(imgHH)\n",
        "\n",
        "\n",
        "    imgLL = scaleData(imgLL, 0, 1)\n",
        "    imgLH = scaleData(imgLH, -1, 1)\n",
        "    imgHL = scaleData(imgHL, -1, 1)\n",
        "    imgHH = scaleData(imgHH, -1, 1)\n",
        "\n",
        "    imgVector = imgLL.reshape(1, WIDTH*HEIGHT)\n",
        "    X_LL[sampleIndex, :] = imgVector\n",
        "    imgVector = imgLH.reshape(1, WIDTH*HEIGHT)\n",
        "    X_LH[sampleIndex, :] = imgVector\n",
        "    imgVector = imgHL.reshape(1, WIDTH*HEIGHT)\n",
        "    X_HL[sampleIndex, :] = imgVector\n",
        "    imgVector = imgHH.reshape(1, WIDTH*HEIGHT)\n",
        "    X_HH[sampleIndex, :] = imgVector\n",
        "    \n",
        "    Y[sampleIndex, 0] = sampleVal;\n",
        "    X_index[sampleIndex, 0] = sampleIndex;\n",
        "\n",
        "    imgVector = None\n",
        "    imgLL = None\n",
        "    imgLH = None\n",
        "    imgHL = None\n",
        "    imgHH = None\n",
        "    \n",
        "    return True\n",
        "\n",
        "def readAndScaleImageScramble(f, customStr, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, sampleVal):\n",
        "  \n",
        "    fileName = (os.path.splitext(f)[0])\n",
        "\n",
        "    fLL = (f.replace(fileName, fileName + customStr + '_LL')).replace('.png','.tiff')\n",
        "    fLH = (f.replace(fileName, fileName + customStr + '_LH')).replace('.png','.tiff')\n",
        "    fHL = (f.replace(fileName, fileName + customStr + '_HL')).replace('.png','.tiff')\n",
        "    fHH = (f.replace(fileName, fileName + customStr + '_HH')).replace('.png','.tiff')\n",
        "\n",
        "    try:\n",
        "        imgLL = Image.open(join(trainImagePath, fLL))\n",
        "        imgLH = Image.open(join(trainImagePath, fLH))\n",
        "        imgHL = Image.open(join(trainImagePath, fHL))\n",
        "        imgHH = Image.open(join(trainImagePath, fHH))\n",
        "    except Exception as e:\n",
        "        print('Error: Couldnt read the file {}. Make sure only images are present in the folder'.format(fileName))\n",
        "        print('Exception:', e)\n",
        "        return None\n",
        "        \n",
        "    imgLL = np.array(imgLL)\n",
        "    imgLH = np.array(imgLH)\n",
        "    imgHL = np.array(imgHL)\n",
        "    imgHH = np.array(imgHH)\n",
        "\n",
        "\n",
        "    imgLL = scaleData(imgLL, 0, 1)\n",
        "    imgLH = scaleData(imgLH, -1, 1)\n",
        "    imgHL = scaleData(imgHL, -1, 1)\n",
        "    imgHH = scaleData(imgHH, -1, 1)\n",
        "\n",
        "    imgVector = imgLL.reshape(1, WIDTH*HEIGHT)\n",
        "    X_LL[sampleIndex, :] = imgVector\n",
        "    imgVector = imgLH.reshape(1, WIDTH*HEIGHT)\n",
        "    X_LH[sampleIndex, :] = imgVector\n",
        "    imgVector = imgHL.reshape(1, WIDTH*HEIGHT)\n",
        "    X_HL[sampleIndex, :] = imgVector\n",
        "    imgVector = imgHH.reshape(1, WIDTH*HEIGHT)\n",
        "    X_HH[sampleIndex, :] = imgVector\n",
        "    \n",
        "    Y[sampleIndex, 0] = sampleVal;\n",
        "    X_index[sampleIndex, 0] = sampleIndex;\n",
        "\n",
        "    imgVector = None\n",
        "    imgLL = None\n",
        "    imgLH = None\n",
        "    imgHL = None\n",
        "    imgHH = None\n",
        "    \n",
        "    return True\n",
        "\n",
        "def reshapeData(X_LL, X_LH, X_HL, X_HH, X_index, Y, imageCount):\n",
        "\n",
        "    print('Dataset length: ' + str(len(X_LL)))\n",
        "    \n",
        "    print('num_train_samples', len(X_LL))\n",
        "    X_LL = np.array(X_LL)\n",
        "    X_LL = X_LL.reshape((len(X_LL), HEIGHT, WIDTH, 1))\n",
        "\n",
        "    X_LH = np.array(X_LH)\n",
        "    X_LH = X_LH.reshape((len(X_LH), HEIGHT, WIDTH, 1))\n",
        "\n",
        "    X_HL = np.array(X_HL)\n",
        "    X_HL = X_HL.reshape((len(X_HL), HEIGHT, WIDTH, 1))\n",
        "    \n",
        "    X_HH = np.array(X_HH)\n",
        "    X_HH = X_HH.reshape((len(X_HH), HEIGHT, WIDTH, 1))\n",
        "\n",
        "    Y = np.array(Y)\n",
        "    \n",
        "    return X_LL, X_LH, X_HL, X_HH, Y\n",
        "\n",
        "def readImageSet(imageFiles, trainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, bClass):\n",
        "\n",
        "    # print('FILES IN DATASET: ' + str(len(imageFiles)))\n",
        "    # print(imageFiles)\n",
        "    for f in imageFiles:\n",
        "        ret = readAndScaleImage(f, '', trainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, bClass)\n",
        "        if ret == True:\n",
        "            sampleIndex = sampleIndex + 1\n",
        "\n",
        "    for f in imageFiles:\n",
        "        ret = readAndScaleImage(f, '_180', trainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, bClass)\n",
        "        if ret == True:\n",
        "            sampleIndex = sampleIndex + 1\n",
        "\n",
        "    for f in imageFiles:\n",
        "        ret = readAndScaleImage(f, '_180_FLIP', trainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, bClass)\n",
        "        if ret == True:\n",
        "            sampleIndex = sampleIndex + 1\n",
        "\n",
        "    print('I AM HERE OK')\n",
        "    print(sampleIndex)\n",
        "    return sampleIndex\n",
        "\n",
        "\n",
        "def readImageSetScramble(absoluteImageFiles, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, bClass):\n",
        "\n",
        "    # print('FILES IN DATASET: ' + str(len(imageFiles)))\n",
        "    # print(imageFiles)\n",
        "    for f in absoluteImageFiles:\n",
        "        ret = readAndScaleImage(f, '', X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, bClass)\n",
        "        if ret == True:\n",
        "            sampleIndex = sampleIndex + 1\n",
        "\n",
        "    for f in absoluteImageFiles:\n",
        "        ret = readAndScaleImage(f, '_180', X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, bClass)\n",
        "        if ret == True:\n",
        "            sampleIndex = sampleIndex + 1\n",
        "\n",
        "    for f in absoluteImageFiles:\n",
        "        ret = readAndScaleImage(f, '_180_FLIP', X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, bClass)\n",
        "        if ret == True:\n",
        "            sampleIndex = sampleIndex + 1\n",
        "\n",
        "    print('I AM HERE OK')\n",
        "    print(sampleIndex)\n",
        "    return sampleIndex\n",
        "\n",
        "def readWaveletData(positiveImagePath, negativeImagePath, unaugmentedPositiveDir, unaugmentedNegativeDir, dataType):\n",
        "    \n",
        "    # get augmented, balanced training data image files by class\n",
        "    positiveImageFiles = [f for f in listdir(unaugmentedPositiveDir) if (isfile(join(unaugmentedPositiveDir, f)))]\n",
        "    negativeImageFiles = [f for f in listdir(unaugmentedNegativeDir) if (isfile(join(unaugmentedNegativeDir, f)))]\n",
        "\n",
        "    positiveDataBorder = round(len(positiveImageFiles) * SPLIT_RATIO)\n",
        "    negativeDataBorder = round(len(negativeImageFiles) * SPLIT_RATIO)\n",
        "\n",
        "    positiveReadFiles = []\n",
        "    negativeReadFiles = []\n",
        "\n",
        "    if dataType == 'train':\n",
        "      positiveReadFiles = positiveImageFiles[:positiveDataBorder]\n",
        "      negativeReadFiles = negativeImageFiles[:negativeDataBorder]\n",
        "    elif dataType == 'test':\n",
        "      positiveReadFiles = positiveImageFiles[positiveDataBorder:]\n",
        "      negativeReadFiles = negativeImageFiles[negativeDataBorder:]\n",
        "\n",
        "    positiveCount = len(positiveReadFiles)\n",
        "    negativeCount = len(negativeReadFiles)\n",
        "\n",
        "    print('positive samples: ' + str(positiveCount))\n",
        "    print('negative samples: ' + str(negativeCount))\n",
        "    imageCount = positiveCount + negativeCount\n",
        "    #intialization\n",
        "    X_LL = np.zeros(((positiveCount + negativeCount) * 3, WIDTH*HEIGHT))\n",
        "    X_LH = np.zeros(((positiveCount + negativeCount) * 3, WIDTH*HEIGHT))\n",
        "    X_HL = np.zeros(((positiveCount + negativeCount) * 3, WIDTH*HEIGHT))\n",
        "    X_HH = np.zeros(((positiveCount + negativeCount) * 3, WIDTH*HEIGHT))\n",
        "    X_index = np.zeros(((positiveCount + negativeCount) * 3, 1))\n",
        "    Y = np.zeros(((positiveCount + negativeCount) * 3, 1))\n",
        "\n",
        "\n",
        "    print(X_LL.shape)\n",
        "    print('HOW BIG IS THE DATASET: ' + str(len(X_LL)))\n",
        "    \n",
        "    sampleIndex = 0\n",
        "    # read all images, convert to float, divide by 255 (leads to gray range 0..1), reshape into a row vector\n",
        "    # write class 0 for positive and 1 for negative samples\n",
        "\n",
        "    sampleIndex = readImageSet(positiveReadFiles, positiveImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, 0)\n",
        "    print('positive data loaded.')\n",
        "    \n",
        "    sampleIndex = readImageSet(negativeReadFiles, negativeImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, 1)\n",
        "    print('negative data loaded.')\n",
        "\n",
        "    print('Total Samples Loaded: ', sampleIndex)\n",
        "\n",
        "    \n",
        "    print(Y.shape)\n",
        "    print(Y)\n",
        "    \n",
        "    X_LL, X_LH, X_HL, X_HH, Y = shuffle(X_LL, X_LH, X_HL, X_HH, Y, random_state=0)\n",
        "    \n",
        "    return X_LL, X_LH, X_HL, X_HH, X_index, Y, imageCount\n",
        "\n",
        "def mainReadDatafromDrive(dataSetNumber, dataType):\n",
        "\n",
        "    # dataType can be 'train' or 'test'\n",
        "\n",
        "    positiveFromTrainDir = '/content/drive/MyDrive/Moire/' + dataType + '/positive/' + dataSetNumber + '/'\n",
        "    negativeFromTrainDir = '/content/drive/MyDrive/Moire/' + dataType + '/negative/' + dataSetNumber + '/'\n",
        "    unaugmentedPositiveDir = '/content/drive/MyDrive/Moire/preaugmented/positive/' + dataSetNumber + '/'\n",
        "    unaugmentedNegativeDir = '/content/drive/MyDrive/Moire/preaugmented/negative/' + dataSetNumber + '/'\n",
        "    print(positiveFromTrainDir)\n",
        "    print(negativeFromTrainDir)\n",
        "\n",
        "    if not os.path.exists(positiveFromTrainDir):\n",
        "      print(\"ERROR: \" + positiveFromTrainDir + ' does not exist.  Exiting.')\n",
        "      raise ValueError('Directory does not exist')\n",
        "    if not os.path.exists(negativeFromTrainDir):\n",
        "      print(\"ERROR: \" + negativeFromTrainDir + ' does not exist.  Exiting.')\n",
        "      raise ValueError('Directory does not exist')\n",
        "    \n",
        "    X_LL, X_LH, X_HL, X_HH, X_index, Y, imageCount = readWaveletData(positiveFromTrainDir, negativeFromTrainDir, unaugmentedPositiveDir, unaugmentedNegativeDir, dataType)\n",
        "\n",
        "    \n",
        "    X_LL, X_LH, X_HL, X_HH, Y = reshapeData(X_LL, X_LH, X_HL, X_HH, X_index, Y, imageCount)\n",
        "\n",
        "    print(X_LL.shape)\n",
        "    print(X_LH.shape)\n",
        "    print(X_HL.shape)\n",
        "    print(X_HH.shape)\n",
        "    \n",
        "\n",
        "    return X_LL, X_LH, X_HL, X_HH, Y\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azf3IN-SRudm"
      },
      "source": [
        "## Train CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ASB86jIRxvK"
      },
      "source": [
        "import os\n",
        "\n",
        "from keras.models import Model # basic class for specifying and training a neural network\n",
        "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, Add, Multiply, Maximum\n",
        "\n",
        "def createModel(height, width, depth, num_classes):\n",
        "#     num_epochs = 20 # 50 26 200 # we iterate 200 times over the entire training set\n",
        "    kernel_size_1 = 7 # we will use 7x7 kernels \n",
        "    kernel_size_2 = 3 # we will use 3x3 kernels \n",
        "    pool_size = 2 # we will use 2x2 pooling throughout\n",
        "    conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n",
        "    conv_depth_2 = 16 # ...switching to 16 after the first pooling layer\n",
        "    drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
        "    drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
        "    hidden_size = 32 # 128 512 the FC layer will have 512 neurons\n",
        "\n",
        "\n",
        "    inpLL = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
        "    inpLH = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
        "    inpHL = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
        "    inpHH = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
        "    \n",
        "    conv_1_LL = Convolution2D(conv_depth_1, (kernel_size_1, kernel_size_1), padding='same', activation='relu')(inpLL)\n",
        "    conv_1_LH = Convolution2D(conv_depth_1, (kernel_size_1, kernel_size_1), padding='same', activation='relu')(inpLH)\n",
        "    conv_1_HL = Convolution2D(conv_depth_1, (kernel_size_1, kernel_size_1), padding='same', activation='relu')(inpHL)\n",
        "    conv_1_HH = Convolution2D(conv_depth_1, (kernel_size_1, kernel_size_1), padding='same', activation='relu')(inpHH)\n",
        "\n",
        "    pool_1_LL = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_1_LL)\n",
        "    pool_1_LH = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_1_LH)\n",
        "    pool_1_HL = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_1_HL)\n",
        "    pool_1_HH = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_1_HH)\n",
        "\n",
        "    avg_LH_HL_HH = Maximum()([pool_1_LH, pool_1_HL, pool_1_HH])\n",
        "    inp_merged = Multiply()([pool_1_LL, avg_LH_HL_HH])\n",
        "    print(inp_merged)\n",
        "    C4 = Convolution2D(conv_depth_2, (kernel_size_2, kernel_size_2), padding='same', activation='relu')(inp_merged)\n",
        "    S2 = MaxPooling2D(pool_size=(4, 4))(C4)\n",
        "    drop_1 = Dropout(drop_prob_1)(S2)\n",
        "    C5 = Convolution2D(conv_depth_1, (kernel_size_2, kernel_size_2), padding='same', activation='relu')(drop_1)\n",
        "    S3 = MaxPooling2D(pool_size=(pool_size, pool_size))(C5)\n",
        "    C6 = Convolution2D(conv_depth_1, (kernel_size_2, kernel_size_2), padding='same', activation='relu')(S3)\n",
        "    S4 = MaxPooling2D(pool_size=(pool_size, pool_size))(C6)\n",
        "    drop_2 = Dropout(drop_prob_1)(S4)\n",
        "    # Now flatten to 1D, apply FC -> ReLU (with dropout) -> softmax\n",
        "    flat = Flatten()(drop_2)\n",
        "    hidden = Dense(hidden_size, activation='relu')(flat)\n",
        "    drop_3 = Dropout(drop_prob_2)(hidden)\n",
        "    out = Dense(num_classes, activation='softmax')(drop_3)\n",
        "    \n",
        "    model = Model(inputs=[inpLL, inpLH, inpHL, inpHH], outputs=out) # To define a model, just specify its input and output layers\n",
        "    intermediate_model = Model(inputs=[inpLL, inpLH, inpHL, inpHH], outputs=inp_merged)\n",
        "    \n",
        "    return model, intermediate_model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTX2vudptIaO"
      },
      "source": [
        "#To detect Moire ́ patternzs, images are first decomposed using Wavelet decomposition (refer to file '') and trained using multi-input Convolutional neural network. The strength of the proposed CNN model is, it uses the LL intensity image (from the Wavelet decomposition) as a weight parameter for the Moire ́ pattern, thereby approximating the spatial spread of the Moire ́ pattern in the image. Usage of CNN model performs better than frequency thresholding approach as the model is trained considering diverse scenarios and it is able to distinguish between the high frequency of background texture and the Moire ́ pattern.\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import argparse\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from skimage import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import time\n",
        "from tensorflow import keras\n",
        "\n",
        "# - read positive and negative training data\n",
        "# - create X and Y from training data\n",
        "\n",
        "def trainMoireScramble(superpoch, numEpochs, weights_file = None):\n",
        "  mainReadDatafromDrive(dataSetNumber[0], 'train')\n",
        "\n",
        "def trainMoire(superpoch, dataSetNumber, numEpochs, weights_file = None):\n",
        "\n",
        "    print('##### TRAINING - superpoch: ' + superpoch + ' - dataset: ' + dataSetNumber[0] + ', ', dataSetNumber[1])\n",
        "\n",
        "    X_LL, X_LH, X_HL, X_HH, Y = mainReadDatafromDrive(dataSetNumber[0], 'train')\n",
        "\n",
        "    # print(\"X_LL\")\n",
        "    # print(X_LL)\n",
        "\n",
        "    # print(\"X_LH\")\n",
        "    # print(X_LH)\n",
        "\n",
        "    # print(\"X_HL\")\n",
        "    # print(X_HL)\n",
        "\n",
        "    # print(\"X_HH\")\n",
        "    # print(X_HH)\n",
        "\n",
        "    X_LL_2, X_LH_2, X_HL_2, X_HH_2, Y_2 = mainReadDatafromDrive(dataSetNumber[1], 'train')\n",
        "\n",
        "\n",
        "    print('Concatenating 2 datasets.')\n",
        "    print(X_LL.shape)\n",
        "    print(X_LL_2.shape)\n",
        "    X_LL = np.concatenate((X_LL, X_LL_2), axis=0)\n",
        "    X_LH = np.concatenate((X_LH, X_LH_2), axis=0)\n",
        "    X_HL = np.concatenate((X_HL, X_HL_2), axis=0)\n",
        "    X_HH = np.concatenate((X_HH, X_HH_2), axis=0)\n",
        "    Y = np.concatenate((Y, Y_2), axis=0)\n",
        "    X_LL_2 = None\n",
        "    X_LH_2 = None\n",
        "    X_HL_2 = None\n",
        "    X_HH_2 = None\n",
        "    Y_2 = None\n",
        "\n",
        "    modelName = trainCNNModel(superpoch, dataSetNumber, X_LL, X_LH, X_HL, X_HH, Y, numEpochs, weights_file)\n",
        "    \n",
        "    X_LL = None\n",
        "    X_LH = None\n",
        "    X_HL = None \n",
        "    X_HH = None\n",
        "    Y = None\n",
        "\n",
        "    return modelName\n",
        "    # evaluate(model, X_LL_test,X_LH_test,X_HL_test,X_HH_test,Y_test)\n",
        "    \n",
        "\n",
        "def trainCNNModel(superpoch, dataSetNumber, X_LL_train, X_LH_train, X_HL_train, X_HH_train, y_train, num_epochs, weights_file):\n",
        "\n",
        "    batch_size = 32 # in each iteration, we consider 32 training examples at once\n",
        "    print(\"SHAPE\")\n",
        "    print(X_LL_train.shape);\n",
        "    num_train, height, width, depth = X_LL_train.shape\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    Y_train = np_utils.to_categorical(y_train, num_classes) # One-hot encode the labels\n",
        "    # Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels\n",
        "\n",
        "    checkPointFolder = '/content/drive/MyDrive/Moire/checkPoint'\n",
        "    checkpoint_name = checkPointFolder + '/mid-sp' + superpoch + '-ds' + dataSetNumber[0] + '_' + dataSetNumber[1] + '-ep{epoch:03d}-ls{val_loss:.5f}-ac{val_accuracy:.2f}-weights' \n",
        "    checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "    callbacks_list = [checkpoint]\n",
        "    \n",
        "    if not os.path.exists(checkPointFolder):\n",
        "        os.makedirs(checkPointFolder)\n",
        "        \n",
        "        \n",
        "    model = None\n",
        "    # if preloaded_model == None:\n",
        "    #   print('A preloaded model was not provided.  Creating a new one')\n",
        "    #   model, _ = createModel(height, width, depth, num_classes)\n",
        "\n",
        "    if weights_file != None:\n",
        "      print('--- Loading weights file: ' + weights_file)\n",
        "      model = keras.models.load_model(weights_file)\n",
        "    else:\n",
        "      print('--- No weights file provided.  Compiling a new one.')\n",
        "      \n",
        "      model, _ = createModel(height, width, depth, num_classes)\n",
        "      model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
        "                    optimizer='adam', # using the Adam optimiser\n",
        "                    metrics=['accuracy']) # reporting the accuracy\n",
        "      # if preloaded_model == None:\n",
        "        \n",
        "      # else:\n",
        "      #   print('--- Model already loaded with weights.  Skipping compilation')\n",
        "\n",
        "    model.fit([X_LL_train,X_LH_train,X_HL_train,X_HH_train], Y_train,                # Train the model using the training set...\n",
        "              batch_size=batch_size, epochs=num_epochs,\n",
        "              verbose=1, validation_split=0.1, callbacks=callbacks_list) # ...holding out 10% of the data for validation\n",
        "    # score, acc = model.evaluate([X_LL_test,X_LH_test,X_HL_test,X_HH_test], Y_test, verbose=1)  # Evaluate the trained model on the test set!\n",
        "\n",
        "    # print('------ weights ------')\n",
        "    # for i in range(len(model.layers)):\n",
        "    #   print(len(model.layers[i].get_weights()))\n",
        "\n",
        "    modelName = '/content/drive/MyDrive/Moire/checkPoint/final-tm' + str(time.time()).split('.')[0] + '-sp' + superpoch + '-ds' + dataSetNumber[0] + '_' + dataSetNumber[1] + '-weights'\n",
        "    model.save(modelName)\n",
        "    \n",
        "    return modelName\n",
        "\n",
        "\n",
        "def evaluate(model, X_LL_test,X_LH_test,X_HL_test,X_HH_test,y_test):\n",
        "\n",
        "    model_out = model.predict([X_LL_test,X_LH_test,X_HL_test,X_HH_test])\n",
        "    passCnt = 0\n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "\n",
        "    positive_confidence_threshold = 0.5\n",
        "    incorrect_threshold_count = 0\n",
        "    largest_fp = 0\n",
        "    smallest_tp = 2\n",
        "\n",
        "    for i in range(len(y_test)):\n",
        "        if np.argmax(model_out[i, :]) == y_test[i]:\n",
        "            str_label='Pass'\n",
        "            passCnt = passCnt + 1\n",
        "        else:\n",
        "            str_label='Fail'\n",
        "\n",
        "        if y_test[i] ==0:\n",
        "            if np.argmax(model_out[i, :]) == y_test[i]:\n",
        "                if model_out[i, 0] < smallest_tp:\n",
        "                  smallest_tp = model_out[i, 0]\n",
        "                # print('TP' + str(model_out[i, :]))\n",
        "                TP = TP + 1;\n",
        "            else:\n",
        "                # print('FN' + str(model_out[i, :]))\n",
        "                FN = FN + 1\n",
        "        else:\n",
        "            if np.argmax(model_out[i, :]) == y_test[i]:\n",
        "                # print('TN' + str(model_out[i, :]))\n",
        "                TN = TN + 1;\n",
        "            else:\n",
        "                if model_out[i, 1] > largest_fp:\n",
        "                  largest_fp = model_out[i, 1]\n",
        "                # print('FP' + str(model_out[i, :]))\n",
        "                FP = FP + 1\n",
        "\n",
        "        if model_out[i, 0] > positive_confidence_threshold and y_test[i] == 1:\n",
        "          incorrect_threshold_count = incorrect_threshold_count + 1\n",
        "\n",
        "    start = \"\\033[1m\"\n",
        "    end = \"\\033[0;0m\"\n",
        "\n",
        "    print(start + 'incorrect positives with threshold: ' + end + str(incorrect_threshold_count))\n",
        "    print(start + 'largest false positive confidence: ' + end + str(largest_fp))\n",
        "    print(start + 'smalles true positive confidence: ' + end + str(smallest_tp))\n",
        "    print(start + 'confusion matrix (test / validation)' + end)\n",
        "    print(start + 'true positive:  '+ end + str(TP))\n",
        "    print(start + 'false positive: '+ end + str(FP))\n",
        "    print(start + 'true negative:  '+ end + str(TN))\n",
        "    print(start + 'false negative: '+ end + str(FN))\n",
        "    print('\\n')\n",
        "\n",
        "    if TP+FP+FN+TN != 0:\n",
        "      print(start + 'accuracy:  ' + end + \"{:.4f} %\".format(100*(TP+TN)/(TP+FP+FN+TN)))\n",
        "    else:\n",
        "      print(start + 'accuracy:  ' + end + \"{:.4f} %\".format(100))\n",
        "\n",
        "    if TP + FP != 0:\n",
        "      print(start + 'precision: ' + end + \"{:.4f} %\".format(100*TP/(TP + FP)))\n",
        "    else:\n",
        "      print(start + 'precision: ' + end + \"{:.4f} %\".format(100))\n",
        "\n",
        "    if TP + FN != 0:\n",
        "      print(start + 'recall:  ' + end + \"{:.4f} %\".format(100*TP/(TP + FN)))\n",
        "    else:\n",
        "      print(start + 'recall:  ' + end + \"{:.4f} %\".format(100))\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWnNYpcTqa_O"
      },
      "source": [
        "## Test CNN Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV0rGdyIueOf"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import argparse\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from skimage import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "#constants\n",
        "width = 500#384 #change dimensions according to the input image in the training\n",
        "height = 375#512 #change dimensions according to the input image in the training\n",
        "depth = 1\n",
        "num_classes = 2\n",
        "\n",
        "def testMoire(weightsFile, superpoch, dataSetNumber):\n",
        "\n",
        "    print('##### TESTING - superpoch: ' + superpoch + ' - dataset: ' + dataSetNumber[0] + ', ' + dataSetNumber[1])\n",
        "    X_LL, X_LH, X_HL, X_HH, Y = mainReadDatafromDrive(dataSetNumber[0], 'test')\n",
        "    X_LL_2, X_LH_2, X_HL_2, X_HH_2, Y_2 = mainReadDatafromDrive(dataSetNumber[1], 'test')\n",
        "\n",
        "    print('Concatenating 2 datasets.')\n",
        "    X_LL = np.concatenate((X_LL, X_LL_2), axis=0)\n",
        "    X_LH = np.concatenate((X_LH, X_LH_2), axis=0)\n",
        "    X_HL = np.concatenate((X_HL, X_HL_2), axis=0)\n",
        "    X_HH = np.concatenate((X_HH, X_HH_2), axis=0)\n",
        "    Y = np.concatenate((Y, Y_2), axis=0)\n",
        "    X_LL_2 = None\n",
        "    X_LH_2 = None\n",
        "    X_HL_2 = None\n",
        "    X_HH_2 = None\n",
        "    Y_2 = None\n",
        "    \n",
        "    CNN_model = keras.models.load_model(weightsFile)\n",
        "    evaluate(CNN_model,X_LL,X_LH,X_HL,X_HH, Y)\n",
        "    X_LL = None\n",
        "    X_LH = None\n",
        "    X_HL = None \n",
        "    X_HH = None\n",
        "    Y = None\n",
        "\n",
        "def run(model, X_LL_test,X_LH_test,X_HL_test,y_test):\n",
        "    return\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmyNG_XqMzIb"
      },
      "source": [
        "## Synthesize Data\n",
        "Synthesize random data wooo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELZtp0_bM3rj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd385d6-063d-48b4-f57a-82f8f20df86b"
      },
      "source": [
        "import os\n",
        "from os.path import isfile, join\n",
        "from PIL import Image, ImageDraw, ImageFilter\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "colors = ['orange', 'blue', 'yellow', 'red', 'green', 'purple']\n",
        "\n",
        "def randomColor():\n",
        "  return colors[random.randint(0, 5)]\n",
        "\n",
        "def getBoundingBox(minWidth, maxWidth, minHeight, maxHeight):\n",
        "  length = random.randint(minWidth, maxWidth)\n",
        "  length1 = random.randint(minHeight, maxHeight)\n",
        "  w, h = length, length1\n",
        "  x = random.randint(0, 1000 - length)\n",
        "  y = random.randint(0, 750 - length1)\n",
        "  return [(x, y), (w + x, h + y)]\n",
        "\n",
        "def getBoundingSquare(minWidth, maxWidth):\n",
        "  length = random.randint(minWidth, maxWidth)\n",
        "  w, h = length, length\n",
        "  x = random.randint(0, 1000 - length)\n",
        "  y = random.randint(0, 750 - length)\n",
        "  return [(x, y), (w + x, h + y)]\n",
        "\n",
        "def getBoxFromSize(widthSize, heightSize, square = False):\n",
        "\n",
        "  minHeight = 0\n",
        "  maxHeight = 0\n",
        "  minWidth = 0\n",
        "  maxWidth = 0\n",
        "\n",
        "  if widthSize == 'small':\n",
        "    minWidth = 25\n",
        "    maxWidth = 100\n",
        "  elif widthSize == 'medium':\n",
        "    minWidth = 120\n",
        "    maxWidth = 280\n",
        "  elif widthSize == 'large':\n",
        "    minWidth = 280\n",
        "    maxWidth = 450\n",
        "  elif widthSize == 'x-large':\n",
        "    minWidth = 450\n",
        "    maxWidth = 620\n",
        "  elif widthSize == 'xx-large':\n",
        "    minWidth = 620\n",
        "    maxWidth = 900\n",
        "  else:\n",
        "    raise ValueError('width was not the proper value')\n",
        "\n",
        "  if heightSize == 'small':\n",
        "    minHeight = 25\n",
        "    maxHeight = 100\n",
        "  elif heightSize == 'medium':\n",
        "    minHeight = 120\n",
        "    maxHeight = 280\n",
        "  elif heightSize == 'large':\n",
        "    minHeight = 280\n",
        "    maxHeight = 450\n",
        "  elif heightSize == 'x-large':\n",
        "    minHeight = 450\n",
        "    maxHeight = 620\n",
        "  else:\n",
        "    raise ValueError('height was not the proper value')\n",
        "\n",
        "  if square == True:\n",
        "    return getBoundingSquare(minWidth, maxWidth)\n",
        "  else:\n",
        "    return getBoundingBox(minWidth, maxWidth, minHeight, maxHeight)\n",
        "\n",
        "def getRandomPointInRegion(shape):\n",
        "  return (random.randint(shape[0][0], shape[1][0]), random.randint(shape[0][1], shape[1][1]))\n",
        "\n",
        "def circle(image, imageConfig):\n",
        "    if imageConfig.widthClass == None or imageConfig.heightClass == None:\n",
        "      raise ValueError('width and height class need to be specified to make a circle')\n",
        "\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    shape = getBoxFromSize(imageConfig.widthClass, imageConfig.heightClass, True)\n",
        "    draw.chord(shape, start=0, end=360, fill=randomColor(), outline=randomColor(), width=random.randint(2,20))\n",
        "\n",
        "def ellipse(image, imageConfig):\n",
        "    if imageConfig.widthClass == None or imageConfig.heightClass == None:\n",
        "      raise ValueError('width and height class need to be specified to make an ellipse')\n",
        "\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    shape = getBoxFromSize(imageConfig.widthClass, imageConfig.heightClass, False)\n",
        "    draw.chord(shape, start=0, end=360, fill=randomColor(), outline=randomColor(), width=random.randint(2,20))\n",
        "\n",
        "def chord(image, imageConfig):\n",
        "    if imageConfig.widthClass == None or imageConfig.heightClass == None:\n",
        "      raise ValueError('width and height class need to be specified to make a chord')\n",
        "\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    shape = getBoxFromSize(imageConfig.widthClass, imageConfig.heightClass, False)\n",
        "    start = random.randint(30, 145)\n",
        "    end = random.randint(225, 340)\n",
        "    draw.chord(shape, start=start, end=end, fill=randomColor(), outline=randomColor(), width=random.randint(2,20))\n",
        "\n",
        "def rectangle(image, imageConfig):\n",
        "    if imageConfig.widthClass == None or imageConfig.heightClass == None:\n",
        "      raise ValueError('width and height class need to be specified to make a rectangle')\n",
        "\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    shape = getBoxFromSize(imageConfig.widthClass, imageConfig.heightClass, False)\n",
        "    draw.rectangle(shape, fill=randomColor(), outline=randomColor(), width=random.randint(2,20))\n",
        "\n",
        "def square(image, imageConfig):\n",
        "    if imageConfig.widthClass == None or imageConfig.heightClass == None:\n",
        "      raise ValueError('width and height class need to be specified to make a square')\n",
        "\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    shape = getBoxFromSize(imageConfig.widthClass, imageConfig.heightClass, True)\n",
        "    draw.rectangle(shape, fill=randomColor(), outline=randomColor(), width=random.randint(2,20))\n",
        "\n",
        "def pieslice(image, imageConfig):\n",
        "    if imageConfig.widthClass == None or imageConfig.heightClass == None:\n",
        "      raise ValueError('width and height class need to be specified to make a pieslice')\n",
        "\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    shape = getBoxFromSize(imageConfig.widthClass, imageConfig.heightClass, True)\n",
        "    start = random.randint(30, 145)\n",
        "    end = random.randint(225, 340)\n",
        "    draw.pieslice(shape, start=start, end=end, fill=randomColor(), outline=randomColor(), width=random.randint(2,20))\n",
        "\n",
        "def line(image, imageConfig):\n",
        "\n",
        "    if imageConfig.widthClass == None or imageConfig.heightClass == None:\n",
        "      raise ValueError('width and height class need to be specified to make a pieslice')\n",
        "    if imageConfig.sides == None:\n",
        "      raise ValueError('number of sides (lines) need to be specified to make a line group')\n",
        "    if imageConfig.noiseLow == None or imageConfig.noiseHigh == None:\n",
        "      raise ValueError('noise-low (low-line-width) and noise-high (high-line-width) need to be specified to create line groups in image')\n",
        "\n",
        "\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    pointRegion1 = getBoxFromSize(imageConfig.widthClass, imageConfig.heightClass, False)\n",
        "    pointRegion2 = getBoxFromSize(imageConfig.widthClass, imageConfig.heightClass, False)\n",
        "\n",
        "    width=random.randint(imageConfig.noiseLow,imageConfig.noiseHigh)\n",
        "\n",
        "    for i in range(random.randint(max(round(imageConfig.sides / 3), 1), imageConfig.sides)):\n",
        "      points = [getRandomPointInRegion(pointRegion1), getRandomPointInRegion(pointRegion2)]\n",
        "      draw.line(points, width=width, fill=randomColor(), joint=\"curve\")\n",
        "\n",
        "\n",
        "def polygon(image, imageConfig):\n",
        "    if imageConfig.widthClass == None or imageConfig.heightClass == None:\n",
        "      raise ValueError('width and height class need to be specified to make a polygon')\n",
        "    if imageConfig.sides == None:\n",
        "      raise ValueError('The number of sides was not specified')\n",
        "\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    shape = getBoxFromSize(imageConfig.widthClass, imageConfig.heightClass, False)\n",
        "    point = None\n",
        "\n",
        "    sides = random.randint(max(round(imageConfig.sides / 2), 3), imageConfig.sides)\n",
        "\n",
        "    if sides < 3:\n",
        "      raise ValueError('There needs to be three or more sides')\n",
        "    elif sides > 10:\n",
        "      raise ValueError('There needs to be ten or less sides')\n",
        "    elif sides == 3:\n",
        "      points = (getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape))\n",
        "    elif sides == 4:\n",
        "      points = (getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape))\n",
        "    elif sides == 5:\n",
        "      points = (getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape))\n",
        "    elif sides == 6:\n",
        "      points = (getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape))\n",
        "    elif sides == 7:\n",
        "      points = (getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape))\n",
        "    elif sides == 8:\n",
        "      points = (getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape))\n",
        "    elif sides == 9:\n",
        "      points = (getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape))\n",
        "    elif sides == 10:\n",
        "      points = (getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape), getRandomPointInRegion(shape))\n",
        "\n",
        "    draw.polygon(points, fill=randomColor())\n",
        "    \n",
        "def noise(image, imageConfig):\n",
        "  if imageConfig.noiseLow == None or imageConfig.noiseHigh == None:\n",
        "      raise ValueError('noise-low and noise-high need to be specified to create noise in image')\n",
        "  \n",
        "  imageData = image.getdata()\n",
        "  threshold = random.randint(imageConfig.noiseLow, imageConfig.noiseHigh)\n",
        "  newImageData = []\n",
        "\n",
        "  for pixel in imageData:\n",
        "    if random.randint(0, 100) < threshold:\n",
        "      newImageData.append((random.randint(25, 255), random.randint(25, 255), random.randint(25, 255)))\n",
        "    else:\n",
        "      newImageData.append(pixel)\n",
        "\n",
        "  noisy = Image.new(image.mode, image.size)\n",
        "  noisy.putdata(newImageData)\n",
        "  return noisy\n",
        "\n",
        "def localNoise(image, imageConfig):\n",
        "  if imageConfig.noiseLow == None or imageConfig.noiseHigh == None:\n",
        "      raise ValueError('noise-low and noise-high need to be specified to create noise in image')\n",
        "  if imageConfig.widthClass == None or imageConfig.heightClass == None:\n",
        "      raise ValueError('width and height class need to be specified to create localized noise in image')\n",
        "\n",
        "  shape = getBoxFromSize(imageConfig.widthClass, imageConfig.heightClass, False)\n",
        "\n",
        "  lowWidth = shape[0][0]\n",
        "  highWidth = shape[1][0]\n",
        "  lowHeight = shape[0][1]\n",
        "  highHeight = shape[1][1]\n",
        "\n",
        "  imageData = image.getdata()\n",
        "  threshold = random.randint(imageConfig.noiseLow, imageConfig.noiseHigh)\n",
        "  newImageData = []\n",
        "\n",
        "  for i in range(len(imageData)):\n",
        "    x = i % 1000\n",
        "    y = i / 1000\n",
        "\n",
        "    inRegion = x > lowWidth and x < highWidth and y > lowHeight and y < highHeight\n",
        "\n",
        "    if random.randint(0, 100) < threshold and inRegion:\n",
        "      newImageData.append((random.randint(25, 255), random.randint(25, 255), random.randint(25, 255)))\n",
        "    else:\n",
        "      newImageData.append(imageData[i])\n",
        "\n",
        "  noisy = Image.new(image.mode, image.size)\n",
        "  noisy.putdata(newImageData)\n",
        "  return noisy\n",
        "\n",
        "def cropShape(cropContour, imageConfig):\n",
        "  files = [name for name in os.listdir(imageConfig.inputImageDirPath)]\n",
        "  cropFile = files[random.randint(0, len(files) - 1)]\n",
        "  img = Image.open(join(imageConfig.inputImageDirPath, cropFile))\n",
        "\n",
        "  # blankImageData = np.full((750, 1000), 255, np.uint8)\n",
        "  # image = Image.fromarray(blankImageData)\n",
        "\n",
        "  # if image.mode != 'RGB':\n",
        "  #   image = image.convert('RGB')\n",
        "\n",
        "  cropContourImageData = cropContour.getdata()\n",
        "  croppedImageData = img.getdata()\n",
        "\n",
        "  newImageData = []\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  print(cropContourImageData)\n",
        "  print(len(cropContourImageData))\n",
        "  for i in range(len(cropContourImageData)):\n",
        "    # print(cropContourImageData[i])\n",
        "    if cropContourImageData[i][0] < 255 or cropContourImageData[i][1] < 255 or cropContourImageData[i][2] < 255:\n",
        "      \n",
        "      count = count + 1\n",
        "      newImageData.append(croppedImageData[i])\n",
        "    else:\n",
        "      newImageData.append((255,255,255))\n",
        "\n",
        "  print(count)\n",
        "  cropped = Image.new(cropContour.mode, cropContour.size)\n",
        "  cropped.putdata(newImageData)\n",
        "  return cropped\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "class ImageConfig(object):\n",
        "    def __init__(self, process, widthClass = None, heightClass = None, amount = None, sides = None, noiseLow = 0, noiseHigh = 0):\n",
        "        self.process = process\n",
        "        self.widthClass = widthClass\n",
        "        self.heightClass = heightClass\n",
        "        self.amount = amount\n",
        "        self.sides = sides\n",
        "        self.noiseLow = noiseLow\n",
        "        self.noiseHigh = noiseHigh\n",
        "\n",
        "class ShapeCropConfig(object):\n",
        "  def __init__(self, process, inputImageDirPath):\n",
        "    self.process = process\n",
        "    self.inputImageDirPath = inputImageDirPath\n",
        "\n",
        "def generateFakeImages(imageConfigList, dataset, i):\n",
        "  imageData = np.full((750, 1000), 255, np.uint8)\n",
        "  image = Image.fromarray(imageData)\n",
        "\n",
        "  if image.mode != 'RGB':\n",
        "    image = image.convert('RGB')\n",
        "\n",
        "  for config in imageConfigList:\n",
        "    \n",
        "\n",
        "    if isinstance(config, ImageConfig):\n",
        "      if config.amount == None:\n",
        "        config.amount = 1\n",
        "\n",
        "      if config.process == 'circle':\n",
        "        for number in range(config.amount):\n",
        "          circle(image, config)\n",
        "      if config.process == 'chord':\n",
        "        for number in range(config.amount):\n",
        "          chord(image, config)\n",
        "      if config.process == 'pieslice':\n",
        "        for number in range(config.amount):\n",
        "          pieslice(image, config)\n",
        "      if config.process == 'square':\n",
        "        for number in range(config.amount):\n",
        "          square(image, config)\n",
        "      if config.process == 'ellipse':\n",
        "        for number in range(config.amount):\n",
        "          ellipse(image, config)\n",
        "      if config.process == 'rectangle':\n",
        "        for number in range(config.amount):\n",
        "          rectangle(image, config)\n",
        "      if config.process == 'polygon':\n",
        "        for number in range(config.amount):\n",
        "          polygon(image, config)\n",
        "      if config.process == 'line':\n",
        "        for number in range(config.amount):\n",
        "          line(image, config)\n",
        "      if config.process == 'noise':\n",
        "        for number in range(config.amount):\n",
        "          image = noise(image, config)\n",
        "      if config.process == 'local-noise':\n",
        "        for number in range(config.amount):\n",
        "          image = localNoise(image, config)\n",
        "    if isinstance(config, ShapeCropConfig):\n",
        "      if config.process == 'crop-shape':\n",
        "        image = cropShape(image, config)\n",
        "\n",
        "  # image.filter(ImageFilter.SMOOTH)\n",
        "  image.save('/content/drive/MyDrive/Moire/unnormalized/negative/' + dataset + '/' + str(i) + '.png')\n",
        "      \n",
        "      \n",
        "\n",
        "imageConfigList18 = []\n",
        "\n",
        "# 018\n",
        "# imageConfigList18.append(ImageConfig('circle', 'large', 'large', 4))\n",
        "# imageConfigList18.append(ImageConfig('pieslice', 'small', 'medium', 5))\n",
        "# imageConfigList18.append(ImageConfig('polygon', 'small', 'large', 8, 8))\n",
        "# imageConfigList18.append(ImageConfig('line', 'small', 'small', amount = 4, sides = 5, noiseLow = 2, noiseHigh = 5))\n",
        "\n",
        "# 019\n",
        "# imageConfigList18.append(ImageConfig('circle', 'small', 'small', 4))\n",
        "# imageConfigList18.append(ImageConfig('pieslice', 'small', 'medium', 5))\n",
        "# imageConfigList18.append(ImageConfig('square', 'medium', 'medium', 5))\n",
        "# imageConfigList18.append(ImageConfig('noise', 'small', 'small', amount = 1, noiseLow = 10, noiseHigh = 50))\n",
        "# imageConfigList18.append(ImageConfig('line', 'small', 'small', amount = 4, sides = 5, noiseLow = 2, noiseHigh = 5))\n",
        "# imageConfigList18.append(ImageConfig('polygon', 'large', 'small', 12, 9))\n",
        "\n",
        "# 020\n",
        "# imageConfigList18.append(ImageConfig('local-noise', 'small', 'small', amount = 10, noiseLow = 10, noiseHigh = 40))\n",
        "# imageConfigList18.append(ImageConfig('local-noise', 'medium', 'small', amount = 7, noiseLow = 10, noiseHigh = 40))\n",
        "# imageConfigList18.append(ImageConfig('local-noise', 'medium', 'large', amount = 5, noiseLow = 10, noiseHigh = 40))\n",
        "# imageConfigList18.append(ImageConfig('polygon', 'large', 'small', 12, 3))\n",
        "# imageConfigList18.append(ImageConfig('local-noise', 'small', 'medium', amount = 7, noiseLow = 10, noiseHigh = 40))\n",
        "\n",
        "# 021\n",
        "# imageConfigList18.append(ImageConfig('pieslice', 'medium', 'small', amount = 10))\n",
        "# imageConfigList18.append(ImageConfig('polygon', 'large', 'small', 20, 6))\n",
        "# imageConfigList18.append(ImageConfig('local-noise', 'medium', 'large', amount = 5, noiseLow = 10, noiseHigh = 40))\n",
        "# imageConfigList18.append(ImageConfig('circle', 'small', 'small', amount = 20))\n",
        "\n",
        "# 022\n",
        "# imageConfigList18.append(ImageConfig('line', 'small', 'small', amount = 8, sides = 6, noiseLow = 2, noiseHigh = 5))\n",
        "# imageConfigList18.append(ImageConfig('line', 'medium', 'medium', amount = 8, sides = 5, noiseLow = 2, noiseHigh = 5))\n",
        "# imageConfigList18.append(ImageConfig('line', 'large', 'large', amount = 8, sides = 4, noiseLow = 2, noiseHigh = 5))\n",
        "# imageConfigList18.append(ImageConfig('line', 'x-large', 'x-large', amount = 8, sides = 3, noiseLow = 2, noiseHigh = 5))\n",
        "\n",
        "# 023\n",
        "# imageConfigList18.append(ImageConfig('line', 'small', 'small', amount = 5, sides = 6, noiseLow = 2, noiseHigh = 5))\n",
        "# imageConfigList18.append(ImageConfig('line', 'medium', 'medium', amount = 5, sides = 5, noiseLow = 2, noiseHigh = 5))\n",
        "# imageConfigList18.append(ImageConfig('line', 'large', 'large', amount = 5, sides = 4, noiseLow = 2, noiseHigh = 5))\n",
        "# imageConfigList18.append(ImageConfig('line', 'x-large', 'x-large', amount = 5, sides = 3, noiseLow = 2, noiseHigh = 5))\n",
        "# imageConfigList18.append(ImageConfig('noise', 'small', 'small', amount = 1, noiseLow = 10, noiseHigh = 50))\n",
        "# imageConfigList18.append(ImageConfig('pieslice', 'medium', 'small', amount = 10))\n",
        "# imageConfigList18.append(ImageConfig('square', 'medium', 'medium', 5))\n",
        "# imageConfigList18.append(ImageConfig('local-noise', 'medium', 'large', amount = 6, noiseLow = 10, noiseHigh = 40))\n",
        "\n",
        "# 024\n",
        "# imageConfigList18.append(ImageConfig('polygon', 'large', 'small', 12, 3))\n",
        "# imageConfigList18.append(ImageConfig('polygon', 'medium', 'medium', 12, 6))\n",
        "# imageConfigList18.append(ShapeCropConfig('crop-shape', '/content/drive/MyDrive/Moire/preaugmented/negative/006'))\n",
        "\n",
        "# 025\n",
        "# imageConfigList18.append(ImageConfig('polygon', 'large', 'small', 12, 3))\n",
        "# imageConfigList18.append(ImageConfig('polygon', 'medium', 'medium', 12, 6))\n",
        "# imageConfigList18.append(ImageConfig('square', 'medium', 'medium', 5))\n",
        "# imageConfigList18.append(ImageConfig('pieslice', 'medium', 'medium', amount = 10))\n",
        "# imageConfigList18.append(ShapeCropConfig('crop-shape', '/content/drive/MyDrive/Moire/preaugmented/negative/007'))\n",
        "\n",
        "# 026\n",
        "imageConfigList18.append(ImageConfig('circle', 'large', 'large', amount = 10))\n",
        "imageConfigList18.append(ImageConfig('polygon', 'large', 'small', 12, 3))\n",
        "imageConfigList18.append(ImageConfig('polygon', 'medium', 'medium', 12, 6))\n",
        "imageConfigList18.append(ImageConfig('square', 'medium', 'medium', 5))\n",
        "imageConfigList18.append(ImageConfig('pieslice', 'medium', 'medium', amount = 10))\n",
        "imageConfigList18.append(ShapeCropConfig('crop-shape', '/content/drive/MyDrive/Moire/preaugmented/negative/008'))\n",
        "\n",
        "# imageConfigList18.append(ImageConfig('polygon', 'large', 'small', 12, 3))\n",
        "# imageConfigList18.append(ImageConfig('local-noise', 'small', 'medium', amount = 7, noiseLow = 10, noiseHigh = 40))\n",
        "# imageConfigList18.append(ImageConfig('circle', 'large', 'x-large', 3))\n",
        "# imageConfigList18.append(ImageConfig('circle', 'small', 'x-large', 5))\n",
        "# imageConfigList18.append(ImageConfig('circle', 'large', 'x-large', 3))\n",
        "# imageConfigList18.append(ImageConfig('chord', 'large', 'x-large', 5))\n",
        "# imageConfigList18.append(ImageConfig('chord', 'x-large', 'x-large', 5))\n",
        "# imageConfigList18.append(ImageConfig('chord', 'small', 'x-large', 5))\n",
        "# imageConfigList18.append(ImageConfig('square', 'medium', 'medium', 5))\n",
        "# imageConfigList18.append(ImageConfig('ellipse', 'small', 'small', 5))\n",
        "# imageConfigList18.append(ImageConfig('rectangle', 'small', 'small', 5))\n",
        "# imageConfigList18.append(ImageConfig('pieslice', 'large', 'x-large', 5))\n",
        "# imageConfigList18.append(ImageConfig('pieslice', 'medium', 'x-large', 5))\n",
        "# imageConfigList18.append(ImageConfig('pieslice', 'large', 'large', 5))\n",
        "\n",
        "dataset = '026'\n",
        "dir = '/content/drive/MyDrive/Moire/unnormalized/negative/' + dataset\n",
        "\n",
        "if len([name for name in os.listdir(dir)]) > 0:\n",
        "  print('Directory ' + dir + ' already has normalized photos in it.  Skipping normalization for negative photos.')\n",
        "else:\n",
        "  for i in range(30):\n",
        "    print(i)\n",
        "    generateFakeImages(imageConfigList18, dataset, i)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Directory /content/drive/MyDrive/Moire/unnormalized/negative/026 already has normalized photos in it.  Skipping normalization for negative photos.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agjwCacSQyN0",
        "outputId": "5732ead6-6cea-4065-f619-293b8405438c"
      },
      "source": [
        "isinstance(ImageConfig('line', 'small', 'small', amount = 5, sides = 6, noiseLow = 2, noiseHigh = 5), int)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPksVriW1E8W"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "my_img = np.full((750, 1000, 3), 255, np.uint8)\n",
        "# my_img = np.zeros((400, 400, 3), dtype = \"uint8\")\n",
        "pts = np.array([[80,261],[110,296],[260,306],[150,276]], np.int32)\n",
        "\n",
        "pts = pts.reshape((-1,1,2))\n",
        "\n",
        "cv2.ellipse(my_img,(256,256),(200,100),0,0,180,255,-1)\n",
        "cv2.polylines(my_img,[pts],True,(0,255,255))\n",
        "\n",
        "image = Image.fromarray(my_img)\n",
        "image.save(\"opencv.jpeg\")\n",
        "# cv2.imshow('Window', my_img)\n",
        "# cv2.waitKey(0)\n",
        "# cv2.destroyAllWindows()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91EoQyF2543f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c40faf5-7a5a-4e7c-dba7-31c8d0f269b3"
      },
      "source": [
        "from IPython.display import Image as ImageDisplay\n",
        "# Image('/content/drive/MyDrive/Moire/unnormalized/negative/018/28.jpeg')\n",
        "ImageDisplay('/content/drive/MyDrive/Moire/unnormalized/negative/020/29.jpeg')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "/content/drive/MyDrive/Moire/unnormalized/negative/020/29.jpeg",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTxUTR6Yv2LG"
      },
      "source": [
        "## Extended Training\n",
        "Take the trained model and continue training with additional data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fb9-mhtcBbo",
        "outputId": "a279619a-08ab-4c74-8fa1-f3c8f7977f02"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "# normalizeRawImages(\"001\", \"001\")\n",
        "# augmentNormalizedData(\"001\", \"001\")\n",
        "# normalizeRawImages(\"001\", \"002\")\n",
        "# augmentNormalizedData(\"001\", \"002\")\n",
        "# normalizeRawImages(\"001\", \"003\")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       001\", \"003\")\n",
        "# augmentNormalizedData(\"001\", \"003\")\n",
        "# normalizeRawImages(\"001\", \"004\")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       001\", \"003\")\n",
        "# augmentNormalizedData(\"001\", \"004\")\n",
        "# normalizeRawImages(\"001\", \"005\")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       001\", \"003\")\n",
        "# augmentNormalizedData(\"001\", \"005\")\n",
        "# normalizeRawImages(\"001\", \"006\")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       001\", \"003\")\n",
        "# augmentNormalizedData(\"001\", \"006\")\n",
        "\n",
        "# weights_file = '/content/drive/MyDrive/Moire/checkPoint/final-tm1626925731-sp001-ds020_013-weights'\n",
        "weights_file = None\n",
        "\n",
        "superpoch_count = 3\n",
        "scramblepoch_count = 1\n",
        "epochs = 20\n",
        "datasets = ['024', '025', '026']\n",
        "\n",
        "# for sp in range(superpoch_count):\n",
        "#   superpoch = '00' + str(sp + 1)\n",
        "#   print('---------------------------------------------------------')\n",
        "#   print('BEGINNING TRAINING OF SUPERPOCH: ' + superpoch)\n",
        "#   print('---------------------------------------------------------')\n",
        "#   for ds in range(len(datasets)):\n",
        "#     dsIndex = ds\n",
        "#     otherDsIndex = ((ds + 1) % len(datasets))\n",
        "#     # datasets = ['00' + str(dsNumber), '00' + str(otherDsNumber)]\n",
        "#     print([datasets[dsIndex], datasets[otherDsIndex]])\n",
        "\n",
        "#     # print(ds)\n",
        "#     # print(i)\n",
        "\n",
        "#     # print(datasets)\n",
        "#     # normalizeRawImages(superpoch, dataset)\n",
        "#     # augmentNormalizedData(superpoch, dataset)\n",
        "\n",
        "#     weights_file = trainMoire(superpoch, [datasets[dsIndex], datasets[otherDsIndex]], epochs, weights_file)\n",
        "#     testMoire(weights_file, superpoch, [datasets[dsIndex], datasets[otherDsIndex]])\n",
        "#     print('---------------------------------------------------------')\n",
        "\n",
        "def scramblepoch():\n",
        "  for scp in range(scramblepoch_count):\n",
        "    allPositiveImageFiles = []\n",
        "    allNegativeImageFiles = []\n",
        "\n",
        "    for ds in range(len(datasets)):\n",
        "      unaugmentedPositiveDir = '/content/drive/MyDrive/Moire/preaugmented/positive/' + datasets[ds] + '/'\n",
        "      unaugmentedNegativeDir = '/content/drive/MyDrive/Moire/preaugmented/negative/' + datasets[ds] + '/'\n",
        "\n",
        "      singleDatasetPositiveImageFiles = [f for f in listdir(unaugmentedPositiveDir) if (isfile(join(unaugmentedPositiveDir, f)))]\n",
        "      singleDatasetNegativeImageFiles = [f for f in listdir(unaugmentedNegativeDir) if (isfile(join(unaugmentedNegativeDir, f)))]\n",
        "\n",
        "      # for file in singleDatasetPositiveImageFiles:\n",
        "      #   allPositiveImageFiles.append()\n",
        "\n",
        "      positiveDataBorder = round(len(singleDatasetPositiveImageFiles) * SPLIT_RATIO)\n",
        "      negativeDataBorder = round(len(singleDatasetNegativeImageFiles) * SPLIT_RATIO)\n",
        "\n",
        "      # if dataType == 'train':\n",
        "      positiveReadFiles = singleDatasetPositiveImageFiles[:positiveDataBorder]\n",
        "      negativeReadFiles = singleDatasetNegativeImageFiles[:negativeDataBorder]\n",
        "\n",
        "      for file in positiveReadFiles:\n",
        "        allPositiveImageFiles.append(join(unaugmentedPositiveDir, file))\n",
        "\n",
        "      for file in negativeReadFiles:\n",
        "        allNegativeImageFiles.append(join(unaugmentedNegativeDir, file))\n",
        "\n",
        "    negativeFilesShuffled, positiveFilesShuffled = shuffle(allNegativeImageFiles, allPositiveImageFiles, random_state=ds)  \n",
        "\n",
        "    print('positive')\n",
        "    print(positiveFilesShuffled)\n",
        "\n",
        "    print('negative')\n",
        "    print(negativeFilesShuffled)\n",
        "\n",
        "\n",
        "    # filename = (os.path.splitext(positiveFilesShuffled[0])[0])\n",
        "    # fullLLFilename = (filename.replace(positiveFilesShuffled[0], positiveFilesShuffled[0] + '_180' + '_LL')) + '.png'\n",
        "\n",
        "\n",
        "\n",
        "scramblepoch()\n",
        "      # elif dataType == 'test':\n",
        "      #   positiveReadFiles = positiveImageFiles[positiveDataBorder:]\n",
        "      #   negativeReadFiles = negativeImageFiles[negativeDataBorder:]\n",
        "      # '/content/drive/MyDrive/Moire/'\n",
        "\n",
        "\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n",
            "['/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3933.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3905.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3865.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3917.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3897.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3890.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3939.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3908.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3940.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3887.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3895.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3925.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3898.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3927.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3878.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3916.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3885.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3886.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3859.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3938.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3889.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3881.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3920.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3864.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3857.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3904.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3896.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3860.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3921.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3942.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3884.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3877.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3873.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3924.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3928.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3931.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3875.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3874.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3872.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3893.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3862.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3919.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3867.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3941.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3906.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3883.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3926.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3891.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3937.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3922.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3918.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3932.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3946.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3888.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3879.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3892.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3880.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3914.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3912.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3929.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3863.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3899.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3911.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3944.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3910.png', '/content/drive/MyDrive/Moire/preaugmented/positive/026/IMG_3930.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3902.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3866.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3894.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3868.png', '/content/drive/MyDrive/Moire/preaugmented/positive/024/IMG_3882.png', '/content/drive/MyDrive/Moire/preaugmented/positive/025/IMG_3903.png']\n",
            "negative\n",
            "['/content/drive/MyDrive/Moire/preaugmented/negative/026/19.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/11.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/1.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/23.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/4.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/22.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/10.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/21.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/0.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/5.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/12.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/2.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/1.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/17.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/10.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/20.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/11.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/0.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/14.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/12.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/3.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/21.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/21.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/17.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/23.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/8.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/6.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/9.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/16.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/18.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/13.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/18.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/8.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/22.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/13.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/11.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/12.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/16.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/2.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/17.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/19.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/14.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/6.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/6.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/0.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/5.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/8.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/9.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/5.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/7.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/9.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/3.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/20.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/18.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/4.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/14.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/3.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/15.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/13.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/4.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/20.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/2.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/7.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/15.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/23.png', '/content/drive/MyDrive/Moire/preaugmented/negative/026/1.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/10.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/7.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/19.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/22.png', '/content/drive/MyDrive/Moire/preaugmented/negative/024/15.png', '/content/drive/MyDrive/Moire/preaugmented/negative/025/16.png']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyiISSNr2ZAr"
      },
      "source": [
        "# X_LL_train, X_LH_train, X_HL_train, X_HH_train, Y_train = mainReadDatafromDrive('/content/drive/MyDrive/Moire/trainDataPositive', '/content/drive/MyDrive/Moire/trainDataNegative')\n",
        "# !ls /content/drive/MyDrive/Moire/preaugmented/positive/003\n",
        "print('negative unnormalized 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/negative/001')])))\n",
        "print('positive unnormalized 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/positive/001')])))\n",
        "print('negative unnormalized 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/negative/002')])))\n",
        "print('positive unnormalized 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/positive/002')])))\n",
        "print('negative unnormalized 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/negative/003')])))\n",
        "print('positive unnormalized 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/positive/003')])))\n",
        "# print('negative train 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/negative/001')])))\n",
        "# print('negative train 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/negative/002')])))\n",
        "# print('negative train 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/negative/003')])))\n",
        "# print('positive train 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/positive/001')])))\n",
        "# print('positive train 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/positive/002')])))\n",
        "# print('positive train 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/positive/003')])))\n",
        "# print('negative test 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/negative/001')])))\n",
        "# print('negative test 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/negative/002')])))\n",
        "# print('negative test 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/negative/003')])))\n",
        "# print('positive test 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/positive/001')])))\n",
        "# print('positive test 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/positive/002')])))\n",
        "# print('positive test 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/positive/003')])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9RFVMxIroLI"
      },
      "source": [
        "# testMoire('/content/drive/MyDrive/Moire/checkPoint/mid-sp005-ds002-ep016-ls0.00455-ac1.00-weights.h5', \"001\", \"001\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDO0lSriFOJt"
      },
      "source": [
        "def intermediateMoire(dataSetNumber, weights_file = None):\n",
        "\n",
        "    X_LL, X_LH, X_HL, X_HH, Y = mainReadDatafromDrive(dataSetNumber, 'test')\n",
        "    return createIntermediateModel(dataSetNumber, X_LL, X_LH, X_HL, X_HH, Y, weights_file)\n",
        "    \n",
        "\n",
        "def createIntermediateModel(dataSetNumber, X_LL_train, X_LH_train, X_HL_train, X_HH_train, y_train, weights_file):\n",
        "\n",
        "    batch_size = 32 # in each iteration, we consider 32 training examples at once\n",
        "    print(\"SHAPE\")\n",
        "    print(X_LL_train.shape);\n",
        "    num_train, height, width, depth = X_LL_train.shape\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    Y_train = np_utils.to_categorical(y_train, num_classes) # One-hot encode the labels\n",
        "    # Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels\n",
        "    \n",
        "    # if not os.path.exists(checkPointFolder):\n",
        "    #     os.makedirs(checkPointFolder)\n",
        "        \n",
        "        \n",
        "    model, intermediate_model = createModel(height, width, depth, num_classes)\n",
        "\n",
        "    if weights_file != None:\n",
        "      print('--- Loading weights file: ' + weights_file)\n",
        "      model.load_weights(weights_file)\n",
        "    else:\n",
        "      print('--- No weights file provided.  Creating new model.')\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
        "                  optimizer='adam', # using the Adam optimiser\n",
        "                  metrics=['accuracy']) # reporting the accuracy\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # weights_list = model.get_weights()\n",
        "    # for i in range(len(model.layers)):\n",
        "    #   print(model.layers[i].get_weights())\n",
        "      # print(model.layers[i].get_weights()[1])\n",
        "    \n",
        "    model_index = 0;\n",
        "    print('----------------')\n",
        "    for layer in intermediate_model.layers:\n",
        "      for weights in layer.get_weights():\n",
        "        weights = model.get_weights()[model_index]\n",
        "        model_index = model_index + 1\n",
        "\n",
        "    # print('----------------')\n",
        "\n",
        "    # for layer in model.layers:\n",
        "    #   for weights in layer.get_weights():\n",
        "    #     print(len(weights))\n",
        "    # print('----------------')\n",
        "    \n",
        "    # intermediate_model.layers[5].set_weights(weights_list[0])\n",
        "    # print(len(model.layers))\n",
        "    # intermediate_model.layers[i].set_weights(weights)\n",
        "\n",
        "    # intermediate_model.summary()\n",
        "\n",
        "    # model.fit([X_LL_train,X_LH_train,X_HL_train,X_HH_train], Y_train,                # Train the model using the training set...\n",
        "    #           batch_size=batch_size, epochs=num_epochs,\n",
        "    #           verbose=1, validation_split=0.1, callbacks=callbacks_list) # ...holding out 10% of the data for validation\n",
        "    # score, acc = model.evaluate([X_LL_test,X_LH_test,X_HL_test,X_HH_test], Y_test, verbose=1)  # Evaluate the trained model on the test set!\n",
        "\n",
        "    # modelName = '/content/drive/MyDrive/Moire/checkPoint/final-tm' + str(time.time()).split('.')[0] + '-sp' + superpoch + '-ds' + dataSetNumber + '-weights.h5'\n",
        "    # model.save(modelName)\n",
        "    \n",
        "    return intermediate_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27zETVwPEMCf"
      },
      "source": [
        "# from keras.models import Model\n",
        "\n",
        "# model = intermediateMoire(\"001\", '/content/drive/MyDrive/Moire/checkPoint/mid-sp005-ds002-ep016-ls0.00455-ac1.00-weights.h5')\n",
        "\n",
        "# model.summary()\n",
        "# X_LL, X_LH, X_HL, X_HH, Y = mainReadDatafromDrive(\"001\", 'test')\n",
        "# model_out = model.predict([X_LL, X_LH, X_HL, X_HH])\n",
        "\n",
        "\n",
        "# print(model_out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ESg5hvZl3u_"
      },
      "source": [
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "# model_shape = model_out.shape\n",
        "\n",
        "# reshaped_output = np.reshape(model_out, (model_shape[0], model_shape[3], model_shape[1], model_shape[2]))\n",
        "\n",
        "# print(reshaped_output.shape)\n",
        "# index = 0\n",
        "# # for image_batch in reshaped_output:\n",
        "# for image in reshaped_output[0]:\n",
        "#   # image_batch = np.reshape(reshaped_output, (model_shape[0], model_shape[3], model_shape[1], model_shape[2]))\n",
        "#   print(image.shape)\n",
        "\n",
        "#   largest = 0\n",
        "#   smallest = 100000\n",
        "\n",
        "#   for row in image:\n",
        "#     for i in row:\n",
        "#       if i > largest:\n",
        "#         largest = i\n",
        "#       if i < smallest:\n",
        "#         smallest = i\n",
        "\n",
        "#   value_range = largest - smallest\n",
        "#   scale_value = 255 / value_range\n",
        "\n",
        "#   for i in range(len(image)):\n",
        "#     for j in range(len(image[i])):\n",
        "#       image[i][j] = scale_value * image[i][j]\n",
        "#   print('largest: ' + str(largest))\n",
        "#   print('smallest: ' + str(smallest))\n",
        "#   im = Image.fromarray(image)\n",
        "#   index = index + 1\n",
        "\n",
        "#   if im.mode != 'RGB':\n",
        "#     im = im.convert('RGB')\n",
        "\n",
        "#   im.save('/content/drive/MyDrive/Moire/intermediate/test/negative/001/' + str(index) + \".jpg\")\n",
        "\n",
        "# print(index)\n",
        "# print(np.reshape(model_out, (model_shape[0], model_shape[3], model_shape[1], model_shape[2])).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnwQH5eYqa_O"
      },
      "source": [
        "# #constants\n",
        "# # width = 500#384 #change dimensions according to the input image in the training\n",
        "# # height = 375#512 #change dimensions according to the input image in the training\n",
        "# # depth = 1\n",
        "# # num_classes = 2\n",
        "\n",
        "# def continueTraining(weightsFile, X_LL_train, X_LH_train, X_HL_train, X_HH_train, Y_train, num_epochs):\n",
        "#     weights_file = (weightsFile)\n",
        "\n",
        "#     # X_LL, X_LH, X_HL, X_HH, Y = mainReadDatafromDrive(positiveImagePath, negativeImagePath)\n",
        "    \n",
        "    \n",
        "\n",
        "#     batch_size = 32 # in each iteration, we consider 32 training examples at once\n",
        "#     print(\"SHAPE\")\n",
        "#     print(X_LL_train.shape);\n",
        "#     num_train, height, width, depth = X_LL_train.shape\n",
        "#     num_classes = len(np.unique(Y_train))\n",
        "#     Y_train = np_utils.to_categorical(Y_train, num_classes) # One-hot encode the labels\n",
        "#     print(Y_train);\n",
        "#     # Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels\n",
        "\n",
        "#     CNN_model = createModel(height, width, depth, num_classes)\n",
        "#     CNN_model.load_weights(weights_file)\n",
        "\n",
        "#     checkPointFolder = 'content/drive/MyDrive/Moire/checkPoint'\n",
        "#     checkpoint_name = checkPointFolder + '/Weights-retrained-002--{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "#     checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "#     callbacks_list = [checkpoint]\n",
        "    \n",
        "#     if not os.path.exists(checkPointFolder):\n",
        "#         os.makedirs(checkPointFolder)\n",
        "        \n",
        "        \n",
        "#     # model = createModel(height, width, depth, num_classes)\n",
        "    \n",
        "#     CNN_model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
        "#                   optimizer='adam', # using the Adam optimiser\n",
        "#                   metrics=['accuracy']) # reporting the accuracy\n",
        "\n",
        "#     CNN_model.fit([X_LL_train,X_LH_train,X_HL_train,X_HH_train], Y_train,                # Train the model using the training set...\n",
        "#               batch_size=batch_size, epochs=num_epochs,\n",
        "#               verbose=1, validation_split=0.1, callbacks=callbacks_list) # ...holding out 10% of the data for validation\n",
        "#     # score, acc = model.evaluate([X_LL_test,X_LH_test,X_HL_test,X_HH_test], Y_test, verbose=1)  # Evaluate the trained model on the test set!\n",
        "\n",
        "#     CNN_model.save('content/drive/MyDrive/Moire/checkPoint/retrained-002-moirePattern3CNN_.h5')\n",
        "    \n",
        "#     return model\n",
        "#     # evaluate(CNN_model,X_LL,X_LH,X_HL,X_HH, Y)\n",
        "# import time\n",
        "# str(time.time()).split('.')[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fyxBfYY4i7U"
      },
      "source": [
        "# continueTraining('content/drive/MyDrive/Moire/checkPoint/Weights-028--0.06866.hdf5', X_LL_train, X_LH_train, X_HL_train, X_HH_train, Y_train, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlSUPMxf6tFa"
      },
      "source": [
        "# !ls content/drive/MyDrive/Moire/checkPoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYl1Npdb623a"
      },
      "source": [
        "# mainTest('content/drive/MyDrive/Moire/checkPoint/retrained-002-moirePattern3CNN_.h5', '/content/drive/MyDrive/Moire/testDataPositive', '/content/drive/MyDrive/Moire/testDataNegative')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}