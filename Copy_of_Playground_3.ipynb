{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Copy of Playground.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianhumphreys/Moire-Detector/blob/main/Copy_of_Playground_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDFp5O-Bqa_H"
      },
      "source": [
        "# Playground"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To3GCs0JndZT",
        "outputId": "b8186a08-a8ec-4781-b055-1a0e3cf85adf"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jul 16 02:55:01 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    33W / 250W |   8869MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5iT4R4PnYT6",
        "outputId": "0d4ab0cc-db05-4e8a-8715-be467c365360"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "muinrEgNqa_J"
      },
      "source": [
        "## Test 2D Wavelet Decomposition function\n",
        "This function(fwdHaarDWT2D) computes the 2D Wavelet Transform in the image. All the input images are passed through a Haar Wavelet Decomposition module, to get the LL, LH, HL and HHH component of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWlOXh6fq3xg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a385ae31-d109-42f0-ef00-74817a5279fe"
      },
      "source": [
        "from google.colab import drive                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r96WGlWEqjL1"
      },
      "source": [
        "# !ls /content/drive/MyDrive/Moire/train/negative/001/"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDw39lK9kAe0"
      },
      "source": [
        "# !ls /content/drive/MyDrive/Moire/trainDataNegative\n",
        "# !ls /content/drive/MyDrive/Moire/train/negative/001\n",
        "# !mv -v /content/drive/MyDrive/Moire/trainDataNegative/ '/content/drive/MyDrive/Moire/train/negative/001'\n",
        "# !ls '/content/drive/MyDrive/Moire/train/negative/001/trainDataNegative'\n",
        "# !mv -v '/content/drive/MyDrive/Moire/train/negative/001/trainDataNegative/*.tiff' '/content/drive/MyDrive/Moire/train/negative/001'\n",
        "# import os\n",
        "\n",
        "# fromDir = '/content/drive/MyDrive/Moire/notNormalizedPositiveImages/'\n",
        "# toDir = \"/content/drive/MyDrive/Moire/unnormalized/positive/002/\"\n",
        "\n",
        "# # !ls /content/drive/MyDrive/Moire/testDataNegative/\n",
        "\n",
        "# for filename in os.listdir(fromDir):\n",
        "#   os.rename(fromDir + filename, toDir + filename)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA7CMKEhro9d"
      },
      "source": [
        "# import os\n",
        "# os.chdir(\"/content/drive/\")\n",
        "# !ls\n",
        "\n",
        "# os.chdir(\"../..\")\n",
        "# !ls\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsTZ_TLIv3TK",
        "outputId": "d37139ba-62ac-4e5e-de94-75f70b0dd13f"
      },
      "source": [
        "!ls /content/drive/MyDrive/Moire\n",
        "!pip install pyheif whatimage\n",
        "# !pip show wand"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkPoint  intermediate  preaugmented\ttest  train  unnormalized\n",
            "Requirement already satisfied: pyheif in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: whatimage in /usr/local/lib/python3.7/dist-packages (0.0.3)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pyheif) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->pyheif) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmgaiBmhtDHX"
      },
      "source": [
        "# !ls /content/drive/MyDrive/Moire/notNormalizedPositiveImages/\n",
        "# !ls /content/drive/MyDrive/Moire/notNormalizedNegativeImages/\n",
        "# !uname -m"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO9ckef9Wrs3"
      },
      "source": [
        "## Normalized Raw data to 1000x750 pixel images\n",
        "Images are cropped to fit a 3:4 aspect ratio and then resized to match a 1000x750 size. Moves photos **unnormalized** -> **preaugmented**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xaN5wepXCy3"
      },
      "source": [
        "# importing the module\n",
        "\n",
        "\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "\n",
        "import whatimage\n",
        "import pyheif\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def decodeImage(bytesIo):\n",
        "    with open(bytesIo, 'rb') as f:\n",
        "      data = f.read()\n",
        "      fmt = whatimage.identify_image(data)\n",
        "      if fmt in ['heic', 'avif']:\n",
        "        i = pyheif.read_heif(data)\n",
        "        pi = Image.frombytes(mode=i.mode, size=i.size, data=i.data)\n",
        "        pi.save(\"heic.jpg\", format=\"jpeg\")\n",
        "\n",
        "def read_heic(path: str):\n",
        "    img = Wimage(path)\n",
        "    img.format = 'jpg'\n",
        "    img.save(filename=\"heic.jpg\")\n",
        "    img.close()\n",
        "\n",
        "def openImage(fileName):\n",
        "  decodeImage(fileName)\n",
        "  return Image.open(\"heic.jpg\")\n",
        "\n",
        "def cropAndSave(image, fileName):\n",
        "    width = image.size[0]\n",
        "    height = image.size[1]\n",
        "\n",
        "    aspect = width / float(height)\n",
        "\n",
        "    if (height > width):\n",
        "        image = image.rotate(90, Image.NEAREST, expand=1)\n",
        "        width = image.size[0]\n",
        "        height = image.size[1]\n",
        "\n",
        "    ideal_width = 1000\n",
        "    ideal_height = 750\n",
        "\n",
        "    ideal_aspect = ideal_width / float(ideal_height)\n",
        "\n",
        "    if aspect > ideal_aspect:\n",
        "        # Then crop the left and right edges:\n",
        "        new_width = int(ideal_aspect * height)\n",
        "        offset = (width - new_width) / 2\n",
        "        resize = (offset, 0, width - offset, height)\n",
        "    else:\n",
        "        # ... crop the top and bottom:\n",
        "        new_height = int(width / ideal_aspect)\n",
        "        offset = (height - new_height) / 2\n",
        "        resize = (0, offset, width, height - offset)\n",
        "\n",
        "    thumb = image.crop(resize).resize((ideal_width, ideal_height), Image.ANTIALIAS)\n",
        "    thumb.save(fileName)\n",
        "\n",
        "\n",
        "\n",
        "def normalizeRawImages(superpoch, dataSetNumber):\n",
        "\n",
        "  print('##### NORMALIZING - superpoch: ' + superpoch + ' - dataset: ' + dataSetNumber)\n",
        "\n",
        "  negativeFromDir = '/content/drive/MyDrive/Moire/unnormalized/negative/' + dataSetNumber + '/'\n",
        "  positiveFromDir = '/content/drive/MyDrive/Moire/unnormalized/positive/' + dataSetNumber + '/'\n",
        "  negativeToDir = '/content/drive/MyDrive/Moire/preaugmented/negative/' + dataSetNumber + '/'\n",
        "  positiveToDir = '/content/drive/MyDrive/Moire/preaugmented/positive/' + dataSetNumber + '/'\n",
        "\n",
        "  if not os.path.exists(positiveToDir):\n",
        "      os.makedirs(positiveToDir)\n",
        "  if not os.path.exists(negativeToDir):\n",
        "      os.makedirs(negativeToDir)\n",
        "\n",
        "  if len([name for name in os.listdir(positiveToDir)]) > 0:\n",
        "    print('Directory ' + positiveToDir + ' already has normalized photos in it.  Skipping normalization for negative photos.')\n",
        "  if len([name for name in os.listdir(negativeToDir)]) > 0:\n",
        "    print('Directory ' + negativeToDir + ' already has normalized photos in it.  Skipping normalization for positive photos.')\n",
        "\n",
        "  positiveImageFiles = [f for f in listdir(positiveFromDir) if (isfile(join(positiveFromDir, f)))]\n",
        "  negativeImageFiles = [f for f in listdir(negativeFromDir) if (isfile(join(negativeFromDir, f)))]\n",
        "\n",
        "  # LLList = [l for l in positiveImageFiles if 'LLL' in l]\n",
        "  # print(LLList)\n",
        "\n",
        "  if len([name for name in os.listdir(positiveToDir)]) <= 0:\n",
        "    for f in positiveImageFiles:\n",
        "        print(join(positiveFromDir, f))\n",
        "        img = openImage(join(positiveFromDir, f))\n",
        "\n",
        "        rgb_im = img.convert(\"RGB\")\n",
        "        components = f.split('.')\n",
        "        newComponents = components[:len(components) - 1]\n",
        "        newComponents.append('jpg')\n",
        "        newFileName = '.'.join(newComponents)\n",
        "\n",
        "        cropAndSave(rgb_im, join(positiveToDir, newFileName))\n",
        "\n",
        "  if len([name for name in os.listdir(negativeToDir)]) <= 0:\n",
        "    for f in negativeImageFiles:\n",
        "        print(join(negativeFromDir, f))\n",
        "        img = Image.open(join(negativeFromDir, f))\n",
        "\n",
        "        rgb_im = img.convert(\"RGB\")\n",
        "        components = f.split('.')\n",
        "        newComponents = components[:len(components) - 1]\n",
        "        newComponents.append('jpg')\n",
        "        newFileName = '.'.join(newComponents)\n",
        "\n",
        "        cropAndSave(rgb_im, join(negativeToDir, newFileName))\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9o2VLHoqyOo"
      },
      "source": [
        "#This function(fwdHaarDWT2D) computes the 2D Wavelet Transform in the image. All the input images are passed through a Haar Wavelet Decomposition module, to get the LL, LH, HL and HHH component of the image\n",
        "\n",
        "import numpy as np\n",
        "import pywt\n",
        "\n",
        "def splitFreqBands(img, levRows, levCols):\n",
        "    halfRow = int(levRows/2)\n",
        "    halfCol = int(levCols/2)\n",
        "    LL = img[0:halfRow, 0:halfCol]\n",
        "    LH = img[0:halfRow, halfCol:levCols]\n",
        "    HL = img[halfRow:levRows, 0:halfCol]\n",
        "    HH = img[halfRow:levRows, halfCol:levCols]\n",
        "    \n",
        "    return LL, LH, HL, HH\n",
        "    \n",
        "def haarDWT1D(data, length):\n",
        "    avg0 = 0.5;\n",
        "    avg1 = 0.5;\n",
        "    dif0 = 0.5;\n",
        "    dif1 = -0.5;\n",
        "    temp = np.empty_like(data)\n",
        "    temp = temp.astype(float)\n",
        "    h = int(length/2)\n",
        "    for i in range(h):\n",
        "        k = i*2\n",
        "        temp[i] = data[k] * avg0 + data[k + 1] * avg1;\n",
        "        temp[i + h] = data[k] * dif0 + data[k + 1] * dif1;\n",
        "    \n",
        "    data[:] = temp\n",
        "\n",
        "# computes the homography coefficients for PIL.Image.transform using point correspondences\n",
        "def fwdHaarDWT2D(img):\n",
        "    img = np.array(img)\n",
        "    levRows = img.shape[0];\n",
        "    levCols = img.shape[1];\n",
        "    img = img.astype(float)\n",
        "    for i in range(levRows):\n",
        "        row = img[i,:]\n",
        "        haarDWT1D(row, levCols)\n",
        "        img[i,:] = row\n",
        "    for j in range(levCols):\n",
        "        col = img[:,j]\n",
        "        haarDWT1D(col, levRows)\n",
        "        img[:,j] = col\n",
        "        \n",
        "    return splitFreqBands(img, levRows, levCols)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a63KHmZYqa_K"
      },
      "source": [
        "# from PIL import Image\n",
        "# from matplotlib import pyplot as plt\n",
        "# !ls /content/drive/MyDrive/Moire/\n",
        "# img = Image.open('/content/drive/MyDrive/Moire/preaugmented/positive/001/IMG_2906.jpg').convert('L')\n",
        "# img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "# img.save('chl.jpg')\n",
        "# LL, LH, HL, HH = fwdHaarDWT2D(img)\n",
        "# fig, axes = plt.subplots(2, 2)\n",
        "# fig.tight_layout()\n",
        "# axes[0, 0].imshow(LL)\n",
        "# axes[0, 1].imshow(LH)\n",
        "# axes[1, 0].imshow(HL)\n",
        "# axes[1, 1].imshow(HH)\n",
        "# axes[0, 0].set_title(\"LL\")\n",
        "# axes[0, 1].set_title(\"LH\")\n",
        "# axes[1, 0].set_title(\"HL\")\n",
        "# axes[1, 1].set_title(\"HH\")\n",
        "# plt.show()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNnfNmoEqa_M"
      },
      "source": [
        "## Augment normalized data\n",
        "The training images need to be put in two folders. positiveImages and negativeImages. positiveImages are the images which are captured from the display devices and has the presence of stron or weak Moiré patterms in it.\n",
        "negativeImages are the ones without Moiré Patterns (i.e. the images which are not captured from the display devices).  Moves photos **preaugmented** -> **train/test** based on a split ratio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO8AKkFsqm5W"
      },
      "source": [
        "import sys\n",
        "import argparse\n",
        "from PIL import Image\n",
        "from PIL import ImageOps\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "\n",
        "#The training images need to be put in two folders. positiveImages and negativeImages. positiveImages are the images which are captured from the display devices and has the presence of stron or weak Moiré patterms in it. negativeImages are the ones without Moiré Patterns (i.e. the images which are not captured from the display devices)\n",
        "\n",
        "\n",
        "def augmentNormalizedData(superpoch, dataSetNumber):\n",
        "\n",
        "  print('##### AUGMENTING - superpoch: ' + superpoch + ' - dataset: ' + dataSetNumber)\n",
        "\n",
        "  negativeFromDir = '/content/drive/MyDrive/Moire/preaugmented/negative/' + dataSetNumber + '/'\n",
        "  positiveFromDir = '/content/drive/MyDrive/Moire/preaugmented/positive/' + dataSetNumber + '/'\n",
        "\n",
        "  negativeToTrainDir = '/content/drive/MyDrive/Moire/train/negative/' + dataSetNumber + '/'\n",
        "  positiveToTrainDir = '/content/drive/MyDrive/Moire/train/positive/' + dataSetNumber + '/'\n",
        "  negativeToTestDir = '/content/drive/MyDrive/Moire/test/negative/' + dataSetNumber + '/'\n",
        "  positiveToTestDir = '/content/drive/MyDrive/Moire/test/positive/' + dataSetNumber + '/'\n",
        "\n",
        "  if not os.path.exists(negativeToTrainDir):\n",
        "      os.makedirs(negativeToTrainDir)\n",
        "  if not os.path.exists(positiveToTrainDir):\n",
        "      os.makedirs(positiveToTrainDir)\n",
        "  if not os.path.exists(negativeToTestDir):\n",
        "      os.makedirs(negativeToTestDir)\n",
        "  if not os.path.exists(positiveToTestDir):\n",
        "      os.makedirs(positiveToTestDir)\n",
        "\n",
        "  if len([name for name in os.listdir(negativeToTrainDir)]) > 0:\n",
        "    print('Directory ' + negativeToTrainDir + ' already has normalized photos in it.  Skipping augmentation for negative photos.')\n",
        "  if len([name for name in os.listdir(positiveToTrainDir)]) > 0:\n",
        "    print('Directory ' + positiveToTrainDir + ' already has normalized photos in it.  Skipping augmentation for positive photos.')\n",
        "  if len([name for name in os.listdir(negativeToTestDir)]) > 0:\n",
        "    print('Directory ' + negativeToTestDir + ' already has normalized photos in it.  Skipping augmentation for negative photos.')\n",
        "  if len([name for name in os.listdir(positiveToTestDir)]) > 0:\n",
        "    print('Directory ' + positiveToTestDir + ' already has normalized photos in it.  Skipping augmentation for positive photos.')\n",
        "        \n",
        "  createTrainingData(positiveFromDir, negativeFromDir, positiveToTrainDir, negativeToTrainDir, positiveToTestDir, negativeToTestDir)\n",
        "\n",
        "    \n",
        "#The wavelet decomposed images are the transformed images representing the spatial and the frequency information of the image. These images are stored as 'tiff' in the disk, to preserve that information. Each image is transformed with 180 degrees rotation and as well flipped, as part of data augmentation.\n",
        "\n",
        "def transformImageAndSave(image, f, customStr, path):\n",
        "    cA, cH, cV, cD  = fwdHaarDWT2D(image);\n",
        "    \n",
        "    fileName = (os.path.splitext(f)[0])\n",
        "    fLL = (f.replace(fileName, fileName+'_' + customStr + 'LL')).replace('.jpg','.tiff')\n",
        "    fLH = (f.replace(fileName, fileName+'_' + customStr + 'LH')).replace('.jpg','.tiff')\n",
        "    fHL = (f.replace(fileName, fileName+'_' + customStr + 'HL')).replace('.jpg','.tiff')\n",
        "    fHH = (f.replace(fileName, fileName+'_' + customStr + 'HH')).replace('.jpg','.tiff')\n",
        "\n",
        "    cA = Image.fromarray(cA)\n",
        "    cH = Image.fromarray(cH)\n",
        "    cV = Image.fromarray(cV)\n",
        "    cD = Image.fromarray(cD)\n",
        "\n",
        "    cA.save(join(path, fLL))\n",
        "    cH.save(join(path, fLH))\n",
        "    cV.save(join(path, fHL))\n",
        "    cD.save(join(path, fHH))\n",
        "    \n",
        "    \n",
        "def augmentAndTrasformImage(f, mainFolder, trainFolder):\n",
        "    try:\n",
        "        print(join(mainFolder, f))\n",
        "        img = Image.open(join(mainFolder, f)) \n",
        "    except:\n",
        "        print('Error: Couldnt read the file {}. Make sure only images are present in the folder'.format(f))\n",
        "        return None\n",
        "\n",
        "    imgGray = img.convert('L')\n",
        "    wdChk, htChk = imgGray.size\n",
        "    if htChk > wdChk:\n",
        "        imgGray = imgGray.rotate(-90, expand=1)\n",
        "        print('training image rotated')\n",
        "    transformImageAndSave(imgGray, f, '', trainFolder)\n",
        "\n",
        "    imgGray = imgGray.transpose(Image.ROTATE_180)\n",
        "    transformImageAndSave(imgGray, f, '180_', trainFolder)\n",
        "\n",
        "    imgGray = imgGray.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    transformImageAndSave(imgGray, f, '180_FLIP_', trainFolder)\n",
        "    \n",
        "    return True\n",
        "    \n",
        "    \n",
        "def createTrainingData(positiveFromDir, negativeFromDir, positiveToTrainDir, negativeToTrainDir, positiveToTestDir, negativeToTestDir):\n",
        "\n",
        "    print('positive image path: ' + positiveFromDir)\n",
        "    print('negative image path: ' + negativeFromDir)\n",
        "    splitRatio = 0.8\n",
        "\n",
        "    # get image files by classes\n",
        "    positiveImageFiles = [f for f in listdir(positiveFromDir) if (isfile(join(positiveFromDir, f)))]\n",
        "    negativeImageFiles = [f for f in listdir(negativeFromDir) if (isfile(join(negativeFromDir, f)))]\n",
        "\n",
        "    positiveDataBorder = round(len(positiveImageFiles) * splitRatio)\n",
        "    negativeDataBorder = round(len(negativeImageFiles) * splitRatio)\n",
        "\n",
        "    positiveTrainFiles = positiveImageFiles[:positiveDataBorder]\n",
        "    positiveTestFiles = positiveImageFiles[positiveDataBorder:]\n",
        "    negativeTrainFiles = negativeImageFiles[:negativeDataBorder]\n",
        "    negativeTestFiles = negativeImageFiles[negativeDataBorder:]\n",
        "\n",
        "    print('positive train samples: ' + str(len(positiveTrainFiles)))\n",
        "    print('negative train samples: ' + str(len(negativeTrainFiles)))\n",
        "    print('positive test samples: ' + str(len(positiveTestFiles)))\n",
        "    print('negative test samples: ' + str(len(negativeTestFiles)))\n",
        "\n",
        "    Knegative = 0\n",
        "    Kpositive = 0\n",
        "\n",
        "    # create positive training images\n",
        "    if len([name for name in os.listdir(positiveToTrainDir)]) <= 0:\n",
        "      for f in positiveTrainFiles:\n",
        "          ret = augmentAndTrasformImage(f, positiveFromDir, positiveToTrainDir)\n",
        "          if ret == None:\n",
        "              continue\n",
        "          Kpositive += 3\n",
        "\n",
        "    if len([name for name in os.listdir(negativeToTrainDir)]) <= 0:\n",
        "      # create positive test images\n",
        "      for f in negativeTrainFiles:\n",
        "          ret = augmentAndTrasformImage(f, negativeFromDir, negativeToTrainDir)\n",
        "          if ret == None:\n",
        "              continue\n",
        "          Kpositive += 3\n",
        "\n",
        "    if len([name for name in os.listdir(positiveToTestDir)]) <= 0:\n",
        "      # create negative training images\n",
        "      for f in positiveTestFiles:\n",
        "          ret = augmentAndTrasformImage(f, positiveFromDir, positiveToTestDir)\n",
        "          if ret == None:\n",
        "              continue\n",
        "          Knegative += 3;\n",
        "\n",
        "    if len([name for name in os.listdir(negativeToTestDir)]) <= 0:\n",
        "      # create negative training images\n",
        "      for f in negativeTestFiles:\n",
        "          ret = augmentAndTrasformImage(f, negativeFromDir, negativeToTestDir)\n",
        "          if ret == None:\n",
        "              continue\n",
        "          Knegative += 3;\n",
        "    #\n",
        "    # print('Total positive files after augmentation: ', Kpositive)\n",
        "    # print('Total negative files after augmentation: ', Knegative)\n",
        "    \n",
        "\n",
        "\n",
        "# mainAugment('/content/drive/MyDrive/Moire/preaugmentedPositiveImages', '/content/drive/MyDrive/Moire/preaugmentedNegativeImages')\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzpWIkVNqa_M"
      },
      "source": [
        "# from os import listdir\n",
        "# from os.path import isfile, join\n",
        "# from PIL import Image\n",
        "\n",
        "\n",
        "# positiveImagePath = '/content/drive/MyDrive/Moire/preaugmentedPositiveImages'\n",
        "# negativeImagePath = '/content/drive/MyDrive/Moire/preaugmentedNegativeImages'\n",
        "\n",
        "# mainAugment(positiveImagePath, negativeImagePath, 0)\n",
        "   "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNTveLG9tEf6"
      },
      "source": [
        "# !cd ../\n",
        "# !ls"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INPlfs7QQXYJ"
      },
      "source": [
        "## Load Data into Memory\n",
        "Get your big boy pants on because it's going to be a lot of data.  Increase runtime memory.  This section will load data from an **augmented** directory and loads it into a tensor for training or evaluation.  Recommended to make sure that there is no accelerator used so that it can be used when training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOTnUFotxJJJ"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import argparse\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from skimage import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#constants\n",
        "WIDTH = 500#384\n",
        "HEIGHT = 375#512\n",
        "\n",
        "#Here, we perform index based splitting and use those indices to split the our multi-input datasets. This is done because the CNN model is multi-input network\n",
        "def splitTrainTestDataForBands(inputData, X_train_ind, X_test_ind):\n",
        "    X_train = np.zeros((len(X_train_ind), WIDTH*HEIGHT))\n",
        "    for i in range(len(X_train_ind)):\n",
        "        X_train[i,:] = inputData[int(X_train_ind[i,0]),:]\n",
        "        \n",
        "    X_test = np.zeros((len(X_test_ind), WIDTH*HEIGHT))\n",
        "    for i in range(len(X_test_ind)):\n",
        "        X_test[i,:] = inputData[int(X_test_ind[i,0]),:]\n",
        "        \n",
        "    return X_train, X_test\n",
        "\n",
        "\n",
        "def countPositiveSamplesAfterSplit(trainData):\n",
        "    count = 0;\n",
        "    for i in range(len(trainData)):\n",
        "        if(trainData[i,0] == 0):\n",
        "            count = count + 1\n",
        "    return count\n",
        "\n",
        "def scaleData(inp, minimum, maximum):\n",
        "    minMaxScaler = preprocessing.MinMaxScaler(copy=True, feature_range=(minimum,maximum))\n",
        "    inp = inp.reshape(-1, 1)\n",
        "    inp = minMaxScaler.fit_transform(inp)\n",
        "    \n",
        "    return inp\n",
        "\n",
        "def readAndScaleImage(f, customStr, trainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, sampleVal):\n",
        "    fileName = (os.path.splitext(f)[0])\n",
        "    # print(fileName)\n",
        "    # print(customStr)\n",
        "\n",
        "    fLL = (f.replace(fileName, fileName + customStr)).replace('.jpg','.tiff')\n",
        "    fLH = (f.replace(fileName, fileName + customStr)).replace('.jpg','.tiff')\n",
        "    fHL = (f.replace(fileName, fileName + customStr)).replace('.jpg','.tiff')\n",
        "    fHH = (f.replace(fileName, fileName + customStr)).replace('.jpg','.tiff')\n",
        "    \n",
        "    try:\n",
        "        imgLL = Image.open(join(trainImagePath, fLL))\n",
        "        imgLH = Image.open(join(trainImagePath, fLH))\n",
        "        imgHL = Image.open(join(trainImagePath, fHL))\n",
        "        imgHH = Image.open(join(trainImagePath, fHH))\n",
        "    except Exception as e:\n",
        "        print('Error: Couldnt read the file {}. Make sure only images are present in the folder'.format(fileName))\n",
        "        print('Exception:', e)\n",
        "        return None\n",
        "        \n",
        "    imgLL = np.array(imgLL)\n",
        "    imgLH = np.array(imgLH)\n",
        "    imgHL = np.array(imgHL)\n",
        "    imgHH = np.array(imgHH)\n",
        "    imgLL = scaleData(imgLL, 0, 1)\n",
        "    imgLH = scaleData(imgLH, -1, 1)\n",
        "    imgHL = scaleData(imgHL, -1, 1)\n",
        "    imgHH = scaleData(imgHH, -1, 1)\n",
        "    \n",
        "    imgVector = imgLL.reshape(1, WIDTH*HEIGHT)\n",
        "    X_LL[sampleIndex, :] = imgVector\n",
        "    imgVector = imgLH.reshape(1, WIDTH*HEIGHT)\n",
        "    X_LH[sampleIndex, :] = imgVector\n",
        "    imgVector = imgHL.reshape(1, WIDTH*HEIGHT)\n",
        "    X_HL[sampleIndex, :] = imgVector\n",
        "    imgVector = imgHH.reshape(1, WIDTH*HEIGHT)\n",
        "    X_HH[sampleIndex, :] = imgVector\n",
        "    \n",
        "    Y[sampleIndex, 0] = sampleVal;\n",
        "    X_index[sampleIndex, 0] = sampleIndex;\n",
        "\n",
        "    imgVector = None\n",
        "    imgLL = None\n",
        "    imgLH = None\n",
        "    imgHL = None\n",
        "    imgHH = None\n",
        "    \n",
        "    return True\n",
        "\n",
        "def reshapeData(X_LL, X_LH, X_HL, X_HH, X_index, Y, imageCount):\n",
        "\n",
        "    print('Dataset length: ' + str(len(X_LL)))\n",
        "    \n",
        "    print('num_train_samples', len(X_LL))\n",
        "    X_LL = np.array(X_LL)\n",
        "    X_LL = X_LL.reshape((len(X_LL), HEIGHT, WIDTH, 1))\n",
        "\n",
        "    X_LH = np.array(X_LH)\n",
        "    X_LH = X_LH.reshape((len(X_LH), HEIGHT, WIDTH, 1))\n",
        "\n",
        "    X_HL = np.array(X_HL)\n",
        "    X_HL = X_HL.reshape((len(X_HL), HEIGHT, WIDTH, 1))\n",
        "    \n",
        "    X_HH = np.array(X_HH)\n",
        "    X_HH = X_HH.reshape((len(X_HH), HEIGHT, WIDTH, 1))\n",
        "\n",
        "    Y = np.array(Y)\n",
        "    \n",
        "    return X_LL, X_LH, X_HL, X_HH, Y\n",
        "\n",
        "def readImageSet(imageFiles, trainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, bClass):\n",
        "\n",
        "    for f in imageFiles:\n",
        "        ret = readAndScaleImage(f, '', trainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, bClass)\n",
        "        if ret == True:\n",
        "            sampleIndex = sampleIndex + 1\n",
        "\n",
        "    return sampleIndex\n",
        "\n",
        "def readWaveletData(positiveImagePath, negativeImagePath):\n",
        "    \n",
        "    # get augmented, balanced training data image files by class\n",
        "    positiveImageFiles = [f for f in listdir(positiveImagePath) if (isfile(join(positiveImagePath, f)))]\n",
        "    negativeImageFiles = [f for f in listdir(negativeImagePath) if (isfile(join(negativeImagePath, f)))]\n",
        "\n",
        "    positiveCount = len(positiveImageFiles)\n",
        "    negativeCount = len(negativeImageFiles)\n",
        "\n",
        "    print('positive samples: ' + str(positiveCount))\n",
        "    print('negative samples: ' + str(negativeCount))\n",
        "    imageCount = positiveCount + negativeCount\n",
        "    #intialization\n",
        "    X_LL = np.zeros((positiveCount + negativeCount, WIDTH*HEIGHT))\n",
        "    X_LH = np.zeros((positiveCount + negativeCount, WIDTH*HEIGHT))\n",
        "    X_HL = np.zeros((positiveCount + negativeCount, WIDTH*HEIGHT))\n",
        "    X_HH = np.zeros((positiveCount + negativeCount, WIDTH*HEIGHT))\n",
        "    X_index = np.zeros((positiveCount + negativeCount, 1))\n",
        "    Y = np.zeros((positiveCount + negativeCount, 1))\n",
        "    \n",
        "    sampleIndex = 0\n",
        "    # read all images, convert to float, divide by 255 (leads to gray range 0..1), reshape into a row vector\n",
        "    # write class 0 for positive and 1 for negative samples\n",
        "\n",
        "    sampleIndex = readImageSet(positiveImageFiles, positiveImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, 0)\n",
        "    print('positive data loaded.')\n",
        "    \n",
        "    sampleIndex += readImageSet(negativeImageFiles, negativeImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, 1)\n",
        "    print('negative data loaded.')\n",
        "\n",
        "    print('Total Samples Loaded: ', sampleIndex)\n",
        "    \n",
        "    X_LL, X_LH, X_HL, X_HH, Y = shuffle(X_LL, X_LH, X_HL, X_HH, Y, random_state=0)\n",
        "    \n",
        "    return X_LL, X_LH, X_HL, X_HH, X_index, Y, imageCount\n",
        "\n",
        "def mainReadDatafromDrive(dataSetNumber, dataType):\n",
        "\n",
        "    # dataType can be 'train' or 'test'\n",
        "\n",
        "    positiveFromTrainDir = '/content/drive/MyDrive/Moire/' + dataType + '/positive/' + dataSetNumber + '/'\n",
        "    negativeFromTrainDir = '/content/drive/MyDrive/Moire/' + dataType + '/negative/' + dataSetNumber + '/'\n",
        "\n",
        "    if not os.path.exists(positiveFromTrainDir):\n",
        "      print(\"ERROR: \" + positiveFromTrainDir + ' does not exist.  Exiting.')\n",
        "      raise ValueError('Directory does not exist')\n",
        "    if not os.path.exists(negativeFromTrainDir):\n",
        "      print(\"ERROR: \" + negativeFromTrainDir + ' does not exist.  Exiting.')\n",
        "      raise ValueError('Directory does not exist')\n",
        "    \n",
        "    X_LL, X_LH, X_HL, X_HH, X_index, Y, imageCount = readWaveletData(positiveFromTrainDir, negativeFromTrainDir)\n",
        "\n",
        "    X_LL, X_LH, X_HL, X_HH, Y = reshapeData(X_LL, X_LH, X_HL, X_HH, X_index, Y, imageCount)\n",
        "\n",
        "    return X_LL, X_LH, X_HL, X_HH, Y\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azf3IN-SRudm"
      },
      "source": [
        "## Train CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ASB86jIRxvK"
      },
      "source": [
        "import os\n",
        "\n",
        "from keras.models import Model # basic class for specifying and training a neural network\n",
        "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, Add, Multiply, Maximum\n",
        "\n",
        "def createModel(height, width, depth, num_classes):\n",
        "#     num_epochs = 20 # 50 26 200 # we iterate 200 times over the entire training set\n",
        "    kernel_size_1 = 7 # we will use 7x7 kernels \n",
        "    kernel_size_2 = 3 # we will use 3x3 kernels \n",
        "    pool_size = 2 # we will use 2x2 pooling throughout\n",
        "    conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n",
        "    conv_depth_2 = 16 # ...switching to 16 after the first pooling layer\n",
        "    drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
        "    drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
        "    hidden_size = 32 # 128 512 the FC layer will have 512 neurons\n",
        "\n",
        "\n",
        "    inpLL = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
        "    inpLH = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
        "    inpHL = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
        "    inpHH = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
        "    \n",
        "    conv_1_LL = Convolution2D(conv_depth_1, (kernel_size_1, kernel_size_1), padding='same', activation='relu')(inpLL)\n",
        "    conv_1_LH = Convolution2D(conv_depth_1, (kernel_size_1, kernel_size_1), padding='same', activation='relu')(inpLH)\n",
        "    conv_1_HL = Convolution2D(conv_depth_1, (kernel_size_1, kernel_size_1), padding='same', activation='relu')(inpHL)\n",
        "    conv_1_HH = Convolution2D(conv_depth_1, (kernel_size_1, kernel_size_1), padding='same', activation='relu')(inpHH)\n",
        "\n",
        "    pool_1_LL = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_1_LL)\n",
        "    pool_1_LH = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_1_LH)\n",
        "    pool_1_HL = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_1_HL)\n",
        "    pool_1_HH = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_1_HH)\n",
        "\n",
        "    avg_LH_HL_HH = Maximum()([pool_1_LH, pool_1_HL, pool_1_HH])\n",
        "    inp_merged = Multiply()([pool_1_LL, avg_LH_HL_HH])\n",
        "    print(inp_merged)\n",
        "    C4 = Convolution2D(conv_depth_2, (kernel_size_2, kernel_size_2), padding='same', activation='relu')(inp_merged)\n",
        "    S2 = MaxPooling2D(pool_size=(4, 4))(C4)\n",
        "    drop_1 = Dropout(drop_prob_1)(S2)\n",
        "    C5 = Convolution2D(conv_depth_1, (kernel_size_2, kernel_size_2), padding='same', activation='relu')(drop_1)\n",
        "    S3 = MaxPooling2D(pool_size=(pool_size, pool_size))(C5)\n",
        "    C6 = Convolution2D(conv_depth_1, (kernel_size_2, kernel_size_2), padding='same', activation='relu')(S3)\n",
        "    S4 = MaxPooling2D(pool_size=(pool_size, pool_size))(C6)\n",
        "    drop_2 = Dropout(drop_prob_1)(S4)\n",
        "    # Now flatten to 1D, apply FC -> ReLU (with dropout) -> softmax\n",
        "    flat = Flatten()(drop_2)\n",
        "    hidden = Dense(hidden_size, activation='relu')(flat)\n",
        "    drop_3 = Dropout(drop_prob_2)(hidden)\n",
        "    out = Dense(num_classes, activation='softmax')(drop_3)\n",
        "    \n",
        "    model = Model(inputs=[inpLL, inpLH, inpHL, inpHH], outputs=out) # To define a model, just specify its input and output layers\n",
        "    intermediate_model = Model(inputs=[inpLL, inpLH, inpHL, inpHH], outputs=inp_merged)\n",
        "    \n",
        "    return model, intermediate_model"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTX2vudptIaO"
      },
      "source": [
        "#To detect Moire ́ patternzs, images are first decomposed using Wavelet decomposition (refer to file '') and trained using multi-input Convolutional neural network. The strength of the proposed CNN model is, it uses the LL intensity image (from the Wavelet decomposition) as a weight parameter for the Moire ́ pattern, thereby approximating the spatial spread of the Moire ́ pattern in the image. Usage of CNN model performs better than frequency thresholding approach as the model is trained considering diverse scenarios and it is able to distinguish between the high frequency of background texture and the Moire ́ pattern.\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import argparse\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from skimage import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import time\n",
        "from tensorflow import keras\n",
        "\n",
        "# - read positive and negative training data\n",
        "# - create X and Y from training data\n",
        "\n",
        "\n",
        "def trainMoire(superpoch, dataSetNumber, numEpochs, weights_file = None):\n",
        "\n",
        "    print('##### TRAINING - superpoch: ' + superpoch + ' - dataset: ' + dataSetNumber[0] + ', ', dataSetNumber[1])\n",
        "\n",
        "    X_LL, X_LH, X_HL, X_HH, Y = mainReadDatafromDrive(dataSetNumber[0], 'train')\n",
        "    X_LL_2, X_LH_2, X_HL_2, X_HH_2, Y_2 = mainReadDatafromDrive(dataSetNumber[1], 'train')\n",
        "\n",
        "\n",
        "    print('Concatenating 2 datasets.')\n",
        "    print(X_LL.shape)\n",
        "    print(X_LL_2.shape)\n",
        "    X_LL = np.concatenate((X_LL, X_LL_2), axis=0)\n",
        "    X_LH = np.concatenate((X_LH, X_LH_2), axis=0)\n",
        "    X_HL = np.concatenate((X_HL, X_HL_2), axis=0)\n",
        "    X_HH = np.concatenate((X_HH, X_HH_2), axis=0)\n",
        "    Y = np.concatenate((Y, Y_2), axis=0)\n",
        "    X_LL_2 = None\n",
        "    X_LH_2 = None\n",
        "    X_HL_2 = None\n",
        "    X_HH_2 = None\n",
        "    Y_2 = None\n",
        "\n",
        "    modelName = trainCNNModel(superpoch, dataSetNumber, X_LL, X_LH, X_HL, X_HH, Y, numEpochs, weights_file)\n",
        "    \n",
        "    X_LL = None\n",
        "    X_LH = None\n",
        "    X_HL = None \n",
        "    X_HH = None\n",
        "    Y = None\n",
        "\n",
        "    return modelName\n",
        "    # evaluate(model, X_LL_test,X_LH_test,X_HL_test,X_HH_test,Y_test)\n",
        "    \n",
        "\n",
        "def trainCNNModel(superpoch, dataSetNumber, X_LL_train, X_LH_train, X_HL_train, X_HH_train, y_train, num_epochs, weights_file):\n",
        "\n",
        "    batch_size = 32 # in each iteration, we consider 32 training examples at once\n",
        "    print(\"SHAPE\")\n",
        "    print(X_LL_train.shape);\n",
        "    num_train, height, width, depth = X_LL_train.shape\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    Y_train = np_utils.to_categorical(y_train, num_classes) # One-hot encode the labels\n",
        "    # Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels\n",
        "\n",
        "    checkPointFolder = '/content/drive/MyDrive/Moire/checkPoint'\n",
        "    checkpoint_name = checkPointFolder + '/mid-sp' + superpoch + '-ds' + dataSetNumber[0] + '_' + dataSetNumber[1] + '-ep{epoch:03d}-ls{val_loss:.5f}-ac{val_accuracy:.2f}-weights' \n",
        "    checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "    callbacks_list = [checkpoint]\n",
        "    \n",
        "    if not os.path.exists(checkPointFolder):\n",
        "        os.makedirs(checkPointFolder)\n",
        "        \n",
        "        \n",
        "    model = None\n",
        "    # if preloaded_model == None:\n",
        "    #   print('A preloaded model was not provided.  Creating a new one')\n",
        "    #   model, _ = createModel(height, width, depth, num_classes)\n",
        "\n",
        "    if weights_file != None:\n",
        "      print('--- Loading weights file: ' + weights_file)\n",
        "      model = keras.models.load_model(weights_file)\n",
        "    else:\n",
        "      print('--- No weights file provided.  Compiling a new one.')\n",
        "      \n",
        "      model, _ = createModel(height, width, depth, num_classes)\n",
        "      model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
        "                    optimizer='adam', # using the Adam optimiser\n",
        "                    metrics=['accuracy']) # reporting the accuracy\n",
        "      # if preloaded_model == None:\n",
        "        \n",
        "      # else:\n",
        "      #   print('--- Model already loaded with weights.  Skipping compilation')\n",
        "\n",
        "    model.fit([X_LL_train,X_LH_train,X_HL_train,X_HH_train], Y_train,                # Train the model using the training set...\n",
        "              batch_size=batch_size, epochs=num_epochs,\n",
        "              verbose=1, validation_split=0.1, callbacks=callbacks_list) # ...holding out 10% of the data for validation\n",
        "    # score, acc = model.evaluate([X_LL_test,X_LH_test,X_HL_test,X_HH_test], Y_test, verbose=1)  # Evaluate the trained model on the test set!\n",
        "\n",
        "    # print('------ weights ------')\n",
        "    # for i in range(len(model.layers)):\n",
        "    #   print(len(model.layers[i].get_weights()))\n",
        "\n",
        "    modelName = '/content/drive/MyDrive/Moire/checkPoint/final-tm' + str(time.time()).split('.')[0] + '-sp' + superpoch + '-ds' + dataSetNumber[0] + '_' + dataSetNumber[1] + '-weights'\n",
        "    model.save(modelName)\n",
        "    \n",
        "    return modelName\n",
        "\n",
        "\n",
        "def evaluate(model, X_LL_test,X_LH_test,X_HL_test,X_HH_test,y_test):\n",
        "\n",
        "    model_out = model.predict([X_LL_test,X_LH_test,X_HL_test,X_HH_test])\n",
        "    passCnt = 0\n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    for i in range(len(y_test)):\n",
        "        if np.argmax(model_out[i, :]) == y_test[i]:\n",
        "            str_label='Pass'\n",
        "            passCnt = passCnt + 1\n",
        "        else:\n",
        "            str_label='Fail'\n",
        "\n",
        "        if y_test[i] ==0:\n",
        "            if np.argmax(model_out[i, :]) == y_test[i]:\n",
        "                TP = TP + 1;\n",
        "            else:\n",
        "                FN = FN + 1\n",
        "        else:\n",
        "            if np.argmax(model_out[i, :]) == y_test[i]:\n",
        "                TN = TN + 1;\n",
        "            else:\n",
        "                FP = FP + 1\n",
        "\n",
        "    start = \"\\033[1m\"\n",
        "    end = \"\\033[0;0m\"\n",
        "    print(start + 'confusion matrix (test / validation)' + end)\n",
        "    print(start + 'true positive:  '+ end + str(TP))\n",
        "    print(start + 'false positive: '+ end + str(FP))\n",
        "    print(start + 'true negative:  '+ end + str(TN))\n",
        "    print(start + 'false negative: '+ end + str(FN))\n",
        "    print('\\n')\n",
        "\n",
        "    if TP+FP+FN+TN != 0:\n",
        "      print(start + 'accuracy:  ' + end + \"{:.4f} %\".format(100*(TP+TN)/(TP+FP+FN+TN)))\n",
        "    else:\n",
        "      print(start + 'accuracy:  ' + end + \"{:.4f} %\".format(100))\n",
        "\n",
        "    if TP + FP != 0:\n",
        "      print(start + 'precision: ' + end + \"{:.4f} %\".format(100*TP/(TP + FP)))\n",
        "    else:\n",
        "      print(start + 'precision: ' + end + \"{:.4f} %\".format(100))\n",
        "\n",
        "    if TP + FN != 0:\n",
        "      print(start + 'recall:  ' + end + \"{:.4f} %\".format(100*TP/(TP + FN)))\n",
        "    else:\n",
        "      print(start + 'recall:  ' + end + \"{:.4f} %\".format(100))\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "120i4Ry-qa_N"
      },
      "source": [
        "\n",
        "# !ls /content/drive/MyDrive/Moire/checkPoint/\n",
        "# from google.colab import files\n",
        "# files.download('content/drive/MyDrive/Moire/checkPoint/Weights-028--0.06866.hdf5')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWnNYpcTqa_O"
      },
      "source": [
        "## Test CNN Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV0rGdyIueOf"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import argparse\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from skimage import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "#constants\n",
        "width = 500#384 #change dimensions according to the input image in the training\n",
        "height = 375#512 #change dimensions according to the input image in the training\n",
        "depth = 1\n",
        "num_classes = 2\n",
        "\n",
        "def testMoire(weightsFile, superpoch, dataSetNumber):\n",
        "\n",
        "    print('##### TESTING - superpoch: ' + superpoch + ' - dataset: ' + dataSetNumber[0] + ', ' + dataSetNumber[1])\n",
        "    X_LL, X_LH, X_HL, X_HH, Y = mainReadDatafromDrive(dataSetNumber[0], 'test')\n",
        "    X_LL_2, X_LH_2, X_HL_2, X_HH_2, Y_2 = mainReadDatafromDrive(dataSetNumber[1], 'test')\n",
        "\n",
        "    print('Concatenating 2 datasets.')\n",
        "    X_LL = np.concatenate((X_LL, X_LL_2), axis=0)\n",
        "    X_LH = np.concatenate((X_LH, X_LH_2), axis=0)\n",
        "    X_HL = np.concatenate((X_HL, X_HL_2), axis=0)\n",
        "    X_HH = np.concatenate((X_HH, X_HH_2), axis=0)\n",
        "    Y = np.concatenate((Y, Y_2), axis=0)\n",
        "    X_LL_2 = None\n",
        "    X_LH_2 = None\n",
        "    X_HL_2 = None\n",
        "    X_HH_2 = None\n",
        "    Y_2 = None\n",
        "    \n",
        "    CNN_model = keras.models.load_model(weightsFile)\n",
        "    evaluate(CNN_model,X_LL,X_LH,X_HL,X_HH, Y)\n",
        "    X_LL = None\n",
        "    X_LH = None\n",
        "    X_HL = None \n",
        "    X_HH = None\n",
        "    Y = None\n",
        "\n",
        "def run(model, X_LL_test,X_LH_test,X_HL_test,y_test):\n",
        "    return\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYSPiTtfqa_O"
      },
      "source": [
        "# weightsFile = \"moirePattern3CNN_.h5\"\n",
        "    \n",
        "    \n",
        "# mainTest('content/drive/MyDrive/Moire/checkPoint/moirePattern3CNN_.h5', '/content/drive/MyDrive/Moire/testDataPositive', '/content/drive/MyDrive/Moire/testDataNegative')\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTxUTR6Yv2LG"
      },
      "source": [
        "## Extended Training\n",
        "Take the trained model and continue training with additional data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "_fb9-mhtcBbo",
        "outputId": "3d0942c3-26e1-4f29-e954-21d864bb7828"
      },
      "source": [
        "# normalizeRawImages(\"001\", \"001\")\n",
        "# augmentNormalizedData(\"001\", \"001\")\n",
        "# normalizeRawImages(\"001\", \"002\")\n",
        "# augmentNormalizedData(\"001\", \"002\")\n",
        "# normalizeRawImages(\"001\", \"003\")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       001\", \"003\")\n",
        "# augmentNormalizedData(\"001\", \"003\")\n",
        "normalizeRawImages(\"001\", \"004\")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       001\", \"003\")\n",
        "# augmentNormalizedData(\"001\", \"004\")\n",
        "\n",
        "# weights_file = '/content/drive/MyDrive/Moire/checkPoint/mid-sp0010-ds003_001-ep005-ls0.00107-ac1.00-weights'\n",
        "# weights_file = None\n",
        "\n",
        "# superpoch_count = 10\n",
        "# dataset_count = 4\n",
        "# epochs = 5\n",
        "\n",
        "# for sp in range(superpoch_count):\n",
        "#   superpoch = '00' + str(sp + 1)\n",
        "#   print('---------------------------------------------------------')\n",
        "#   print('BEGINNING TRAINING OF SUPERPOCH: ' + superpoch)\n",
        "#   print('---------------------------------------------------------')\n",
        "#   for ds in range(dataset_count):\n",
        "#     dsNumber = ds + 1\n",
        "#     otherDsNumber = ((ds + 1) % dataset_count) + 1\n",
        "#     datasets = ['00' + str(dsNumber), '00' + str(otherDsNumber)]\n",
        "\n",
        "#     print(datasets)\n",
        "#     # normalizeRawImages(superpoch, dataset)\n",
        "#     # augmentNormalizedData(superpoch, dataset)\n",
        "\n",
        "#     weights_file = trainMoire(superpoch, datasets, epochs, weights_file)\n",
        "#     testMoire(weights_file, superpoch, datasets)\n",
        "#     print('---------------------------------------------------------')\n",
        "# /content/drive/MyDrive/Moire/preaugmented/positive/001/IMG_2866.HEIC\n",
        "# /content/drive/MyDrive/Moire/preaugmented/positive/001/IMG_2875.HEIC\n",
        "# /content/drive/MyDrive/Moire/preaugmented/positive/001/IMG_2893.HEIC\n",
        "# /content/drive/MyDrive/Moire/preaugmented/positive/002/IMG_2900.HEIC\n",
        "# /content/drive/MyDrive/Moire/preaugmented/positive/002/IMG_2904.HEIC\n",
        "# /content/drive/MyDrive/Moire/preaugmented/positive/002/IMG_2915.HEIC\n",
        "# /content/drive/MyDrive/Moire/preaugmented/positive/002/IMG_2921.HEIC\n",
        "# /content/drive/MyDrive/Moire/preaugmented/positive/002/IMG_2929.HEIC\n",
        "\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-54-849bd46cf416>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    normalizeRawImages(\"001\", \"004\")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       001\", \"003\")\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyiISSNr2ZAr"
      },
      "source": [
        "# X_LL_train, X_LH_train, X_HL_train, X_HH_train, Y_train = mainReadDatafromDrive('/content/drive/MyDrive/Moire/trainDataPositive', '/content/drive/MyDrive/Moire/trainDataNegative')\n",
        "# !ls /content/drive/MyDrive/Moire/preaugmented/positive/003\n",
        "print('negative unnormalized 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/negative/001')])))\n",
        "print('positive unnormalized 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/positive/001')])))\n",
        "print('negative unnormalized 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/negative/002')])))\n",
        "print('positive unnormalized 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/positive/002')])))\n",
        "print('negative unnormalized 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/negative/003')])))\n",
        "print('positive unnormalized 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/positive/003')])))\n",
        "# print('negative train 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/negative/001')])))\n",
        "# print('negative train 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/negative/002')])))\n",
        "# print('negative train 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/negative/003')])))\n",
        "# print('positive train 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/positive/001')])))\n",
        "# print('positive train 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/positive/002')])))\n",
        "# print('positive train 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/positive/003')])))\n",
        "# print('negative test 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/negative/001')])))\n",
        "# print('negative test 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/negative/002')])))\n",
        "# print('negative test 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/negative/003')])))\n",
        "# print('positive test 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/positive/001')])))\n",
        "# print('positive test 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/positive/002')])))\n",
        "# print('positive test 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/positive/003')])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9RFVMxIroLI"
      },
      "source": [
        "# testMoire('/content/drive/MyDrive/Moire/checkPoint/mid-sp005-ds002-ep016-ls0.00455-ac1.00-weights.h5', \"001\", \"001\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDO0lSriFOJt"
      },
      "source": [
        "def intermediateMoire(dataSetNumber, weights_file = None):\n",
        "\n",
        "    X_LL, X_LH, X_HL, X_HH, Y = mainReadDatafromDrive(dataSetNumber, 'test')\n",
        "    return createIntermediateModel(dataSetNumber, X_LL, X_LH, X_HL, X_HH, Y, weights_file)\n",
        "    \n",
        "\n",
        "def createIntermediateModel(dataSetNumber, X_LL_train, X_LH_train, X_HL_train, X_HH_train, y_train, weights_file):\n",
        "\n",
        "    batch_size = 32 # in each iteration, we consider 32 training examples at once\n",
        "    print(\"SHAPE\")\n",
        "    print(X_LL_train.shape);\n",
        "    num_train, height, width, depth = X_LL_train.shape\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    Y_train = np_utils.to_categorical(y_train, num_classes) # One-hot encode the labels\n",
        "    # Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels\n",
        "    \n",
        "    # if not os.path.exists(checkPointFolder):\n",
        "    #     os.makedirs(checkPointFolder)\n",
        "        \n",
        "        \n",
        "    model, intermediate_model = createModel(height, width, depth, num_classes)\n",
        "\n",
        "    if weights_file != None:\n",
        "      print('--- Loading weights file: ' + weights_file)\n",
        "      model.load_weights(weights_file)\n",
        "    else:\n",
        "      print('--- No weights file provided.  Creating new model.')\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
        "                  optimizer='adam', # using the Adam optimiser\n",
        "                  metrics=['accuracy']) # reporting the accuracy\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # weights_list = model.get_weights()\n",
        "    # for i in range(len(model.layers)):\n",
        "    #   print(model.layers[i].get_weights())\n",
        "      # print(model.layers[i].get_weights()[1])\n",
        "    \n",
        "    model_index = 0;\n",
        "    print('----------------')\n",
        "    for layer in intermediate_model.layers:\n",
        "      for weights in layer.get_weights():\n",
        "        weights = model.get_weights()[model_index]\n",
        "        model_index = model_index + 1\n",
        "\n",
        "    # print('----------------')\n",
        "\n",
        "    # for layer in model.layers:\n",
        "    #   for weights in layer.get_weights():\n",
        "    #     print(len(weights))\n",
        "    # print('----------------')\n",
        "    \n",
        "    # intermediate_model.layers[5].set_weights(weights_list[0])\n",
        "    # print(len(model.layers))\n",
        "    # intermediate_model.layers[i].set_weights(weights)\n",
        "\n",
        "    # intermediate_model.summary()\n",
        "\n",
        "    # model.fit([X_LL_train,X_LH_train,X_HL_train,X_HH_train], Y_train,                # Train the model using the training set...\n",
        "    #           batch_size=batch_size, epochs=num_epochs,\n",
        "    #           verbose=1, validation_split=0.1, callbacks=callbacks_list) # ...holding out 10% of the data for validation\n",
        "    # score, acc = model.evaluate([X_LL_test,X_LH_test,X_HL_test,X_HH_test], Y_test, verbose=1)  # Evaluate the trained model on the test set!\n",
        "\n",
        "    # modelName = '/content/drive/MyDrive/Moire/checkPoint/final-tm' + str(time.time()).split('.')[0] + '-sp' + superpoch + '-ds' + dataSetNumber + '-weights.h5'\n",
        "    # model.save(modelName)\n",
        "    \n",
        "    return intermediate_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27zETVwPEMCf"
      },
      "source": [
        "# from keras.models import Model\n",
        "\n",
        "# model = intermediateMoire(\"001\", '/content/drive/MyDrive/Moire/checkPoint/mid-sp005-ds002-ep016-ls0.00455-ac1.00-weights.h5')\n",
        "\n",
        "# model.summary()\n",
        "# X_LL, X_LH, X_HL, X_HH, Y = mainReadDatafromDrive(\"001\", 'test')\n",
        "# model_out = model.predict([X_LL, X_LH, X_HL, X_HH])\n",
        "\n",
        "\n",
        "# print(model_out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ESg5hvZl3u_"
      },
      "source": [
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "# model_shape = model_out.shape\n",
        "\n",
        "# reshaped_output = np.reshape(model_out, (model_shape[0], model_shape[3], model_shape[1], model_shape[2]))\n",
        "\n",
        "# print(reshaped_output.shape)\n",
        "# index = 0\n",
        "# # for image_batch in reshaped_output:\n",
        "# for image in reshaped_output[0]:\n",
        "#   # image_batch = np.reshape(reshaped_output, (model_shape[0], model_shape[3], model_shape[1], model_shape[2]))\n",
        "#   print(image.shape)\n",
        "\n",
        "#   largest = 0\n",
        "#   smallest = 100000\n",
        "\n",
        "#   for row in image:\n",
        "#     for i in row:\n",
        "#       if i > largest:\n",
        "#         largest = i\n",
        "#       if i < smallest:\n",
        "#         smallest = i\n",
        "\n",
        "#   value_range = largest - smallest\n",
        "#   scale_value = 255 / value_range\n",
        "\n",
        "#   for i in range(len(image)):\n",
        "#     for j in range(len(image[i])):\n",
        "#       image[i][j] = scale_value * image[i][j]\n",
        "#   print('largest: ' + str(largest))\n",
        "#   print('smallest: ' + str(smallest))\n",
        "#   im = Image.fromarray(image)\n",
        "#   index = index + 1\n",
        "\n",
        "#   if im.mode != 'RGB':\n",
        "#     im = im.convert('RGB')\n",
        "\n",
        "#   im.save('/content/drive/MyDrive/Moire/intermediate/test/negative/001/' + str(index) + \".jpg\")\n",
        "\n",
        "# print(index)\n",
        "# print(np.reshape(model_out, (model_shape[0], model_shape[3], model_shape[1], model_shape[2])).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnwQH5eYqa_O"
      },
      "source": [
        "# #constants\n",
        "# # width = 500#384 #change dimensions according to the input image in the training\n",
        "# # height = 375#512 #change dimensions according to the input image in the training\n",
        "# # depth = 1\n",
        "# # num_classes = 2\n",
        "\n",
        "# def continueTraining(weightsFile, X_LL_train, X_LH_train, X_HL_train, X_HH_train, Y_train, num_epochs):\n",
        "#     weights_file = (weightsFile)\n",
        "\n",
        "#     # X_LL, X_LH, X_HL, X_HH, Y = mainReadDatafromDrive(positiveImagePath, negativeImagePath)\n",
        "    \n",
        "    \n",
        "\n",
        "#     batch_size = 32 # in each iteration, we consider 32 training examples at once\n",
        "#     print(\"SHAPE\")\n",
        "#     print(X_LL_train.shape);\n",
        "#     num_train, height, width, depth = X_LL_train.shape\n",
        "#     num_classes = len(np.unique(Y_train))\n",
        "#     Y_train = np_utils.to_categorical(Y_train, num_classes) # One-hot encode the labels\n",
        "#     print(Y_train);\n",
        "#     # Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels\n",
        "\n",
        "#     CNN_model = createModel(height, width, depth, num_classes)\n",
        "#     CNN_model.load_weights(weights_file)\n",
        "\n",
        "#     checkPointFolder = 'content/drive/MyDrive/Moire/checkPoint'\n",
        "#     checkpoint_name = checkPointFolder + '/Weights-retrained-002--{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "#     checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "#     callbacks_list = [checkpoint]\n",
        "    \n",
        "#     if not os.path.exists(checkPointFolder):\n",
        "#         os.makedirs(checkPointFolder)\n",
        "        \n",
        "        \n",
        "#     # model = createModel(height, width, depth, num_classes)\n",
        "    \n",
        "#     CNN_model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
        "#                   optimizer='adam', # using the Adam optimiser\n",
        "#                   metrics=['accuracy']) # reporting the accuracy\n",
        "\n",
        "#     CNN_model.fit([X_LL_train,X_LH_train,X_HL_train,X_HH_train], Y_train,                # Train the model using the training set...\n",
        "#               batch_size=batch_size, epochs=num_epochs,\n",
        "#               verbose=1, validation_split=0.1, callbacks=callbacks_list) # ...holding out 10% of the data for validation\n",
        "#     # score, acc = model.evaluate([X_LL_test,X_LH_test,X_HL_test,X_HH_test], Y_test, verbose=1)  # Evaluate the trained model on the test set!\n",
        "\n",
        "#     CNN_model.save('content/drive/MyDrive/Moire/checkPoint/retrained-002-moirePattern3CNN_.h5')\n",
        "    \n",
        "#     return model\n",
        "#     # evaluate(CNN_model,X_LL,X_LH,X_HL,X_HH, Y)\n",
        "# import time\n",
        "# str(time.time()).split('.')[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fyxBfYY4i7U"
      },
      "source": [
        "# continueTraining('content/drive/MyDrive/Moire/checkPoint/Weights-028--0.06866.hdf5', X_LL_train, X_LH_train, X_HL_train, X_HH_train, Y_train, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlSUPMxf6tFa"
      },
      "source": [
        "# !ls content/drive/MyDrive/Moire/checkPoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYl1Npdb623a"
      },
      "source": [
        "# mainTest('content/drive/MyDrive/Moire/checkPoint/retrained-002-moirePattern3CNN_.h5', '/content/drive/MyDrive/Moire/testDataPositive', '/content/drive/MyDrive/Moire/testDataNegative')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}