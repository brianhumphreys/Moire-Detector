{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Copy of Playground.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianhumphreys/Moire-Detector/blob/main/Copy_of_Playground_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDFp5O-Bqa_H"
      },
      "source": [
        "# Playground"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To3GCs0JndZT",
        "outputId": "d603a938-8fd2-4bfd-a7fa-0b867a0d36dd"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jul 18 16:25:20 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5iT4R4PnYT6",
        "outputId": "5729abe3-d956-4cf1-90c3-1a118886843b"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "muinrEgNqa_J"
      },
      "source": [
        "## Test 2D Wavelet Decomposition function\n",
        "This function(fwdHaarDWT2D) computes the 2D Wavelet Transform in the image. All the input images are passed through a Haar Wavelet Decomposition module, to get the LL, LH, HL and HHH component of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWlOXh6fq3xg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "1265e6b1-978d-40ba-82f7-2b2d9766fa32"
      },
      "source": [
        "from google.colab import drive                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b7db30dd3256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r96WGlWEqjL1"
      },
      "source": [
        "# !ls /content/drive/MyDrive/Moire/train/negative/001/"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDw39lK9kAe0"
      },
      "source": [
        "# !ls /content/drive/MyDrive/Moire/trainDataNegative\n",
        "# !ls /content/drive/MyDrive/Moire/train/negative/001\n",
        "# !mv -v /content/drive/MyDrive/Moire/trainDataNegative/ '/content/drive/MyDrive/Moire/train/negative/001'\n",
        "# !ls '/content/drive/MyDrive/Moire/train/negative/001/trainDataNegative'\n",
        "# !mv -v '/content/drive/MyDrive/Moire/train/negative/001/trainDataNegative/*.tiff' '/content/drive/MyDrive/Moire/train/negative/001'\n",
        "# import os\n",
        "\n",
        "# fromDir = '/content/drive/MyDrive/Moire/notNormalizedPositiveImages/'\n",
        "# toDir = \"/content/drive/MyDrive/Moire/unnormalized/positive/002/\"\n",
        "\n",
        "# # !ls /content/drive/MyDrive/Moire/testDataNegative/\n",
        "\n",
        "# for filename in os.listdir(fromDir):\n",
        "#   os.rename(fromDir + filename, toDir + filename)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA7CMKEhro9d"
      },
      "source": [
        "# import os\n",
        "# os.chdir(\"/content/drive/\")\n",
        "# !ls\n",
        "\n",
        "# os.chdir(\"../..\")\n",
        "# !ls\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsTZ_TLIv3TK",
        "outputId": "6016594d-2b1f-4397-f97a-385623253d03"
      },
      "source": [
        "!ls /content/drive/MyDrive/Moire\n",
        "!pip install pyheif whatimage\n",
        "# !pip show wand"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test  train\n",
            "Collecting pyheif\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/fc/8d4714687b351f098426832dedf8fee73a87d44bfdb15168cdbd68973e4d/pyheif-0.5.1-cp37-cp37m-manylinux2014_x86_64.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 4.7MB/s \n",
            "\u001b[?25hCollecting whatimage\n",
            "  Downloading https://files.pythonhosted.org/packages/6b/84/66f1cbaa0ea04cd4d5284bbbb64724d2cd17fb7f743e75ee46e98a953318/whatimage-0.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pyheif) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->pyheif) (2.20)\n",
            "Installing collected packages: pyheif, whatimage\n",
            "Successfully installed pyheif-0.5.1 whatimage-0.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmgaiBmhtDHX"
      },
      "source": [
        "# !ls /content/drive/MyDrive/Moire/notNormalizedPositiveImages/\n",
        "# !ls /content/drive/MyDrive/Moire/notNormalizedNegativeImages/\n",
        "# !uname -m"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO9ckef9Wrs3"
      },
      "source": [
        "## Normalized Raw data to 1000x750 pixel images\n",
        "Images are cropped to fit a 3:4 aspect ratio and then resized to match a 1000x750 size. Moves photos **unnormalized** -> **preaugmented**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xaN5wepXCy3",
        "outputId": "06d69145-c8ec-44d5-db5f-c7e9efcd4f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "# importing the module\n",
        "\n",
        "\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "\n",
        "import whatimage\n",
        "import pyheif\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def decodeImage(bytesIo):\n",
        "    with open(bytesIo, 'rb') as f:\n",
        "      data = f.read()\n",
        "      fmt = whatimage.identify_image(data)\n",
        "      if fmt in ['heic', 'avif']:\n",
        "        i = pyheif.read_heif(data)\n",
        "        pi = Image.frombytes(mode=i.mode, size=i.size, data=i.data)\n",
        "        pi.save(\"heic.jpg\", format=\"jpeg\")\n",
        "\n",
        "def read_heic(path: str):\n",
        "    img = Wimage(path)\n",
        "    img.format = 'jpg'\n",
        "    img.save(filename=\"heic.jpg\")\n",
        "    img.close()\n",
        "\n",
        "def openImage(fileName):\n",
        "  decodeImage(fileName)\n",
        "  return Image.open(\"heic.jpg\")\n",
        "\n",
        "def cropAndSave(image, fileName):\n",
        "    width = image.size[0]\n",
        "    height = image.size[1]\n",
        "\n",
        "    aspect = width / float(height)\n",
        "\n",
        "    if (height > width):\n",
        "        image = image.rotate(90, Image.NEAREST, expand=1)\n",
        "        width = image.size[0]\n",
        "        height = image.size[1]\n",
        "\n",
        "    ideal_width = 1000\n",
        "    ideal_height = 750\n",
        "\n",
        "    ideal_aspect = ideal_width / float(ideal_height)\n",
        "\n",
        "    if aspect > ideal_aspect:\n",
        "        # Then crop the left and right edges:\n",
        "        new_width = int(ideal_aspect * height)\n",
        "        offset = (width - new_width) / 2\n",
        "        resize = (offset, 0, width - offset, height)\n",
        "    else:\n",
        "        # ... crop the top and bottom:\n",
        "        new_height = int(width / ideal_aspect)\n",
        "        offset = (height - new_height) / 2\n",
        "        resize = (0, offset, width, height - offset)\n",
        "\n",
        "    thumb = image.crop(resize).resize((ideal_width, ideal_height), Image.ANTIALIAS)\n",
        "    thumb.save(fileName)\n",
        "\n",
        "\n",
        "\n",
        "def normalizeRawImages(superpoch, dataSetNumber):\n",
        "\n",
        "  print('##### NORMALIZING - superpoch: ' + superpoch + ' - dataset: ' + dataSetNumber)\n",
        "\n",
        "  negativeFromDir = '/content/drive/MyDrive/Moire/unnormalized/negative/' + dataSetNumber + '/'\n",
        "  positiveFromDir = '/content/drive/MyDrive/Moire/unnormalized/positive/' + dataSetNumber + '/'\n",
        "  negativeToDir = '/content/drive/MyDrive/Moire/preaugmented/negative/' + dataSetNumber + '/'\n",
        "  positiveToDir = '/content/drive/MyDrive/Moire/preaugmented/positive/' + dataSetNumber + '/'\n",
        "\n",
        "  if not os.path.exists(positiveToDir):\n",
        "      os.makedirs(positiveToDir)\n",
        "  if not os.path.exists(negativeToDir):\n",
        "      os.makedirs(negativeToDir)\n",
        "\n",
        "  if len([name for name in os.listdir(positiveToDir)]) > 0:\n",
        "    print('Directory ' + positiveToDir + ' already has normalized photos in it.  Skipping normalization for negative photos.')\n",
        "  if len([name for name in os.listdir(negativeToDir)]) > 0:\n",
        "    print('Directory ' + negativeToDir + ' already has normalized photos in it.  Skipping normalization for positive photos.')\n",
        "\n",
        "  positiveImageFiles = [f for f in listdir(positiveFromDir) if (isfile(join(positiveFromDir, f)))]\n",
        "  negativeImageFiles = [f for f in listdir(negativeFromDir) if (isfile(join(negativeFromDir, f)))]\n",
        "\n",
        "  # LLList = [l for l in positiveImageFiles if 'LLL' in l]\n",
        "  # print(LLList)\n",
        "\n",
        "  if len([name for name in os.listdir(positiveToDir)]) <= 0:\n",
        "    for f in positiveImageFiles:\n",
        "        print(join(positiveFromDir, f))\n",
        "        img = openImage(join(positiveFromDir, f))\n",
        "\n",
        "        rgb_im = img.convert(\"RGB\")\n",
        "        components = f.split('.')\n",
        "        newComponents = components[:len(components) - 1]\n",
        "        newComponents.append('jpg')\n",
        "        newFileName = '.'.join(newComponents)\n",
        "\n",
        "        cropAndSave(rgb_im, join(positiveToDir, newFileName))\n",
        "\n",
        "  if len([name for name in os.listdir(negativeToDir)]) <= 0:\n",
        "    for f in negativeImageFiles:\n",
        "        print(join(negativeFromDir, f))\n",
        "        img = Image.open(join(negativeFromDir, f))\n",
        "\n",
        "        rgb_im = img.convert(\"RGB\")\n",
        "        components = f.split('.')\n",
        "        newComponents = components[:len(components) - 1]\n",
        "        newComponents.append('jpg')\n",
        "        newFileName = '.'.join(newComponents)\n",
        "\n",
        "        cropAndSave(rgb_im, join(negativeToDir, newFileName))\n",
        "\n",
        "# normalizeRawImages(\"001\", \"005\")\n",
        "# normalizeRawImages(\"001\", \"006\")  \n",
        "# normalizeRawImages(\"001\", \"007\")\n",
        "# normalizeRawImages(\"001\", \"008\") \n",
        "# normalizeRawImages(\"001\", \"009\")\n",
        "# normalizeRawImages(\"001\", \"010\")\n",
        "# normalizeRawImages(\"001\", \"011\") \n",
        "# normalizeRawImages(\"001\", \"012\") \n",
        "normalizeRawImages(\"001\", \"013\") \n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##### NORMALIZING - superpoch: 001 - dataset: 013\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d0c9215df86c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m# normalizeRawImages(\"001\", \"011\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m# normalizeRawImages(\"001\", \"012\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m \u001b[0mnormalizeRawImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"001\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"013\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-d0c9215df86c>\u001b[0m in \u001b[0;36mnormalizeRawImages\u001b[0;34m(superpoch, dataSetNumber)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Directory '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnegativeToDir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' already has normalized photos in it.  Skipping normalization for positive photos.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m   \u001b[0mpositiveImageFiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositiveFromDir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositiveFromDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m   \u001b[0mnegativeImageFiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegativeFromDir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegativeFromDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Moire/unnormalized/positive/013/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9o2VLHoqyOo"
      },
      "source": [
        "#This function(fwdHaarDWT2D) computes the 2D Wavelet Transform in the image. All the input images are passed through a Haar Wavelet Decomposition module, to get the LL, LH, HL and HHH component of the image\n",
        "\n",
        "import numpy as np\n",
        "import pywt\n",
        "\n",
        "def splitFreqBands(img, levRows, levCols):\n",
        "    halfRow = int(levRows/2)\n",
        "    halfCol = int(levCols/2)\n",
        "    LL = img[0:halfRow, 0:halfCol]\n",
        "    LH = img[0:halfRow, halfCol:levCols]\n",
        "    HL = img[halfRow:levRows, 0:halfCol]\n",
        "    HH = img[halfRow:levRows, halfCol:levCols]\n",
        "    \n",
        "    return LL, LH, HL, HH\n",
        "    \n",
        "def haarDWT1D(data, length):\n",
        "    avg0 = 0.5;\n",
        "    avg1 = 0.5;\n",
        "    dif0 = 0.5;\n",
        "    dif1 = -0.5;\n",
        "    temp = np.empty_like(data)\n",
        "    temp = temp.astype(float)\n",
        "    h = int(length/2)\n",
        "    for i in range(h):\n",
        "        k = i*2\n",
        "        temp[i] = data[k] * avg0 + data[k + 1] * avg1;\n",
        "        temp[i + h] = data[k] * dif0 + data[k + 1] * dif1;\n",
        "    \n",
        "    data[:] = temp\n",
        "\n",
        "# computes the homography coefficients for PIL.Image.transform using point correspondences\n",
        "def fwdHaarDWT2D(img):\n",
        "    img = np.array(img)\n",
        "    levRows = img.shape[0];\n",
        "    levCols = img.shape[1];\n",
        "    img = img.astype(float)\n",
        "    for i in range(levRows):\n",
        "        row = img[i,:]\n",
        "        haarDWT1D(row, levCols)\n",
        "        img[i,:] = row\n",
        "    for j in range(levCols):\n",
        "        col = img[:,j]\n",
        "        haarDWT1D(col, levRows)\n",
        "        img[:,j] = col\n",
        "        \n",
        "    return splitFreqBands(img, levRows, levCols)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a63KHmZYqa_K"
      },
      "source": [
        "# from PIL import Image\n",
        "# from matplotlib import pyplot as plt\n",
        "# !ls /content/drive/MyDrive/Moire/\n",
        "# img = Image.open('/content/drive/MyDrive/Moire/preaugmented/positive/001/IMG_2906.jpg').convert('L')\n",
        "# img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "# img.save('chl.jpg')\n",
        "# LL, LH, HL, HH = fwdHaarDWT2D(img)\n",
        "# fig, axes = plt.subplots(2, 2)\n",
        "# fig.tight_layout()\n",
        "# axes[0, 0].imshow(LL)\n",
        "# axes[0, 1].imshow(LH)\n",
        "# axes[1, 0].imshow(HL)\n",
        "# axes[1, 1].imshow(HH)\n",
        "# axes[0, 0].set_title(\"LL\")\n",
        "# axes[0, 1].set_title(\"LH\")\n",
        "# axes[1, 0].set_title(\"HL\")\n",
        "# axes[1, 1].set_title(\"HH\")\n",
        "# plt.show()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNnfNmoEqa_M"
      },
      "source": [
        "## Augment normalized data\n",
        "The training images need to be put in two folders. positiveImages and negativeImages. positiveImages are the images which are captured from the display devices and has the presence of stron or weak Moiré patterms in it.\n",
        "negativeImages are the ones without Moiré Patterns (i.e. the images which are not captured from the display devices).  Moves photos **preaugmented** -> **train/test** based on a split ratio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO8AKkFsqm5W",
        "outputId": "9c85f467-971d-4a0d-8fea-bfc5de5b0653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "import sys\n",
        "import argparse\n",
        "from PIL import Image\n",
        "from PIL import ImageOps\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "\n",
        "#The training images need to be put in two folders. positiveImages and negativeImages. positiveImages are the images which are captured from the display devices and has the presence of stron or weak Moiré patterms in it. negativeImages are the ones without Moiré Patterns (i.e. the images which are not captured from the display devices)\n",
        "\n",
        "\n",
        "def augmentNormalizedData(superpoch, dataSetNumber):\n",
        "\n",
        "  print('##### AUGMENTING - superpoch: ' + superpoch + ' - dataset: ' + dataSetNumber)\n",
        "\n",
        "  negativeFromDir = '/content/drive/MyDrive/Moire/preaugmented/negative/' + dataSetNumber + '/'\n",
        "  positiveFromDir = '/content/drive/MyDrive/Moire/preaugmented/positive/' + dataSetNumber + '/'\n",
        "\n",
        "  negativeToTrainDir = '/content/drive/MyDrive/Moire/train/negative/' + dataSetNumber + '/'\n",
        "  positiveToTrainDir = '/content/drive/MyDrive/Moire/train/positive/' + dataSetNumber + '/'\n",
        "  negativeToTestDir = '/content/drive/MyDrive/Moire/test/negative/' + dataSetNumber + '/'\n",
        "  positiveToTestDir = '/content/drive/MyDrive/Moire/test/positive/' + dataSetNumber + '/'\n",
        "\n",
        "  if not os.path.exists(negativeToTrainDir):\n",
        "      os.makedirs(negativeToTrainDir)\n",
        "  if not os.path.exists(positiveToTrainDir):\n",
        "      os.makedirs(positiveToTrainDir)\n",
        "  if not os.path.exists(negativeToTestDir):\n",
        "      os.makedirs(negativeToTestDir)\n",
        "  if not os.path.exists(positiveToTestDir):\n",
        "      os.makedirs(positiveToTestDir)\n",
        "\n",
        "  if len([name for name in os.listdir(negativeToTrainDir)]) > 0:\n",
        "    print('Directory ' + negativeToTrainDir + ' already has normalized photos in it.  Skipping augmentation for negative photos.')\n",
        "  if len([name for name in os.listdir(positiveToTrainDir)]) > 0:\n",
        "    print('Directory ' + positiveToTrainDir + ' already has normalized photos in it.  Skipping augmentation for positive photos.')\n",
        "  if len([name for name in os.listdir(negativeToTestDir)]) > 0:\n",
        "    print('Directory ' + negativeToTestDir + ' already has normalized photos in it.  Skipping augmentation for negative photos.')\n",
        "  if len([name for name in os.listdir(positiveToTestDir)]) > 0:\n",
        "    print('Directory ' + positiveToTestDir + ' already has normalized photos in it.  Skipping augmentation for positive photos.')\n",
        "        \n",
        "  createTrainingData(positiveFromDir, negativeFromDir, positiveToTrainDir, negativeToTrainDir, positiveToTestDir, negativeToTestDir)\n",
        "\n",
        "    \n",
        "#The wavelet decomposed images are the transformed images representing the spatial and the frequency information of the image. These images are stored as 'tiff' in the disk, to preserve that information. Each image is transformed with 180 degrees rotation and as well flipped, as part of data augmentation.\n",
        "\n",
        "def transformImageAndSave(image, f, customStr, path):\n",
        "    cA, cH, cV, cD  = fwdHaarDWT2D(image);\n",
        "    \n",
        "    fileName = (os.path.splitext(f)[0])\n",
        "    fLL = (f.replace(fileName, fileName+'_' + customStr + 'LL')).replace('.jpg','.tiff')\n",
        "    fLH = (f.replace(fileName, fileName+'_' + customStr + 'LH')).replace('.jpg','.tiff')\n",
        "    fHL = (f.replace(fileName, fileName+'_' + customStr + 'HL')).replace('.jpg','.tiff')\n",
        "    fHH = (f.replace(fileName, fileName+'_' + customStr + 'HH')).replace('.jpg','.tiff')\n",
        "\n",
        "    cA = Image.fromarray(cA)\n",
        "    cH = Image.fromarray(cH)\n",
        "    cV = Image.fromarray(cV)\n",
        "    cD = Image.fromarray(cD)\n",
        "\n",
        "    cA.save(join(path, fLL))\n",
        "    cH.save(join(path, fLH))\n",
        "    cV.save(join(path, fHL))\n",
        "    cD.save(join(path, fHH))\n",
        "    \n",
        "    \n",
        "def augmentAndTrasformImage(f, mainFolder, trainFolder):\n",
        "    try:\n",
        "        print(join(mainFolder, f))\n",
        "        img = Image.open(join(mainFolder, f)) \n",
        "    except:\n",
        "        print('Error: Couldnt read the file {}. Make sure only images are present in the folder'.format(f))\n",
        "        return None\n",
        "\n",
        "    imgGray = img.convert('L')\n",
        "    wdChk, htChk = imgGray.size\n",
        "    if htChk > wdChk:\n",
        "        imgGray = imgGray.rotate(-90, expand=1)\n",
        "        print('training image rotated')\n",
        "    transformImageAndSave(imgGray, f, '', trainFolder)\n",
        "\n",
        "    imgGray = imgGray.transpose(Image.ROTATE_180)\n",
        "    transformImageAndSave(imgGray, f, '180_', trainFolder)\n",
        "\n",
        "    imgGray = imgGray.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    transformImageAndSave(imgGray, f, '180_FLIP_', trainFolder)\n",
        "    \n",
        "    return True\n",
        "    \n",
        "    \n",
        "def createTrainingData(positiveFromDir, negativeFromDir, positiveToTrainDir, negativeToTrainDir, positiveToTestDir, negativeToTestDir):\n",
        "\n",
        "    print('positive image path: ' + positiveFromDir)\n",
        "    print('negative image path: ' + negativeFromDir)\n",
        "    splitRatio = 0.8\n",
        "\n",
        "    # get image files by classes\n",
        "    positiveImageFiles = [f for f in listdir(positiveFromDir) if (isfile(join(positiveFromDir, f)))]\n",
        "    negativeImageFiles = [f for f in listdir(negativeFromDir) if (isfile(join(negativeFromDir, f)))]\n",
        "\n",
        "    positiveDataBorder = round(len(positiveImageFiles) * splitRatio)\n",
        "    negativeDataBorder = round(len(negativeImageFiles) * splitRatio)\n",
        "\n",
        "    positiveTrainFiles = positiveImageFiles[:positiveDataBorder]\n",
        "    positiveTestFiles = positiveImageFiles[positiveDataBorder:]\n",
        "    negativeTrainFiles = negativeImageFiles[:negativeDataBorder]\n",
        "    negativeTestFiles = negativeImageFiles[negativeDataBorder:]\n",
        "\n",
        "    print('positive train samples: ' + str(len(positiveTrainFiles)))\n",
        "    print('negative train samples: ' + str(len(negativeTrainFiles)))\n",
        "    print('positive test samples: ' + str(len(positiveTestFiles)))\n",
        "    print('negative test samples: ' + str(len(negativeTestFiles)))\n",
        "\n",
        "    Knegative = 0\n",
        "    Kpositive = 0\n",
        "\n",
        "    # create positive training images\n",
        "    if len([name for name in os.listdir(positiveToTrainDir)]) <= 0:\n",
        "      for f in positiveTrainFiles:\n",
        "          ret = augmentAndTrasformImage(f, positiveFromDir, positiveToTrainDir)\n",
        "          if ret == None:\n",
        "              continue\n",
        "          Kpositive += 3\n",
        "\n",
        "    if len([name for name in os.listdir(negativeToTrainDir)]) <= 0:\n",
        "      # create positive test images\n",
        "      for f in negativeTrainFiles:\n",
        "          ret = augmentAndTrasformImage(f, negativeFromDir, negativeToTrainDir)\n",
        "          if ret == None:\n",
        "              continue\n",
        "          Kpositive += 3\n",
        "\n",
        "    if len([name for name in os.listdir(positiveToTestDir)]) <= 0:\n",
        "      # create negative training images\n",
        "      for f in positiveTestFiles:\n",
        "          ret = augmentAndTrasformImage(f, positiveFromDir, positiveToTestDir)\n",
        "          if ret == None:\n",
        "              continue\n",
        "          Knegative += 3;\n",
        "\n",
        "    if len([name for name in os.listdir(negativeToTestDir)]) <= 0:\n",
        "      # create negative training images\n",
        "      for f in negativeTestFiles:\n",
        "          ret = augmentAndTrasformImage(f, negativeFromDir, negativeToTestDir)\n",
        "          if ret == None:\n",
        "              continue\n",
        "          Knegative += 3;\n",
        "    #\n",
        "    # print('Total positive files after augmentation: ', Kpositive)\n",
        "    # print('Total negative files after augmentation: ', Knegative)\n",
        "    \n",
        "\n",
        "\n",
        "# mainAugment('/content/drive/MyDrive/Moire/preaugmentedPositiveImages', '/content/drive/MyDrive/Moire/preaugmentedNegativeImages')\n",
        "\n",
        "# augmentNormalizedData('001', '005')\n",
        "# augmentNormalizedData('001', '006')\n",
        "# augmentNormalizedData('001', '007')\n",
        "# augmentNormalizedData('001', '008')\n",
        "# augmentNormalizedData('001', '009')\n",
        "# augmentNormalizedData('001', '010')\n",
        "# augmentNormalizedData('001', '011')\n",
        "# augmentNormalizedData('001', '012')\n",
        "augmentNormalizedData('001', '013')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##### AUGMENTING - superpoch: 001 - dataset: 013\n",
            "positive image path: /content/drive/MyDrive/Moire/preaugmented/positive/013/\n",
            "negative image path: /content/drive/MyDrive/Moire/preaugmented/negative/013/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1acbcbf5cff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;31m# augmentNormalizedData('001', '011')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;31m# augmentNormalizedData('001', '012')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m \u001b[0maugmentNormalizedData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'001'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'013'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-1acbcbf5cff2>\u001b[0m in \u001b[0;36maugmentNormalizedData\u001b[0;34m(superpoch, dataSetNumber)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Directory '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpositiveToTestDir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' already has normalized photos in it.  Skipping augmentation for positive photos.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0mcreateTrainingData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositiveFromDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegativeFromDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositiveToTrainDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegativeToTrainDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositiveToTestDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegativeToTestDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-1acbcbf5cff2>\u001b[0m in \u001b[0;36mcreateTrainingData\u001b[0;34m(positiveFromDir, negativeFromDir, positiveToTrainDir, negativeToTrainDir, positiveToTestDir, negativeToTestDir)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# get image files by classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mpositiveImageFiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositiveFromDir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositiveFromDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mnegativeImageFiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegativeFromDir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegativeFromDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Moire/preaugmented/positive/013/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzpWIkVNqa_M"
      },
      "source": [
        "# from os import listdir\n",
        "# from os.path import isfile, join\n",
        "# from PIL import Image\n",
        "\n",
        "\n",
        "# positiveImagePath = '/content/drive/MyDrive/Moire/preaugmentedPositiveImages'\n",
        "# negativeImagePath = '/content/drive/MyDrive/Moire/preaugmentedNegativeImages'\n",
        "\n",
        "# mainAugment(positiveImagePath, negativeImagePath, 0)\n",
        "   "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNTveLG9tEf6"
      },
      "source": [
        "# !cd ../\n",
        "# !ls"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INPlfs7QQXYJ"
      },
      "source": [
        "## Load Data into Memory\n",
        "Get your big boy pants on because it's going to be a lot of data.  Increase runtime memory.  This section will load data from an **augmented** directory and loads it into a tensor for training or evaluation.  Recommended to make sure that there is no accelerator used so that it can be used when training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOTnUFotxJJJ"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import argparse\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from skimage import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#constants\n",
        "WIDTH = 500#384\n",
        "HEIGHT = 375#512\n",
        "\n",
        "#Here, we perform index based splitting and use those indices to split the our multi-input datasets. This is done because the CNN model is multi-input network\n",
        "def splitTrainTestDataForBands(inputData, X_train_ind, X_test_ind):\n",
        "    X_train = np.zeros((len(X_train_ind), WIDTH*HEIGHT))\n",
        "    for i in range(len(X_train_ind)):\n",
        "        X_train[i,:] = inputData[int(X_train_ind[i,0]),:]\n",
        "        \n",
        "    X_test = np.zeros((len(X_test_ind), WIDTH*HEIGHT))\n",
        "    for i in range(len(X_test_ind)):\n",
        "        X_test[i,:] = inputData[int(X_test_ind[i,0]),:]\n",
        "        \n",
        "    return X_train, X_test\n",
        "\n",
        "\n",
        "def countPositiveSamplesAfterSplit(trainData):\n",
        "    count = 0;\n",
        "    for i in range(len(trainData)):\n",
        "        if(trainData[i,0] == 0):\n",
        "            count = count + 1\n",
        "    return count\n",
        "\n",
        "def scaleData(inp, minimum, maximum):\n",
        "    minMaxScaler = preprocessing.MinMaxScaler(copy=True, feature_range=(minimum,maximum))\n",
        "    inp = inp.reshape(-1, 1)\n",
        "    inp = minMaxScaler.fit_transform(inp)\n",
        "    \n",
        "    return inp\n",
        "\n",
        "def readAndScaleImage(f, customStr, trainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, sampleVal):\n",
        "    fileName = (os.path.splitext(f)[0])\n",
        "    # print(fileName)\n",
        "    # print(customStr)\n",
        "\n",
        "    fLL = (f.replace(fileName, fileName + customStr)).replace('.jpg','.tiff')\n",
        "    fLH = (f.replace(fileName, fileName + customStr)).replace('.jpg','.tiff')\n",
        "    fHL = (f.replace(fileName, fileName + customStr)).replace('.jpg','.tiff')\n",
        "    fHH = (f.replace(fileName, fileName + customStr)).replace('.jpg','.tiff')\n",
        "    \n",
        "    try:\n",
        "        imgLL = Image.open(join(trainImagePath, fLL))\n",
        "        imgLH = Image.open(join(trainImagePath, fLH))\n",
        "        imgHL = Image.open(join(trainImagePath, fHL))\n",
        "        imgHH = Image.open(join(trainImagePath, fHH))\n",
        "    except Exception as e:\n",
        "        print('Error: Couldnt read the file {}. Make sure only images are present in the folder'.format(fileName))\n",
        "        print('Exception:', e)\n",
        "        return None\n",
        "        \n",
        "    imgLL = np.array(imgLL)\n",
        "    imgLH = np.array(imgLH)\n",
        "    imgHL = np.array(imgHL)\n",
        "    imgHH = np.array(imgHH)\n",
        "    imgLL = scaleData(imgLL, 0, 1)\n",
        "    imgLH = scaleData(imgLH, -1, 1)\n",
        "    imgHL = scaleData(imgHL, -1, 1)\n",
        "    imgHH = scaleData(imgHH, -1, 1)\n",
        "    \n",
        "    imgVector = imgLL.reshape(1, WIDTH*HEIGHT)\n",
        "    X_LL[sampleIndex, :] = imgVector\n",
        "    imgVector = imgLH.reshape(1, WIDTH*HEIGHT)\n",
        "    X_LH[sampleIndex, :] = imgVector\n",
        "    imgVector = imgHL.reshape(1, WIDTH*HEIGHT)\n",
        "    X_HL[sampleIndex, :] = imgVector\n",
        "    imgVector = imgHH.reshape(1, WIDTH*HEIGHT)\n",
        "    X_HH[sampleIndex, :] = imgVector\n",
        "    \n",
        "    Y[sampleIndex, 0] = sampleVal;\n",
        "    X_index[sampleIndex, 0] = sampleIndex;\n",
        "\n",
        "    imgVector = None\n",
        "    imgLL = None\n",
        "    imgLH = None\n",
        "    imgHL = None\n",
        "    imgHH = None\n",
        "    \n",
        "    return True\n",
        "\n",
        "def reshapeData(X_LL, X_LH, X_HL, X_HH, X_index, Y, imageCount):\n",
        "\n",
        "    print('Dataset length: ' + str(len(X_LL)))\n",
        "    \n",
        "    print('num_train_samples', len(X_LL))\n",
        "    X_LL = np.array(X_LL)\n",
        "    X_LL = X_LL.reshape((len(X_LL), HEIGHT, WIDTH, 1))\n",
        "\n",
        "    X_LH = np.array(X_LH)\n",
        "    X_LH = X_LH.reshape((len(X_LH), HEIGHT, WIDTH, 1))\n",
        "\n",
        "    X_HL = np.array(X_HL)\n",
        "    X_HL = X_HL.reshape((len(X_HL), HEIGHT, WIDTH, 1))\n",
        "    \n",
        "    X_HH = np.array(X_HH)\n",
        "    X_HH = X_HH.reshape((len(X_HH), HEIGHT, WIDTH, 1))\n",
        "\n",
        "    Y = np.array(Y)\n",
        "    \n",
        "    return X_LL, X_LH, X_HL, X_HH, Y\n",
        "\n",
        "def readImageSet(imageFiles, trainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, bClass):\n",
        "\n",
        "    for f in imageFiles:\n",
        "        ret = readAndScaleImage(f, '', trainImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, bClass)\n",
        "        if ret == True:\n",
        "            sampleIndex = sampleIndex + 1\n",
        "\n",
        "    return sampleIndex\n",
        "\n",
        "def readWaveletData(positiveImagePath, negativeImagePath):\n",
        "    \n",
        "    # get augmented, balanced training data image files by class\n",
        "    positiveImageFiles = [f for f in listdir(positiveImagePath) if (isfile(join(positiveImagePath, f)))]\n",
        "    negativeImageFiles = [f for f in listdir(negativeImagePath) if (isfile(join(negativeImagePath, f)))]\n",
        "\n",
        "    positiveCount = len(positiveImageFiles)\n",
        "    negativeCount = len(negativeImageFiles)\n",
        "\n",
        "    print('positive samples: ' + str(positiveCount))\n",
        "    print('negative samples: ' + str(negativeCount))\n",
        "    imageCount = positiveCount + negativeCount\n",
        "    #intialization\n",
        "    X_LL = np.zeros((positiveCount + negativeCount, WIDTH*HEIGHT))\n",
        "    X_LH = np.zeros((positiveCount + negativeCount, WIDTH*HEIGHT))\n",
        "    X_HL = np.zeros((positiveCount + negativeCount, WIDTH*HEIGHT))\n",
        "    X_HH = np.zeros((positiveCount + negativeCount, WIDTH*HEIGHT))\n",
        "    X_index = np.zeros((positiveCount + negativeCount, 1))\n",
        "    Y = np.zeros((positiveCount + negativeCount, 1))\n",
        "    \n",
        "    sampleIndex = 0\n",
        "    # read all images, convert to float, divide by 255 (leads to gray range 0..1), reshape into a row vector\n",
        "    # write class 0 for positive and 1 for negative samples\n",
        "\n",
        "    sampleIndex = readImageSet(positiveImageFiles, positiveImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, 0)\n",
        "    print('positive data loaded.')\n",
        "    \n",
        "    sampleIndex += readImageSet(negativeImageFiles, negativeImagePath, X_LL, X_LH, X_HL, X_HH, X_index, Y, sampleIndex, 1)\n",
        "    print('negative data loaded.')\n",
        "\n",
        "    print('Total Samples Loaded: ', sampleIndex)\n",
        "    \n",
        "    X_LL, X_LH, X_HL, X_HH, Y = shuffle(X_LL, X_LH, X_HL, X_HH, Y, random_state=0)\n",
        "    \n",
        "    return X_LL, X_LH, X_HL, X_HH, X_index, Y, imageCount\n",
        "\n",
        "def mainReadDatafromDrive(dataSetNumber, dataType):\n",
        "\n",
        "    # dataType can be 'train' or 'test'\n",
        "\n",
        "    positiveFromTrainDir = '/content/drive/MyDrive/Moire/' + dataType + '/positive/' + dataSetNumber + '/'\n",
        "    negativeFromTrainDir = '/content/drive/MyDrive/Moire/' + dataType + '/negative/' + dataSetNumber + '/'\n",
        "    print(positiveFromTrainDir)\n",
        "    print(negativeFromTrainDir)\n",
        "\n",
        "    if not os.path.exists(positiveFromTrainDir):\n",
        "      print(\"ERROR: \" + positiveFromTrainDir + ' does not exist.  Exiting.')\n",
        "      raise ValueError('Directory does not exist')\n",
        "    if not os.path.exists(negativeFromTrainDir):\n",
        "      print(\"ERROR: \" + negativeFromTrainDir + ' does not exist.  Exiting.')\n",
        "      raise ValueError('Directory does not exist')\n",
        "    \n",
        "    X_LL, X_LH, X_HL, X_HH, X_index, Y, imageCount = readWaveletData(positiveFromTrainDir, negativeFromTrainDir)\n",
        "\n",
        "    X_LL, X_LH, X_HL, X_HH, Y = reshapeData(X_LL, X_LH, X_HL, X_HH, X_index, Y, imageCount)\n",
        "\n",
        "    return X_LL, X_LH, X_HL, X_HH, Y\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azf3IN-SRudm"
      },
      "source": [
        "## Train CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ASB86jIRxvK"
      },
      "source": [
        "import os\n",
        "\n",
        "from keras.models import Model # basic class for specifying and training a neural network\n",
        "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, Add, Multiply, Maximum\n",
        "\n",
        "def createModel(height, width, depth, num_classes):\n",
        "#     num_epochs = 20 # 50 26 200 # we iterate 200 times over the entire training set\n",
        "    kernel_size_1 = 7 # we will use 7x7 kernels \n",
        "    kernel_size_2 = 3 # we will use 3x3 kernels \n",
        "    pool_size = 2 # we will use 2x2 pooling throughout\n",
        "    conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n",
        "    conv_depth_2 = 16 # ...switching to 16 after the first pooling layer\n",
        "    drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
        "    drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
        "    hidden_size = 32 # 128 512 the FC layer will have 512 neurons\n",
        "\n",
        "\n",
        "    inpLL = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
        "    inpLH = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
        "    inpHL = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
        "    inpHH = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
        "    \n",
        "    conv_1_LL = Convolution2D(conv_depth_1, (kernel_size_1, kernel_size_1), padding='same', activation='relu')(inpLL)\n",
        "    conv_1_LH = Convolution2D(conv_depth_1, (kernel_size_1, kernel_size_1), padding='same', activation='relu')(inpLH)\n",
        "    conv_1_HL = Convolution2D(conv_depth_1, (kernel_size_1, kernel_size_1), padding='same', activation='relu')(inpHL)\n",
        "    conv_1_HH = Convolution2D(conv_depth_1, (kernel_size_1, kernel_size_1), padding='same', activation='relu')(inpHH)\n",
        "\n",
        "    pool_1_LL = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_1_LL)\n",
        "    pool_1_LH = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_1_LH)\n",
        "    pool_1_HL = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_1_HL)\n",
        "    pool_1_HH = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_1_HH)\n",
        "\n",
        "    avg_LH_HL_HH = Maximum()([pool_1_LH, pool_1_HL, pool_1_HH])\n",
        "    inp_merged = Multiply()([pool_1_LL, avg_LH_HL_HH])\n",
        "    print(inp_merged)\n",
        "    C4 = Convolution2D(conv_depth_2, (kernel_size_2, kernel_size_2), padding='same', activation='relu')(inp_merged)\n",
        "    S2 = MaxPooling2D(pool_size=(4, 4))(C4)\n",
        "    drop_1 = Dropout(drop_prob_1)(S2)\n",
        "    C5 = Convolution2D(conv_depth_1, (kernel_size_2, kernel_size_2), padding='same', activation='relu')(drop_1)\n",
        "    S3 = MaxPooling2D(pool_size=(pool_size, pool_size))(C5)\n",
        "    C6 = Convolution2D(conv_depth_1, (kernel_size_2, kernel_size_2), padding='same', activation='relu')(S3)\n",
        "    S4 = MaxPooling2D(pool_size=(pool_size, pool_size))(C6)\n",
        "    drop_2 = Dropout(drop_prob_1)(S4)\n",
        "    # Now flatten to 1D, apply FC -> ReLU (with dropout) -> softmax\n",
        "    flat = Flatten()(drop_2)\n",
        "    hidden = Dense(hidden_size, activation='relu')(flat)\n",
        "    drop_3 = Dropout(drop_prob_2)(hidden)\n",
        "    out = Dense(num_classes, activation='softmax')(drop_3)\n",
        "    \n",
        "    model = Model(inputs=[inpLL, inpLH, inpHL, inpHH], outputs=out) # To define a model, just specify its input and output layers\n",
        "    intermediate_model = Model(inputs=[inpLL, inpLH, inpHL, inpHH], outputs=inp_merged)\n",
        "    \n",
        "    return model, intermediate_model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTX2vudptIaO"
      },
      "source": [
        "#To detect Moire ́ patternzs, images are first decomposed using Wavelet decomposition (refer to file '') and trained using multi-input Convolutional neural network. The strength of the proposed CNN model is, it uses the LL intensity image (from the Wavelet decomposition) as a weight parameter for the Moire ́ pattern, thereby approximating the spatial spread of the Moire ́ pattern in the image. Usage of CNN model performs better than frequency thresholding approach as the model is trained considering diverse scenarios and it is able to distinguish between the high frequency of background texture and the Moire ́ pattern.\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import argparse\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from skimage import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import time\n",
        "from tensorflow import keras\n",
        "\n",
        "# - read positive and negative training data\n",
        "# - create X and Y from training data\n",
        "\n",
        "\n",
        "def trainMoire(superpoch, dataSetNumber, numEpochs, weights_file = None):\n",
        "\n",
        "    print('##### TRAINING - superpoch: ' + superpoch + ' - dataset: ' + dataSetNumber[0] + ', ', dataSetNumber[1])\n",
        "\n",
        "    X_LL, X_LH, X_HL, X_HH, Y = mainReadDatafromDrive(dataSetNumber[0], 'train')\n",
        "    X_LL_2, X_LH_2, X_HL_2, X_HH_2, Y_2 = mainReadDatafromDrive(dataSetNumber[1], 'train')\n",
        "\n",
        "\n",
        "    print('Concatenating 2 datasets.')\n",
        "    print(X_LL.shape)\n",
        "    print(X_LL_2.shape)\n",
        "    X_LL = np.concatenate((X_LL, X_LL_2), axis=0)\n",
        "    X_LH = np.concatenate((X_LH, X_LH_2), axis=0)\n",
        "    X_HL = np.concatenate((X_HL, X_HL_2), axis=0)\n",
        "    X_HH = np.concatenate((X_HH, X_HH_2), axis=0)\n",
        "    Y = np.concatenate((Y, Y_2), axis=0)\n",
        "    X_LL_2 = None\n",
        "    X_LH_2 = None\n",
        "    X_HL_2 = None\n",
        "    X_HH_2 = None\n",
        "    Y_2 = None\n",
        "\n",
        "    modelName = trainCNNModel(superpoch, dataSetNumber, X_LL, X_LH, X_HL, X_HH, Y, numEpochs, weights_file)\n",
        "    \n",
        "    X_LL = None\n",
        "    X_LH = None\n",
        "    X_HL = None \n",
        "    X_HH = None\n",
        "    Y = None\n",
        "\n",
        "    return modelName\n",
        "    # evaluate(model, X_LL_test,X_LH_test,X_HL_test,X_HH_test,Y_test)\n",
        "    \n",
        "\n",
        "def trainCNNModel(superpoch, dataSetNumber, X_LL_train, X_LH_train, X_HL_train, X_HH_train, y_train, num_epochs, weights_file):\n",
        "\n",
        "    batch_size = 32 # in each iteration, we consider 32 training examples at once\n",
        "    print(\"SHAPE\")\n",
        "    print(X_LL_train.shape);\n",
        "    num_train, height, width, depth = X_LL_train.shape\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    Y_train = np_utils.to_categorical(y_train, num_classes) # One-hot encode the labels\n",
        "    # Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels\n",
        "\n",
        "    checkPointFolder = '/content/drive/MyDrive/Moire/checkPoint'\n",
        "    checkpoint_name = checkPointFolder + '/mid-sp' + superpoch + '-ds' + dataSetNumber[0] + '_' + dataSetNumber[1] + '-ep{epoch:03d}-ls{val_loss:.5f}-ac{val_accuracy:.2f}-weights' \n",
        "    checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "    callbacks_list = [checkpoint]\n",
        "    \n",
        "    if not os.path.exists(checkPointFolder):\n",
        "        os.makedirs(checkPointFolder)\n",
        "        \n",
        "        \n",
        "    model = None\n",
        "    # if preloaded_model == None:\n",
        "    #   print('A preloaded model was not provided.  Creating a new one')\n",
        "    #   model, _ = createModel(height, width, depth, num_classes)\n",
        "\n",
        "    if weights_file != None:\n",
        "      print('--- Loading weights file: ' + weights_file)\n",
        "      model = keras.models.load_model(weights_file)\n",
        "    else:\n",
        "      print('--- No weights file provided.  Compiling a new one.')\n",
        "      \n",
        "      model, _ = createModel(height, width, depth, num_classes)\n",
        "      model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
        "                    optimizer='adam', # using the Adam optimiser\n",
        "                    metrics=['accuracy']) # reporting the accuracy\n",
        "      # if preloaded_model == None:\n",
        "        \n",
        "      # else:\n",
        "      #   print('--- Model already loaded with weights.  Skipping compilation')\n",
        "\n",
        "    model.fit([X_LL_train,X_LH_train,X_HL_train,X_HH_train], Y_train,                # Train the model using the training set...\n",
        "              batch_size=batch_size, epochs=num_epochs,\n",
        "              verbose=1, validation_split=0.1, callbacks=callbacks_list) # ...holding out 10% of the data for validation\n",
        "    # score, acc = model.evaluate([X_LL_test,X_LH_test,X_HL_test,X_HH_test], Y_test, verbose=1)  # Evaluate the trained model on the test set!\n",
        "\n",
        "    # print('------ weights ------')\n",
        "    # for i in range(len(model.layers)):\n",
        "    #   print(len(model.layers[i].get_weights()))\n",
        "\n",
        "    modelName = '/content/drive/MyDrive/Moire/checkPoint/final-tm' + str(time.time()).split('.')[0] + '-sp' + superpoch + '-ds' + dataSetNumber[0] + '_' + dataSetNumber[1] + '-weights'\n",
        "    model.save(modelName)\n",
        "    \n",
        "    return modelName\n",
        "\n",
        "\n",
        "def evaluate(model, X_LL_test,X_LH_test,X_HL_test,X_HH_test,y_test):\n",
        "\n",
        "    model_out = model.predict([X_LL_test,X_LH_test,X_HL_test,X_HH_test])\n",
        "    passCnt = 0\n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "\n",
        "    positive_confidence_threshold = 0.5\n",
        "    incorrect_threshold_count = 0\n",
        "    largest_fp = 0\n",
        "    smallest_tp = 2\n",
        "\n",
        "    for i in range(len(y_test)):\n",
        "        if np.argmax(model_out[i, :]) == y_test[i]:\n",
        "            str_label='Pass'\n",
        "            passCnt = passCnt + 1\n",
        "        else:\n",
        "            str_label='Fail'\n",
        "\n",
        "        if y_test[i] ==0:\n",
        "            if np.argmax(model_out[i, :]) == y_test[i]:\n",
        "                if model_out[i, 0] < smallest_tp:\n",
        "                  smallest_tp = model_out[i, 0]\n",
        "                # print('TP' + str(model_out[i, :]))\n",
        "                TP = TP + 1;\n",
        "            else:\n",
        "                # print('FN' + str(model_out[i, :]))\n",
        "                FN = FN + 1\n",
        "        else:\n",
        "            if np.argmax(model_out[i, :]) == y_test[i]:\n",
        "                # print('TN' + str(model_out[i, :]))\n",
        "                TN = TN + 1;\n",
        "            else:\n",
        "                if model_out[i, 1] > largest_fp:\n",
        "                  largest_fp = model_out[i, 1]\n",
        "                # print('FP' + str(model_out[i, :]))\n",
        "                FP = FP + 1\n",
        "\n",
        "        if model_out[i, 0] > positive_confidence_threshold and y_test[i] == 1:\n",
        "          incorrect_threshold_count = incorrect_threshold_count + 1\n",
        "\n",
        "    start = \"\\033[1m\"\n",
        "    end = \"\\033[0;0m\"\n",
        "\n",
        "    print(start + 'incorrect positives with threshold: ' + end + str(incorrect_threshold_count))\n",
        "    print(start + 'largest false positive confidence: ' + end + str(largest_fp))\n",
        "    print(start + 'smalles true positive confidence: ' + end + str(smallest_tp))\n",
        "    print(start + 'confusion matrix (test / validation)' + end)\n",
        "    print(start + 'true positive:  '+ end + str(TP))\n",
        "    print(start + 'false positive: '+ end + str(FP))\n",
        "    print(start + 'true negative:  '+ end + str(TN))\n",
        "    print(start + 'false negative: '+ end + str(FN))\n",
        "    print('\\n')\n",
        "\n",
        "    if TP+FP+FN+TN != 0:\n",
        "      print(start + 'accuracy:  ' + end + \"{:.4f} %\".format(100*(TP+TN)/(TP+FP+FN+TN)))\n",
        "    else:\n",
        "      print(start + 'accuracy:  ' + end + \"{:.4f} %\".format(100))\n",
        "\n",
        "    if TP + FP != 0:\n",
        "      print(start + 'precision: ' + end + \"{:.4f} %\".format(100*TP/(TP + FP)))\n",
        "    else:\n",
        "      print(start + 'precision: ' + end + \"{:.4f} %\".format(100))\n",
        "\n",
        "    if TP + FN != 0:\n",
        "      print(start + 'recall:  ' + end + \"{:.4f} %\".format(100*TP/(TP + FN)))\n",
        "    else:\n",
        "      print(start + 'recall:  ' + end + \"{:.4f} %\".format(100))\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "120i4Ry-qa_N"
      },
      "source": [
        "\n",
        "# !ls /content/drive/MyDrive/Moire/checkPoint/\n",
        "# from google.colab import files\n",
        "# files.download('content/drive/MyDrive/Moire/checkPoint/Weights-028--0.06866.hdf5')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWnNYpcTqa_O"
      },
      "source": [
        "## Test CNN Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV0rGdyIueOf"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import argparse\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from skimage import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "#constants\n",
        "width = 500#384 #change dimensions according to the input image in the training\n",
        "height = 375#512 #change dimensions according to the input image in the training\n",
        "depth = 1\n",
        "num_classes = 2\n",
        "\n",
        "def testMoire(weightsFile, superpoch, dataSetNumber):\n",
        "\n",
        "    print('##### TESTING - superpoch: ' + superpoch + ' - dataset: ' + dataSetNumber[0] + ', ' + dataSetNumber[1])\n",
        "    X_LL, X_LH, X_HL, X_HH, Y = mainReadDatafromDrive(dataSetNumber[0], 'test')\n",
        "    X_LL_2, X_LH_2, X_HL_2, X_HH_2, Y_2 = mainReadDatafromDrive(dataSetNumber[1], 'test')\n",
        "\n",
        "    print('Concatenating 2 datasets.')\n",
        "    X_LL = np.concatenate((X_LL, X_LL_2), axis=0)\n",
        "    X_LH = np.concatenate((X_LH, X_LH_2), axis=0)\n",
        "    X_HL = np.concatenate((X_HL, X_HL_2), axis=0)\n",
        "    X_HH = np.concatenate((X_HH, X_HH_2), axis=0)\n",
        "    Y = np.concatenate((Y, Y_2), axis=0)\n",
        "    X_LL_2 = None\n",
        "    X_LH_2 = None\n",
        "    X_HL_2 = None\n",
        "    X_HH_2 = None\n",
        "    Y_2 = None\n",
        "    \n",
        "    CNN_model = keras.models.load_model(weightsFile)\n",
        "    evaluate(CNN_model,X_LL,X_LH,X_HL,X_HH, Y)\n",
        "    X_LL = None\n",
        "    X_LH = None\n",
        "    X_HL = None \n",
        "    X_HH = None\n",
        "    Y = None\n",
        "\n",
        "def run(model, X_LL_test,X_LH_test,X_HL_test,y_test):\n",
        "    return\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYSPiTtfqa_O"
      },
      "source": [
        "# weightsFile = \"moirePattern3CNN_.h5\"\n",
        "    \n",
        "    \n",
        "# mainTest('content/drive/MyDrive/Moire/checkPoint/moirePattern3CNN_.h5', '/content/drive/MyDrive/Moire/testDataPositive', '/content/drive/MyDrive/Moire/testDataNegative')\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTxUTR6Yv2LG"
      },
      "source": [
        "## Extended Training\n",
        "Take the trained model and continue training with additional data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fb9-mhtcBbo",
        "outputId": "dc4d8b47-d365-4c7e-b977-40bc778e48ef"
      },
      "source": [
        "# normalizeRawImages(\"001\", \"001\")\n",
        "# augmentNormalizedData(\"001\", \"001\")\n",
        "# normalizeRawImages(\"001\", \"002\")\n",
        "# augmentNormalizedData(\"001\", \"002\")\n",
        "# normalizeRawImages(\"001\", \"003\")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       001\", \"003\")\n",
        "# augmentNormalizedData(\"001\", \"003\")\n",
        "# normalizeRawImages(\"001\", \"004\")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       001\", \"003\")\n",
        "# augmentNormalizedData(\"001\", \"004\")\n",
        "# normalizeRawImages(\"001\", \"005\")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       001\", \"003\")\n",
        "# augmentNormalizedData(\"001\", \"005\")\n",
        "# normalizeRawImages(\"001\", \"006\")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       001\", \"003\")\n",
        "# augmentNormalizedData(\"001\", \"006\")\n",
        "\n",
        "# weights_file = '/content/drive/MyDrive/Moire/checkPoint/final-tm1626590047-sp001-ds011_008-weights'\n",
        "weights_file = None\n",
        "\n",
        "superpoch_count = 2\n",
        "epochs = 40\n",
        "datasets = ['011', '012']\n",
        "\n",
        "for sp in range(superpoch_count):\n",
        "  superpoch = '00' + str(sp + 1)\n",
        "  print('---------------------------------------------------------')\n",
        "  print('BEGINNING TRAINING OF SUPERPOCH: ' + superpoch)\n",
        "  print('---------------------------------------------------------')\n",
        "  for ds in range(len(datasets)):\n",
        "    dsIndex = ds\n",
        "    otherDsIndex = ((ds + 1) % len(datasets))\n",
        "    # datasets = ['00' + str(dsNumber), '00' + str(otherDsNumber)]\n",
        "    print([datasets[dsIndex], datasets[otherDsIndex]])\n",
        "\n",
        "    # print(ds)\n",
        "    # print(i)\n",
        "\n",
        "    # print(datasets)\n",
        "    # normalizeRawImages(superpoch, dataset)\n",
        "    # augmentNormalizedData(superpoch, dataset)\n",
        "\n",
        "    weights_file = trainMoire(superpoch, [datasets[dsIndex], datasets[otherDsIndex]], epochs, weights_file)\n",
        "    testMoire(weights_file, superpoch, [datasets[dsIndex], datasets[otherDsIndex]])\n",
        "    print('---------------------------------------------------------')\n",
        "\n",
        "# testMoire(weights_file, '001', ['004', '005'])\n",
        "# /content/drive/MyDrive/Moire/preaugmented/positive/001/IMG_2866.HEIC\n",
        "# /content/drive/MyDrive/Moire/preaugmented/positive/001/IMG_2875.HEIC\n",
        "# /content/drive/MyDrive/Moire/preaugmented/positive/001/IMG_2893.HEIC\n",
        "# /content/drive/MyDrive/Moire/preaugmented/positive/002/IMG_2900.HEIC\n",
        "# /content/drive/MyDrive/Moire/preaugmented/positive/002/IMG_2904.HEIC\n",
        "# /content/drive/MyDrive/Moire/preaugmented/positive/002/IMG_2915.HEIC\n",
        "# /content/drive/MyDrive/Moire/preaugmented/positive/002/IMG_2921.HEIC\n",
        "# /content/drive/MyDrive/Moire/preaugmented/positive/002/IMG_2929.HEIC\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------\n",
            "BEGINNING TRAINING OF SUPERPOCH: 001\n",
            "---------------------------------------------------------\n",
            "['011', '012']\n",
            "##### TRAINING - superpoch: 001 - dataset: 011,  012\n",
            "/content/drive/MyDrive/Moire/train/positive/011/\n",
            "/content/drive/MyDrive/Moire/train/negative/011/\n",
            "positive samples: 264\n",
            "negative samples: 288\n",
            "positive data loaded.\n",
            "negative data loaded.\n",
            "Total Samples Loaded:  816\n",
            "Dataset length: 552\n",
            "num_train_samples 552\n",
            "/content/drive/MyDrive/Moire/train/positive/012/\n",
            "/content/drive/MyDrive/Moire/train/negative/012/\n",
            "positive samples: 336\n",
            "negative samples: 288\n",
            "positive data loaded.\n",
            "negative data loaded.\n",
            "Total Samples Loaded:  960\n",
            "Dataset length: 624\n",
            "num_train_samples 624\n",
            "Concatenating 2 datasets.\n",
            "(552, 375, 500, 1)\n",
            "(624, 375, 500, 1)\n",
            "SHAPE\n",
            "(1176, 375, 500, 1)\n",
            "--- No weights file provided.  Compiling a new one.\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 187, 250, 32), dtype=tf.float32, name=None), name='multiply/mul:0', description=\"created by layer 'multiply'\")\n",
            "Epoch 1/40\n",
            "34/34 [==============================] - 37s 252ms/step - loss: 0.6923 - accuracy: 0.4744 - val_loss: 0.6163 - val_accuracy: 0.7034\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.61634, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep001-ls0.61634-ac0.70-weights\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep001-ls0.61634-ac0.70-weights/assets\n",
            "Epoch 2/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.6136 - accuracy: 0.6622 - val_loss: 0.5337 - val_accuracy: 0.7881\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.61634 to 0.53373, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep002-ls0.53373-ac0.79-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep002-ls0.53373-ac0.79-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.5569 - accuracy: 0.7328 - val_loss: 0.4864 - val_accuracy: 0.7966\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.53373 to 0.48642, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep003-ls0.48642-ac0.80-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep003-ls0.48642-ac0.80-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/40\n",
            "34/34 [==============================] - 7s 214ms/step - loss: 0.5337 - accuracy: 0.7196 - val_loss: 0.4337 - val_accuracy: 0.8729\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.48642 to 0.43368, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep004-ls0.43368-ac0.87-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep004-ls0.43368-ac0.87-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.4995 - accuracy: 0.7702 - val_loss: 0.3708 - val_accuracy: 0.8729\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.43368 to 0.37083, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep005-ls0.37083-ac0.87-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep005-ls0.37083-ac0.87-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.4410 - accuracy: 0.7920 - val_loss: 0.3083 - val_accuracy: 0.8814\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.37083 to 0.30829, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep006-ls0.30829-ac0.88-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep006-ls0.30829-ac0.88-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.3814 - accuracy: 0.8218 - val_loss: 0.3007 - val_accuracy: 0.8644\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.30829 to 0.30073, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep007-ls0.30073-ac0.86-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep007-ls0.30073-ac0.86-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.3452 - accuracy: 0.8470 - val_loss: 0.3302 - val_accuracy: 0.8729\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.30073\n",
            "Epoch 9/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.4067 - accuracy: 0.8187 - val_loss: 0.2498 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.30073 to 0.24983, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep009-ls0.24983-ac0.91-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep009-ls0.24983-ac0.91-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/40\n",
            "34/34 [==============================] - 7s 214ms/step - loss: 0.2892 - accuracy: 0.8613 - val_loss: 0.2014 - val_accuracy: 0.9153\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.24983 to 0.20142, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep010-ls0.20142-ac0.92-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep010-ls0.20142-ac0.92-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.2131 - accuracy: 0.9204 - val_loss: 0.2066 - val_accuracy: 0.9237\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.20142\n",
            "Epoch 12/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.1701 - accuracy: 0.9400 - val_loss: 0.1991 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.20142 to 0.19911, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep012-ls0.19911-ac0.91-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep012-ls0.19911-ac0.91-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.1544 - accuracy: 0.9354 - val_loss: 0.3013 - val_accuracy: 0.8644\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.19911\n",
            "Epoch 14/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.3064 - accuracy: 0.8786 - val_loss: 0.2262 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.19911\n",
            "Epoch 15/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.1307 - accuracy: 0.9600 - val_loss: 0.1500 - val_accuracy: 0.9576\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.19911 to 0.15001, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep015-ls0.15001-ac0.96-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep015-ls0.15001-ac0.96-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.1068 - accuracy: 0.9644 - val_loss: 0.1605 - val_accuracy: 0.9407\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.15001\n",
            "Epoch 17/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.1204 - accuracy: 0.9553 - val_loss: 0.2128 - val_accuracy: 0.9322\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.15001\n",
            "Epoch 18/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0968 - accuracy: 0.9708 - val_loss: 0.1745 - val_accuracy: 0.9237\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.15001\n",
            "Epoch 19/40\n",
            "34/34 [==============================] - 7s 214ms/step - loss: 0.0990 - accuracy: 0.9571 - val_loss: 0.1722 - val_accuracy: 0.9407\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.15001\n",
            "Epoch 20/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.0755 - accuracy: 0.9743 - val_loss: 0.1764 - val_accuracy: 0.9407\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.15001\n",
            "Epoch 21/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.0715 - accuracy: 0.9778 - val_loss: 0.1352 - val_accuracy: 0.9576\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.15001 to 0.13519, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep021-ls0.13519-ac0.96-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep021-ls0.13519-ac0.96-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.0584 - accuracy: 0.9767 - val_loss: 0.2249 - val_accuracy: 0.9492\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.13519\n",
            "Epoch 23/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.1142 - accuracy: 0.9656 - val_loss: 0.1259 - val_accuracy: 0.9576\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.13519 to 0.12587, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep023-ls0.12587-ac0.96-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep023-ls0.12587-ac0.96-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.2258 - accuracy: 0.9304 - val_loss: 0.1574 - val_accuracy: 0.9492\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.12587\n",
            "Epoch 25/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.1235 - accuracy: 0.9625 - val_loss: 0.1563 - val_accuracy: 0.9492\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.12587\n",
            "Epoch 26/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0742 - accuracy: 0.9673 - val_loss: 0.1525 - val_accuracy: 0.9576\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.12587\n",
            "Epoch 27/40\n",
            "34/34 [==============================] - 7s 214ms/step - loss: 0.0837 - accuracy: 0.9622 - val_loss: 0.1070 - val_accuracy: 0.9661\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.12587 to 0.10700, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep027-ls0.10700-ac0.97-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds011_012-ep027-ls0.10700-ac0.97-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.0598 - accuracy: 0.9780 - val_loss: 0.1335 - val_accuracy: 0.9576\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.10700\n",
            "Epoch 29/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.0427 - accuracy: 0.9875 - val_loss: 0.1427 - val_accuracy: 0.9407\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.10700\n",
            "Epoch 30/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.0329 - accuracy: 0.9905 - val_loss: 0.1235 - val_accuracy: 0.9661\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.10700\n",
            "Epoch 31/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0252 - accuracy: 0.9951 - val_loss: 0.1503 - val_accuracy: 0.9576\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.10700\n",
            "Epoch 32/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0303 - accuracy: 0.9914 - val_loss: 0.1931 - val_accuracy: 0.9576\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.10700\n",
            "Epoch 33/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0417 - accuracy: 0.9821 - val_loss: 0.1682 - val_accuracy: 0.9576\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.10700\n",
            "Epoch 34/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0361 - accuracy: 0.9817 - val_loss: 0.1604 - val_accuracy: 0.9576\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.10700\n",
            "Epoch 35/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0226 - accuracy: 0.9958 - val_loss: 0.1380 - val_accuracy: 0.9661\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.10700\n",
            "Epoch 36/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0289 - accuracy: 0.9926 - val_loss: 0.1817 - val_accuracy: 0.9576\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.10700\n",
            "Epoch 37/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.0286 - accuracy: 0.9874 - val_loss: 0.2335 - val_accuracy: 0.9661\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.10700\n",
            "Epoch 38/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.0116 - accuracy: 0.9993 - val_loss: 0.2416 - val_accuracy: 0.9576\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.10700\n",
            "Epoch 39/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0159 - accuracy: 0.9947 - val_loss: 0.2516 - val_accuracy: 0.9576\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.10700\n",
            "Epoch 40/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0200 - accuracy: 0.9946 - val_loss: 0.2045 - val_accuracy: 0.9661\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.10700\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/final-tm1626627190-sp001-ds011_012-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "##### TESTING - superpoch: 001 - dataset: 011, 012\n",
            "/content/drive/MyDrive/Moire/test/positive/011/\n",
            "/content/drive/MyDrive/Moire/test/negative/011/\n",
            "positive samples: 60\n",
            "negative samples: 72\n",
            "positive data loaded.\n",
            "negative data loaded.\n",
            "Total Samples Loaded:  192\n",
            "Dataset length: 132\n",
            "num_train_samples 132\n",
            "/content/drive/MyDrive/Moire/test/positive/012/\n",
            "/content/drive/MyDrive/Moire/test/negative/012/\n",
            "positive samples: 84\n",
            "negative samples: 72\n",
            "positive data loaded.\n",
            "negative data loaded.\n",
            "Total Samples Loaded:  240\n",
            "Dataset length: 156\n",
            "num_train_samples 156\n",
            "Concatenating 2 datasets.\n",
            "\u001b[1mincorrect positives with threshold: \u001b[0;0m54\n",
            "\u001b[1mlargest false positive confidence: \u001b[0;0m0.34551\n",
            "\u001b[1msmalles true positive confidence: \u001b[0;0m0.5588285\n",
            "\u001b[1mconfusion matrix (test / validation)\u001b[0;0m\n",
            "\u001b[1mtrue positive:  \u001b[0;0m128\n",
            "\u001b[1mfalse positive: \u001b[0;0m54\n",
            "\u001b[1mtrue negative:  \u001b[0;0m90\n",
            "\u001b[1mfalse negative: \u001b[0;0m16\n",
            "\n",
            "\n",
            "\u001b[1maccuracy:  \u001b[0;0m75.6944 %\n",
            "\u001b[1mprecision: \u001b[0;0m70.3297 %\n",
            "\u001b[1mrecall:  \u001b[0;0m88.8889 %\n",
            "---------------------------------------------------------\n",
            "['012', '011']\n",
            "##### TRAINING - superpoch: 001 - dataset: 012,  011\n",
            "/content/drive/MyDrive/Moire/train/positive/012/\n",
            "/content/drive/MyDrive/Moire/train/negative/012/\n",
            "positive samples: 336\n",
            "negative samples: 288\n",
            "positive data loaded.\n",
            "negative data loaded.\n",
            "Total Samples Loaded:  960\n",
            "Dataset length: 624\n",
            "num_train_samples 624\n",
            "/content/drive/MyDrive/Moire/train/positive/011/\n",
            "/content/drive/MyDrive/Moire/train/negative/011/\n",
            "positive samples: 264\n",
            "negative samples: 288\n",
            "positive data loaded.\n",
            "negative data loaded.\n",
            "Total Samples Loaded:  816\n",
            "Dataset length: 552\n",
            "num_train_samples 552\n",
            "Concatenating 2 datasets.\n",
            "(624, 375, 500, 1)\n",
            "(552, 375, 500, 1)\n",
            "SHAPE\n",
            "(1176, 375, 500, 1)\n",
            "--- Loading weights file: /content/drive/MyDrive/Moire/checkPoint/final-tm1626627190-sp001-ds011_012-weights\n",
            "Epoch 1/40\n",
            "34/34 [==============================] - 9s 224ms/step - loss: 0.0689 - accuracy: 0.9811 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00195, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds012_011-ep001-ls0.00195-ac1.00-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds012_011-ep001-ls0.00195-ac1.00-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.0381 - accuracy: 0.9887 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00195\n",
            "Epoch 3/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.0393 - accuracy: 0.9877 - val_loss: 0.0161 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00195\n",
            "Epoch 4/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.0419 - accuracy: 0.9811 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00195 to 0.00125, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds012_011-ep004-ls0.00125-ac1.00-weights\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds012_011-ep004-ls0.00125-ac1.00-weights/assets\n",
            "Epoch 5/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0287 - accuracy: 0.9896 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00125\n",
            "Epoch 6/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0321 - accuracy: 0.9877 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00125\n",
            "Epoch 7/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0376 - accuracy: 0.9858 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00125\n",
            "Epoch 8/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0299 - accuracy: 0.9868 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00125\n",
            "Epoch 9/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0309 - accuracy: 0.9858 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00125\n",
            "Epoch 10/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0316 - accuracy: 0.9877 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00125\n",
            "Epoch 11/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0212 - accuracy: 0.9934 - val_loss: 0.0098 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00125\n",
            "Epoch 12/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.0316 - accuracy: 0.9868 - val_loss: 0.0083 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00125\n",
            "Epoch 13/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0225 - accuracy: 0.9924 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00125\n",
            "Epoch 14/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.0208 - accuracy: 0.9896 - val_loss: 0.1063 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00125\n",
            "Epoch 15/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.0223 - accuracy: 0.9924 - val_loss: 0.0141 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00125\n",
            "Epoch 16/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0241 - accuracy: 0.9924 - val_loss: 5.5154e-04 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.00125 to 0.00055, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds012_011-ep016-ls0.00055-ac1.00-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp001-ds012_011-ep016-ls0.00055-ac1.00-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0273 - accuracy: 0.9915 - val_loss: 0.0149 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00055\n",
            "Epoch 18/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0303 - accuracy: 0.9896 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00055\n",
            "Epoch 19/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0282 - accuracy: 0.9924 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00055\n",
            "Epoch 20/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0550 - accuracy: 0.9839 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00055\n",
            "Epoch 21/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.1205 - accuracy: 0.9688 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00055\n",
            "Epoch 22/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0482 - accuracy: 0.9811 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00055\n",
            "Epoch 23/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0210 - accuracy: 0.9943 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00055\n",
            "Epoch 24/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0420 - accuracy: 0.9905 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00055\n",
            "Epoch 25/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0294 - accuracy: 0.9858 - val_loss: 0.0124 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00055\n",
            "Epoch 26/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0325 - accuracy: 0.9887 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00055\n",
            "Epoch 27/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.0338 - accuracy: 0.9868 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00055\n",
            "Epoch 28/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0731 - accuracy: 0.9773 - val_loss: 0.0295 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00055\n",
            "Epoch 29/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0549 - accuracy: 0.9802 - val_loss: 0.3804 - val_accuracy: 0.9831\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00055\n",
            "Epoch 30/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0253 - accuracy: 0.9887 - val_loss: 0.0174 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00055\n",
            "Epoch 31/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0260 - accuracy: 0.9905 - val_loss: 0.1438 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00055\n",
            "Epoch 32/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0224 - accuracy: 0.9924 - val_loss: 0.0092 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00055\n",
            "Epoch 33/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0110 - accuracy: 0.9962 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00055\n",
            "Epoch 34/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0243 - accuracy: 0.9905 - val_loss: 0.0206 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00055\n",
            "Epoch 35/40\n",
            "34/34 [==============================] - 7s 213ms/step - loss: 0.0248 - accuracy: 0.9934 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00055\n",
            "Epoch 36/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0133 - accuracy: 0.9953 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00055\n",
            "Epoch 37/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0186 - accuracy: 0.9924 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00055\n",
            "Epoch 38/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0087 - accuracy: 0.9953 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00055\n",
            "Epoch 39/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0115 - accuracy: 0.9953 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00055\n",
            "Epoch 40/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0140 - accuracy: 0.9943 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00055\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/final-tm1626627724-sp001-ds012_011-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "##### TESTING - superpoch: 001 - dataset: 012, 011\n",
            "/content/drive/MyDrive/Moire/test/positive/012/\n",
            "/content/drive/MyDrive/Moire/test/negative/012/\n",
            "positive samples: 84\n",
            "negative samples: 72\n",
            "positive data loaded.\n",
            "negative data loaded.\n",
            "Total Samples Loaded:  240\n",
            "Dataset length: 156\n",
            "num_train_samples 156\n",
            "/content/drive/MyDrive/Moire/test/positive/011/\n",
            "/content/drive/MyDrive/Moire/test/negative/011/\n",
            "positive samples: 60\n",
            "negative samples: 72\n",
            "positive data loaded.\n",
            "negative data loaded.\n",
            "Total Samples Loaded:  192\n",
            "Dataset length: 132\n",
            "num_train_samples 132\n",
            "Concatenating 2 datasets.\n",
            "\u001b[1mincorrect positives with threshold: \u001b[0;0m46\n",
            "\u001b[1mlargest false positive confidence: \u001b[0;0m0.47396588\n",
            "\u001b[1msmalles true positive confidence: \u001b[0;0m0.54502195\n",
            "\u001b[1mconfusion matrix (test / validation)\u001b[0;0m\n",
            "\u001b[1mtrue positive:  \u001b[0;0m119\n",
            "\u001b[1mfalse positive: \u001b[0;0m46\n",
            "\u001b[1mtrue negative:  \u001b[0;0m98\n",
            "\u001b[1mfalse negative: \u001b[0;0m25\n",
            "\n",
            "\n",
            "\u001b[1maccuracy:  \u001b[0;0m75.3472 %\n",
            "\u001b[1mprecision: \u001b[0;0m72.1212 %\n",
            "\u001b[1mrecall:  \u001b[0;0m82.6389 %\n",
            "---------------------------------------------------------\n",
            "---------------------------------------------------------\n",
            "BEGINNING TRAINING OF SUPERPOCH: 002\n",
            "---------------------------------------------------------\n",
            "['011', '012']\n",
            "##### TRAINING - superpoch: 002 - dataset: 011,  012\n",
            "/content/drive/MyDrive/Moire/train/positive/011/\n",
            "/content/drive/MyDrive/Moire/train/negative/011/\n",
            "positive samples: 264\n",
            "negative samples: 288\n",
            "positive data loaded.\n",
            "negative data loaded.\n",
            "Total Samples Loaded:  816\n",
            "Dataset length: 552\n",
            "num_train_samples 552\n",
            "/content/drive/MyDrive/Moire/train/positive/012/\n",
            "/content/drive/MyDrive/Moire/train/negative/012/\n",
            "positive samples: 336\n",
            "negative samples: 288\n",
            "positive data loaded.\n",
            "negative data loaded.\n",
            "Total Samples Loaded:  960\n",
            "Dataset length: 624\n",
            "num_train_samples 624\n",
            "Concatenating 2 datasets.\n",
            "(552, 375, 500, 1)\n",
            "(624, 375, 500, 1)\n",
            "SHAPE\n",
            "(1176, 375, 500, 1)\n",
            "--- Loading weights file: /content/drive/MyDrive/Moire/checkPoint/final-tm1626627724-sp001-ds012_011-weights\n",
            "Epoch 1/40\n",
            "34/34 [==============================] - 8s 222ms/step - loss: 0.0698 - accuracy: 0.9868 - val_loss: 0.0837 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.08369, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp002-ds011_012-ep001-ls0.08369-ac0.99-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp002-ds011_012-ep001-ls0.08369-ac0.99-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0767 - accuracy: 0.9896 - val_loss: 0.0083 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.08369 to 0.00826, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp002-ds011_012-ep002-ls0.00826-ac1.00-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp002-ds011_012-ep002-ls0.00826-ac1.00-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0220 - accuracy: 0.9943 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00826 to 0.00436, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp002-ds011_012-ep003-ls0.00436-ac1.00-weights\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp002-ds011_012-ep003-ls0.00436-ac1.00-weights/assets\n",
            "Epoch 4/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0253 - accuracy: 0.9924 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00436 to 0.00256, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp002-ds011_012-ep004-ls0.00256-ac1.00-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp002-ds011_012-ep004-ls0.00256-ac1.00-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0281 - accuracy: 0.9943 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00256\n",
            "Epoch 6/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0310 - accuracy: 0.9924 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00256 to 0.00107, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp002-ds011_012-ep006-ls0.00107-ac1.00-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp002-ds011_012-ep006-ls0.00107-ac1.00-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0096 - accuracy: 0.9972 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00107\n",
            "Epoch 8/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0140 - accuracy: 0.9962 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00107\n",
            "Epoch 9/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0092 - accuracy: 0.9962 - val_loss: 7.1382e-04 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00107 to 0.00071, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp002-ds011_012-ep009-ls0.00071-ac1.00-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp002-ds011_012-ep009-ls0.00071-ac1.00-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0122 - accuracy: 0.9972 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00071\n",
            "Epoch 11/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0079 - accuracy: 0.9972 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00071\n",
            "Epoch 12/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0084 - accuracy: 0.9972 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00071\n",
            "Epoch 13/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0099 - accuracy: 0.9981 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00071\n",
            "Epoch 14/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0145 - accuracy: 0.9962 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00071\n",
            "Epoch 15/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0104 - accuracy: 0.9943 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00071\n",
            "Epoch 16/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0104 - accuracy: 0.9962 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00071\n",
            "Epoch 17/40\n",
            "34/34 [==============================] - 7s 211ms/step - loss: 0.0148 - accuracy: 0.9943 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00071\n",
            "Epoch 18/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0139 - accuracy: 0.9943 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00071\n",
            "Epoch 19/40\n",
            "34/34 [==============================] - 7s 212ms/step - loss: 0.0106 - accuracy: 0.9953 - val_loss: 3.4013e-04 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.00071 to 0.00034, saving model to /content/drive/MyDrive/Moire/checkPoint/mid-sp002-ds011_012-ep019-ls0.00034-ac1.00-weights\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Moire/checkPoint/mid-sp002-ds011_012-ep019-ls0.00034-ac1.00-weights/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/40\n",
            "26/34 [=====================>........] - ETA: 1s - loss: 0.0172 - accuracy: 0.9940"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyiISSNr2ZAr"
      },
      "source": [
        "# X_LL_train, X_LH_train, X_HL_train, X_HH_train, Y_train = mainReadDatafromDrive('/content/drive/MyDrive/Moire/trainDataPositive', '/content/drive/MyDrive/Moire/trainDataNegative')\n",
        "# !ls /content/drive/MyDrive/Moire/preaugmented/positive/003\n",
        "print('negative unnormalized 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/negative/001')])))\n",
        "print('positive unnormalized 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/positive/001')])))\n",
        "print('negative unnormalized 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/negative/002')])))\n",
        "print('positive unnormalized 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/positive/002')])))\n",
        "print('negative unnormalized 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/negative/003')])))\n",
        "print('positive unnormalized 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/unnormalized/positive/003')])))\n",
        "# print('negative train 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/negative/001')])))\n",
        "# print('negative train 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/negative/002')])))\n",
        "# print('negative train 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/negative/003')])))\n",
        "# print('positive train 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/positive/001')])))\n",
        "# print('positive train 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/positive/002')])))\n",
        "# print('positive train 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/train/positive/003')])))\n",
        "# print('negative test 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/negative/001')])))\n",
        "# print('negative test 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/negative/002')])))\n",
        "# print('negative test 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/negative/003')])))\n",
        "# print('positive test 001: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/positive/001')])))\n",
        "# print('positive test 002: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/positive/002')])))\n",
        "# print('positive test 003: ' + str(len([name for name in os.listdir('/content/drive/MyDrive/Moire/test/positive/003')])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9RFVMxIroLI"
      },
      "source": [
        "# testMoire('/content/drive/MyDrive/Moire/checkPoint/mid-sp005-ds002-ep016-ls0.00455-ac1.00-weights.h5', \"001\", \"001\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDO0lSriFOJt"
      },
      "source": [
        "def intermediateMoire(dataSetNumber, weights_file = None):\n",
        "\n",
        "    X_LL, X_LH, X_HL, X_HH, Y = mainReadDatafromDrive(dataSetNumber, 'test')\n",
        "    return createIntermediateModel(dataSetNumber, X_LL, X_LH, X_HL, X_HH, Y, weights_file)\n",
        "    \n",
        "\n",
        "def createIntermediateModel(dataSetNumber, X_LL_train, X_LH_train, X_HL_train, X_HH_train, y_train, weights_file):\n",
        "\n",
        "    batch_size = 32 # in each iteration, we consider 32 training examples at once\n",
        "    print(\"SHAPE\")\n",
        "    print(X_LL_train.shape);\n",
        "    num_train, height, width, depth = X_LL_train.shape\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    Y_train = np_utils.to_categorical(y_train, num_classes) # One-hot encode the labels\n",
        "    # Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels\n",
        "    \n",
        "    # if not os.path.exists(checkPointFolder):\n",
        "    #     os.makedirs(checkPointFolder)\n",
        "        \n",
        "        \n",
        "    model, intermediate_model = createModel(height, width, depth, num_classes)\n",
        "\n",
        "    if weights_file != None:\n",
        "      print('--- Loading weights file: ' + weights_file)\n",
        "      model.load_weights(weights_file)\n",
        "    else:\n",
        "      print('--- No weights file provided.  Creating new model.')\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
        "                  optimizer='adam', # using the Adam optimiser\n",
        "                  metrics=['accuracy']) # reporting the accuracy\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # weights_list = model.get_weights()\n",
        "    # for i in range(len(model.layers)):\n",
        "    #   print(model.layers[i].get_weights())\n",
        "      # print(model.layers[i].get_weights()[1])\n",
        "    \n",
        "    model_index = 0;\n",
        "    print('----------------')\n",
        "    for layer in intermediate_model.layers:\n",
        "      for weights in layer.get_weights():\n",
        "        weights = model.get_weights()[model_index]\n",
        "        model_index = model_index + 1\n",
        "\n",
        "    # print('----------------')\n",
        "\n",
        "    # for layer in model.layers:\n",
        "    #   for weights in layer.get_weights():\n",
        "    #     print(len(weights))\n",
        "    # print('----------------')\n",
        "    \n",
        "    # intermediate_model.layers[5].set_weights(weights_list[0])\n",
        "    # print(len(model.layers))\n",
        "    # intermediate_model.layers[i].set_weights(weights)\n",
        "\n",
        "    # intermediate_model.summary()\n",
        "\n",
        "    # model.fit([X_LL_train,X_LH_train,X_HL_train,X_HH_train], Y_train,                # Train the model using the training set...\n",
        "    #           batch_size=batch_size, epochs=num_epochs,\n",
        "    #           verbose=1, validation_split=0.1, callbacks=callbacks_list) # ...holding out 10% of the data for validation\n",
        "    # score, acc = model.evaluate([X_LL_test,X_LH_test,X_HL_test,X_HH_test], Y_test, verbose=1)  # Evaluate the trained model on the test set!\n",
        "\n",
        "    # modelName = '/content/drive/MyDrive/Moire/checkPoint/final-tm' + str(time.time()).split('.')[0] + '-sp' + superpoch + '-ds' + dataSetNumber + '-weights.h5'\n",
        "    # model.save(modelName)\n",
        "    \n",
        "    return intermediate_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27zETVwPEMCf"
      },
      "source": [
        "# from keras.models import Model\n",
        "\n",
        "# model = intermediateMoire(\"001\", '/content/drive/MyDrive/Moire/checkPoint/mid-sp005-ds002-ep016-ls0.00455-ac1.00-weights.h5')\n",
        "\n",
        "# model.summary()\n",
        "# X_LL, X_LH, X_HL, X_HH, Y = mainReadDatafromDrive(\"001\", 'test')\n",
        "# model_out = model.predict([X_LL, X_LH, X_HL, X_HH])\n",
        "\n",
        "\n",
        "# print(model_out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ESg5hvZl3u_"
      },
      "source": [
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "# model_shape = model_out.shape\n",
        "\n",
        "# reshaped_output = np.reshape(model_out, (model_shape[0], model_shape[3], model_shape[1], model_shape[2]))\n",
        "\n",
        "# print(reshaped_output.shape)\n",
        "# index = 0\n",
        "# # for image_batch in reshaped_output:\n",
        "# for image in reshaped_output[0]:\n",
        "#   # image_batch = np.reshape(reshaped_output, (model_shape[0], model_shape[3], model_shape[1], model_shape[2]))\n",
        "#   print(image.shape)\n",
        "\n",
        "#   largest = 0\n",
        "#   smallest = 100000\n",
        "\n",
        "#   for row in image:\n",
        "#     for i in row:\n",
        "#       if i > largest:\n",
        "#         largest = i\n",
        "#       if i < smallest:\n",
        "#         smallest = i\n",
        "\n",
        "#   value_range = largest - smallest\n",
        "#   scale_value = 255 / value_range\n",
        "\n",
        "#   for i in range(len(image)):\n",
        "#     for j in range(len(image[i])):\n",
        "#       image[i][j] = scale_value * image[i][j]\n",
        "#   print('largest: ' + str(largest))\n",
        "#   print('smallest: ' + str(smallest))\n",
        "#   im = Image.fromarray(image)\n",
        "#   index = index + 1\n",
        "\n",
        "#   if im.mode != 'RGB':\n",
        "#     im = im.convert('RGB')\n",
        "\n",
        "#   im.save('/content/drive/MyDrive/Moire/intermediate/test/negative/001/' + str(index) + \".jpg\")\n",
        "\n",
        "# print(index)\n",
        "# print(np.reshape(model_out, (model_shape[0], model_shape[3], model_shape[1], model_shape[2])).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnwQH5eYqa_O"
      },
      "source": [
        "# #constants\n",
        "# # width = 500#384 #change dimensions according to the input image in the training\n",
        "# # height = 375#512 #change dimensions according to the input image in the training\n",
        "# # depth = 1\n",
        "# # num_classes = 2\n",
        "\n",
        "# def continueTraining(weightsFile, X_LL_train, X_LH_train, X_HL_train, X_HH_train, Y_train, num_epochs):\n",
        "#     weights_file = (weightsFile)\n",
        "\n",
        "#     # X_LL, X_LH, X_HL, X_HH, Y = mainReadDatafromDrive(positiveImagePath, negativeImagePath)\n",
        "    \n",
        "    \n",
        "\n",
        "#     batch_size = 32 # in each iteration, we consider 32 training examples at once\n",
        "#     print(\"SHAPE\")\n",
        "#     print(X_LL_train.shape);\n",
        "#     num_train, height, width, depth = X_LL_train.shape\n",
        "#     num_classes = len(np.unique(Y_train))\n",
        "#     Y_train = np_utils.to_categorical(Y_train, num_classes) # One-hot encode the labels\n",
        "#     print(Y_train);\n",
        "#     # Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels\n",
        "\n",
        "#     CNN_model = createModel(height, width, depth, num_classes)\n",
        "#     CNN_model.load_weights(weights_file)\n",
        "\n",
        "#     checkPointFolder = 'content/drive/MyDrive/Moire/checkPoint'\n",
        "#     checkpoint_name = checkPointFolder + '/Weights-retrained-002--{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "#     checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "#     callbacks_list = [checkpoint]\n",
        "    \n",
        "#     if not os.path.exists(checkPointFolder):\n",
        "#         os.makedirs(checkPointFolder)\n",
        "        \n",
        "        \n",
        "#     # model = createModel(height, width, depth, num_classes)\n",
        "    \n",
        "#     CNN_model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
        "#                   optimizer='adam', # using the Adam optimiser\n",
        "#                   metrics=['accuracy']) # reporting the accuracy\n",
        "\n",
        "#     CNN_model.fit([X_LL_train,X_LH_train,X_HL_train,X_HH_train], Y_train,                # Train the model using the training set...\n",
        "#               batch_size=batch_size, epochs=num_epochs,\n",
        "#               verbose=1, validation_split=0.1, callbacks=callbacks_list) # ...holding out 10% of the data for validation\n",
        "#     # score, acc = model.evaluate([X_LL_test,X_LH_test,X_HL_test,X_HH_test], Y_test, verbose=1)  # Evaluate the trained model on the test set!\n",
        "\n",
        "#     CNN_model.save('content/drive/MyDrive/Moire/checkPoint/retrained-002-moirePattern3CNN_.h5')\n",
        "    \n",
        "#     return model\n",
        "#     # evaluate(CNN_model,X_LL,X_LH,X_HL,X_HH, Y)\n",
        "# import time\n",
        "# str(time.time()).split('.')[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fyxBfYY4i7U"
      },
      "source": [
        "# continueTraining('content/drive/MyDrive/Moire/checkPoint/Weights-028--0.06866.hdf5', X_LL_train, X_LH_train, X_HL_train, X_HH_train, Y_train, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlSUPMxf6tFa"
      },
      "source": [
        "# !ls content/drive/MyDrive/Moire/checkPoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYl1Npdb623a"
      },
      "source": [
        "# mainTest('content/drive/MyDrive/Moire/checkPoint/retrained-002-moirePattern3CNN_.h5', '/content/drive/MyDrive/Moire/testDataPositive', '/content/drive/MyDrive/Moire/testDataNegative')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}